{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Quickstart","text":""},{"location":"#quickstart","title":"Quickstart","text":"<p>Info</p> <p>You can use the <code>llms.txt</code> or <code>llms-full.txt</code> to feed your favorite LMs with Synalinks documentation. Or better, use Synalinks Claude Skills with Claude Code to use Synalinks right away!</p>"},{"location":"#install","title":"Install","text":"<pre><code>uv pip install synalinks\n</code></pre>"},{"location":"#programming-your-application-4-ways","title":"Programming your application: 4 ways","text":""},{"location":"#using-the-functional-api","title":"Using the <code>Functional</code> API","text":"<p>You start from <code>Input</code>, you chain modules calls to specify the program's structure,  and finally, you create your program from inputs and outputs:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    x1 = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n    )(x0)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x1,\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#subclassing-the-program-class","title":"Subclassing the <code>Program</code> class","text":"<p>In that case, you should define your modules in <code>__init__()</code> and implement the program's structure in <code>call()</code>.</p> <p>Note: you can optionaly have a <code>training</code> argument (boolean), which you can use to specify a different behavior in training and inference.</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    class ChainOfThought(synalinks.Program):\n        \"\"\"Useful to answer in a step by step manner.\n\n        The first line of the docstring is provided as description\n        for the program if not provided in the `super().__init__()`.\n        In a similar way the name is automatically infered based on\n        the class name if not provided.\n        \"\"\"\n\n        def __init__(\n            self,\n            language_model=None,\n            name=None,\n            description=None,\n            trainable=True,\n        ):\n            super().__init__(\n                name=name,\n                description=description,\n                trainable=trainable,\n            )\n            self.answer = synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=language_model,\n                name=\"generator_\"+self.name,\n            )\n\n        async def call(self, inputs, training=False):\n            if not inputs:\n                return None\n            x = await self.answer(inputs, training=training)\n            return x\n\n        def get_config(self):\n            config = {\n                \"name\": self.name,\n                \"description\": self.description,\n                \"trainable\": self.trainable,\n            }\n            language_model_config = \\\n            {\n                \"language_model\": synalinks.saving.serialize_synalinks_object(\n                    self.language_model\n                )\n            }\n            return {**config, **language_model_config}\n\n        @classmethod\n        def from_config(cls, config):\n            language_model = synalinks.saving.deserialize_synalinks_object(\n                config.pop(\"language_model\")\n            )\n            return cls(language_model=language_model, **config)\n\n    language_model = synalinks.LanguageModel(model=\"ollama/mistral\")\n\n    program = ChainOfThought(language_model=language_model)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#mixing-the-subclassing-and-the-functional-api","title":"Mixing the subclassing and the <code>Functional</code> API","text":"<p>This way of programming is recommended to encapsulate your application while providing an easy to use setup. It is the recommended way for most users as it avoid making your program/agents from scratch. In that case, you should implement only the <code>__init__()</code> and <code>build()</code> methods.</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    class ChainOfThought(synalinks.Program):\n        \"\"\"Useful to answer in a step by step manner.\"\"\"\n\n        def __init__(\n            self,\n            language_model=None,\n            name=None,\n            description=None,\n            trainable=True,\n        ):\n            super().__init__(\n                name=name,\n                description=description,\n                trainable=trainable,\n            )\n\n            self.language_model = language_model\n\n        async def build(self, inputs):\n            outputs = await synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=self.language_model,\n            )(inputs)\n\n            # Create your program using the functional API\n            super().__init__(\n                inputs=inputs,\n                outputs=outputs,\n                name=self.name,\n                description=self.description,\n                trainable=self.trainable,\n            )\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    program = ChainOfThought(\n        language_model=language_model,\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This allows you to not have to implement the <code>call()</code> and serialization methods (<code>get_config()</code> and <code>from_config()</code>). The program will be built for any inputs the first time called.</p>"},{"location":"#using-the-sequential-api","title":"Using the <code>Sequential</code> API","text":"<p>In addition, <code>Sequential</code> is a special case of program where the program is purely a stack of single-input, single-output modules.</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    program = synalinks.Sequential(\n        [\n            synalinks.Input(\n                data_model=Query,\n            ),\n            synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=language_model,\n            ),\n        ],\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#getting-a-summary-of-your-program","title":"Getting a summary of your program","text":"<p>To print a tabular summary of your program:</p> <pre><code>program.summary()\n</code></pre> <p>Or a plot (Useful to document your system):</p> <pre><code>synalinks.utils.plot_program(\n    program,\n    show_module_names=True,\n    show_trainable=True,\n    show_schemas=True,\n)\n</code></pre> <p></p>"},{"location":"#running-your-program","title":"Running your program","text":"<p>To run your program use the following:</p> <pre><code>result = await program(\n    Query(query=\"What is the French city of aerospace?\"),\n)\n</code></pre>"},{"location":"#training-your-program","title":"Training your program","text":"<pre><code>async def main():\n\n    # ... your program definition\n\n    (x_train, y_train), (x_test, y_test) = synalinks.datasets.gsm8k.load_data()\n\n    program.compile(\n        reward=synalinks.rewards.ExactMatch(in_mask=[\"answer\"]),\n        optimizer=synalinks.optimizers.RandomFewShot()\n    )\n\n    batch_size=32\n    epochs=10\n\n    history = await program.fit(\n        x_train,\n        y_train,\n        validation_data=(x_test, y_test),\n        batch_size=batch_size,\n        epochs=epochs,\n    )\n\n    synalinks.utils.plot_history(history)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"CheatSheet/","title":"CheatSheet","text":""},{"location":"CheatSheet/#cheatsheet","title":"CheatSheet","text":"<p>This document provides a quick recap on how to use Synalinks efficiently.</p> <ul> <li>If you come from a Deep Learning / ML background, Synalinks will feel natural and structured.</li> <li>If you come from a Python Backend background, expect a short learning curve to learn ML concepts.</li> <li>If you have experience with Pydantic and LM workflows, you can likely use it immediately.</li> </ul>"},{"location":"CheatSheet/#what-do-i-need-to-know-to-use-synalinks","title":"What do I need to know to use Synalinks?","text":"<p>To work effectively with Synalinks, you should already know:</p> Skill Required Level Python 3\u20135 years Pydantic + Structured Output ~1 year Basic Linear Algebra &amp; Metrics High-school level is enough CS Fundamentals (OOP + FP) Comfortable"},{"location":"CheatSheet/#how-to-choose-which-programming-way-to-use","title":"How to choose which programming way to use?","text":"<p>Synalinks supports 4 main ways of building programs:</p> Approach Difficulty When to Use Functional API Easy Simple chaining, branching, aggregation, basic logic Sequential API Easy Simple sequence without branching Mixing (Functional + Subclassing) Easy Wrap chains into reusable systems Subclassing Advanced Custom loops, evolving state, or complex logic <pre><code>flowchart TD\n    A[Start] --&gt; B{Do you need a chain without branch or logic}\n    B --&gt;|Yes| C[Use Sequential API]\n    B --&gt;|No| D{Do you need only, chain, branching and logic}\n    D --&gt;|Yes| E{Do you need to encapculate it into a bigger program}\n    D --&gt;|No| F[Use Subclassing strategy]\n    E --&gt;|Yes| G[Use Mixing strategy]\n    E --&gt;|No| H[Use Functional API]</code></pre> <ul> <li>Prefer Functional API when possible it is versatile, simplest &amp; fastest.</li> <li>Use Mixing strategy when packaging functional logic into reusable components.</li> <li>Use Subclassing only for: Custom loops, validation logic beyond Pydantic constraints and evolving internal state inside the module.</li> </ul> <p>For more information on how to use each way, refer to the Program section of the documentation.</p>"},{"location":"CheatSheet/#how-to-execute-a-program","title":"How to execute a program?","text":"<pre><code>import synalinks\n\nclass UserQuery(synalinks.DataModel):\n    query: str\n\nasync def main():\n\n    # ... your program definition\n\n    result = await program(UserQuery(query=\"what is the capital of France?\"))\n</code></pre>"},{"location":"CheatSheet/#how-to-batch-the-program-execution","title":"How to batch the program execution?","text":"<p>You can batch the program execution to make inferences in parrallel (use asyncio under the hood) by using the <code>program.predict()</code> method. This method take as input a numpy array of data models.</p> <pre><code>import numpy as np\nimport synalinks\n\nbatch = np.array(\n    [\n        UserQuery(query=\"what is the capital of France?\"),\n        UserQuery(query=\"what is the capital of Germany?\"),\n        UserQuery(query=\"what is the capital of Italy?\"),\n    ]\n    dtype=\"object\",\n)\n\nasync def main():\n\n    # ... your program definition\n\n    results = await program.predict(batch)\n\n    for result in results:\n        print(result.prettify_json())\n</code></pre>"},{"location":"CheatSheet/#how-to-define-the-reward-and-optimizer-to-use-during-training","title":"How to define the reward and optimizer to use during training?","text":"<p>To define the reward and optimizer to use during training, use the <code>program.compile()</code> method.</p> <pre><code>import synalinks\n\n\nasync def main():\n\n    # ... your program definition\n\n    program.compile(\n        reward=synalinks.rewards.ExactMath()\n        optimizer=synalinks.optimizers.OMEGA(\n            language_model=language_model,\n            embedding_model=embedding_model,\n        )\n        metrics=[\n            synalinks.metrics.F1Score()\n        ]\n    )\n\n    history = await program.fit(...)\n</code></pre>"},{"location":"CheatSheet/#how-to-evaluate-a-program","title":"How to evaluate a program?","text":"<p>To evaluate a program, you can use the <code>program.evaluate()</code> method, after compiling your program.</p> <pre><code>import synalinks\n\nasync def main():\n\n    # ... your program definition\n\n    program.compile(...)\n\n    metrics = program.evaluate(x=x_train, y=y_train)\n</code></pre>"},{"location":"CheatSheet/#how-to-train-my-program","title":"How to train my program?","text":"<p>You can train your program using the <code>program.fit()</code> method, after compiling you program.</p> <pre><code>import synalinks\n\nasync def main():\n\n    # ...\n    # Your program definition\n\n    program.compile(...)\n\n    history = await program.fit(\n        x=x_train,\n        y=y_train,\n        validation_split=0.2,\n        batch_size=1,\n        epochs=5,\n    )\n</code></pre>"},{"location":"CheatSheet/#how-to-use-the-concatenation-operation","title":"How to use the concatenation operation?","text":"<pre><code>import synalinks\n\nsynalinks.enable_logging()\n\nclass Query(synalinks.DataModel):\n    query: str\n\n\nclass Answer(synalinks.DataModel):\n    answer: str\n\n\n# Synalinks operators works at a metaclass level\n# In that case, the result is a `SymbolicDataModel`\n# A `SymbolicDataModel` can be understand as a data\n# specification/contract. It only contains a JSON schema\n# and cannot be used for computation. It allow Synalinks\n# to build directed acyclic graph (DAG) of computation\n# from inputs and outputs, like the tensor shape\n# in deep learning frameworks.\n\nqa_pair = Query + Answer\n\nassert isinstance(qa_pair, synalinks.SymbolicDataModel)\n\nprint(qa_pair.prettify_schema())\n# {\n#   \"additionalProperties\": false,\n#   \"properties\": {\n#     \"query\": {\n#       \"title\": \"Query\",\n#       \"type\": \"string\"\n#     },\n#     \"answer\": {\n#       \"title\": \"Answer\",\n#       \"type\": \"string\"\n#     }\n#   },\n#   \"required\": [\n#     \"query\",\n#     \"answer\"\n#   ],\n#   \"title\": \"Query\",\n#   \"type\": \"object\"\n# }\n\n# Once we concatenate two instanciated data models, the result\n# is a JsonDataModel, a data model containing both a JSON schema and\n# a JSON object containing the actual data.\n\nqa_pair = Query(query=\"What is the French city of aeronautics and robotics?\") + Answer(\n    answer=\"Toulouse\"\n)\n\nassert isinstance(qa_pair, synalinks.JsonDataModel)\n\nprint(qa_pair.prettify_json())\n# {\n#   \"query\": \"What is the French city of aeronautics and robotics?\",\n#   \"answer\": \"Toulouse\"\n# }\n</code></pre> <p>What happen if you concatenate two data models with the same fields? When property names conflict, numerical suffixes are added to ensure uniqueness.</p> <pre><code>two_queries = Query + Query\n\nprint(two_queries.prettify_schema())\n\n# {\n#   \"additionalProperties\": false,\n#   \"properties\": {\n#     \"query\": {\n#       \"title\": \"Query\",\n#       \"type\": \"string\"\n#     },\n#     \"query_1\": {\n#       \"title\": \"Query 1\",\n#       \"type\": \"string\"\n#     }\n#   },\n#   \"required\": [\n#     \"query\",\n#     \"query_1\"\n#   ],\n#   \"title\": \"Query\",\n#   \"type\": \"object\"\n# }\n\ntwo_queries = Query(\n    query=\"Why is neuro-symbolic systems powering the next AI wave?\"\n) + Query(query=\"Can you give a multiple of 5?\")\n\n\nprint(two_queries.prettify_json())\n# {\n#   \"query\": \"Why is neuro-symbolic systems powering the next AI wave?\",\n#   \"query_1\": \"Can you give a multiple of 5?\"\n# }\n</code></pre> <p>Now, what happen when you concatenate with <code>None</code>? An exception is raised!</p> <pre><code>failing_query = Query(query=\"Why is neuro-symbolic AI powering the next wave?\") + None\n# ValueError: Received x1=query='Why is neuro-symbolic AI powering the next wave?' and x2=None\n</code></pre> <p>This behavior can be summarized with the following truth table:</p> <code>x1</code> <code>x2</code> Concat (<code>+</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>Exception</code> <code>None</code> <code>x2</code> <code>Exception</code> <code>None</code> <code>None</code> <code>Exception</code>"},{"location":"CheatSheet/#how-to-use-the-logical-and-operation","title":"How to use the logical and operation?","text":"<pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str\n\n\nclass Answer(synalinks.DataModel):\n    answer: str\n\n\nqa_pair = Query &amp; Answer\n\nassert isinstance(qa_pair, synalinks.SymbolicDataModel)\n\nprint(qa_pair.prettify_schema())\n# {\n#   \"additionalProperties\": false,\n#   \"properties\": {\n#     \"query\": {\n#       \"title\": \"Query\",\n#       \"type\": \"string\"\n#     },\n#     \"answer\": {\n#       \"title\": \"Answer\",\n#       \"type\": \"string\"\n#     }\n#   },\n#   \"required\": [\n#     \"query\",\n#     \"answer\"\n#   ],\n#   \"title\": \"Query\",\n#   \"type\": \"object\"\n# }\n\n# When performing an And operation with `None` the output is `None`\n# You can see the logical And as a robust concatenation operation.\n\nqa_pair = Query(query=\"Why is neuro-symbolic AI powering the next wave?\") &amp; None\n\nassert isinstance(qa_pair, None)\n</code></pre> <p>Here is the table summarizing the behavior:</p> <code>x1</code> <code>x2</code> Logical And (<code>&amp;</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>None</code> <code>None</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <code>None</code>"},{"location":"CheatSheet/#how-to-use-the-logical-or-operation","title":"How to use the logical or operation?","text":"<pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str\n\n\nclass Answer(synalinks.DataModel):\n    answer: str\n\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str\n    answer: str\n</code></pre> <p>When a two data models are provided, the logical or perform a concatenation of the two data models. However when given a <code>None</code>, it ignore it to give you the one that isn't None.</p> <p>This behavior can be summarized in the following truth table:</p> <p>Truth Table:</p> <code>x1</code> <code>x2</code> Logical Or (<code>|</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>x1</code> <code>None</code> <code>x2</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <pre><code>answer = Answer(answer=\"Toulouse\") | None\n\nprint(answer.prettify_json())\n# {\n#   \"answer\": \"Toulouse\"\n# }\n\nanswer = None | AnswerWithThinking(\n    thinking=(\n        \"LAAS CNRS (Laboratoire d'Analyse et d'Architecture des Syst\u00e8mes) is located in \"\n        \"Toulouse and is renowned for its research in robotics.\"\n        \" Toulouse is also widely recognized as a central hub for aeronautics and\"\n        \" space in Europe. It houses the headquarters of Airbus and several \"\n        \"important aerospace research centers\"\n    ),\n    answer=\"Toulouse\",\n)\n\nprint(answer.prettify_json())\n# {\n#   \"thinking\": \"LAAS CNRS (Laboratoire d'Analyse et d'Architecture des\n# Syst\\u00e8mes) is located in Toulouse and is renowned for its research\n# in robotics. Toulouse is also widely recognized as a central hub for\n# aeronautics and space in Europe. It houses the headquarters of Airbus\n# and several important aerospace research centers.\",\n#   \"answer\": \"Toulouse\"\n# }\n</code></pre> <p>Why is that useful ? Let's explain it with an example, imagine you want an adaptative system that is able to answer shortly, or take more time to \"think\" before answering depending on the question difficulty.</p> <pre><code>async def main():\n    language_model = synalinks.LanguageModel(model=\"ollama/mistral\")\n\n    inputs = synalinks.Input(data_model=Query)\n    answer_without_thinking, answer_with_thinking = await synalinks.Branch(\n        question=\"Evaluate the difficulty of the query\",\n        labels=[\"easy\", \"difficult\"],\n        branches=[\n            synalinks.Generator(\n                data_model=Answer,\n                language_model=language_model,\n            ),\n            synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=language_model,\n            ),\n        ],\n        language_model=language_model,\n        # We can optionally return the decision,\n        # in Synalinks there is no black-box component!\n        # Every LM inference, can be returned\n        # for evaluation or explainability\n        return_decision=False,\n    )(inputs)\n\n    # The outputs is the answer without thinking OR the answer with thinking\n    outputs = answer_without_thinking | answer_with_thinking\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"adaptative_qa\",\n        description=\"A program that take the time to think if the query is difficult to answer\",\n    )\n\n    answer = await program(\n        Query(query=\"What is French city of robotics and aeronautics?\")\n    )\n\n    print(answer.prettify_json())\n\n# {\n#   \"thinking\": \"The answer to the given query involves finding a city in\n# France that is known for robotics and aeronautics. While there might be\n# several cities that have significant presence in these fields, Toulouse\n# is one of the most renowned due to the presence of well-established\n# institutions like EADS (European Aeronautic Defence and Space Company),\n# IRIT (Institut de Recherche en Informatique pour le Traitement Automatique des Images)\n# and LAAS CNRS (Laboratoire d'Analyse et d'Architecture des Syst\\u00e8mes).\",\n#   \"answer\": \"Toulouse\"\n# }\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"CheatSheet/#how-to-create-custom-rewards","title":"How to create custom rewards?","text":"<pre><code>import synalinks\n\n@synalinks.saving.register_synalinks_serializable()\nasync def my_custom_reward(y_true, y_pred):\n    # ...\n    # your custom logic, should return a float between 0.0 and 1.0\n    return 1.0\n\nasync def main():\n\n    # ...\n    # Your program definition\n\n    program.compile(\n        reward=synalinks.rewards.MeanRewardWrapper(\n            fn=my_custom_reward,\n        )\n        optimizer=...\n    )\n\n    history = await program.fit(...)\n</code></pre>"},{"location":"CheatSheet/#how-to-create-custom-metrics","title":"How to create custom metrics?","text":"<pre><code>import synalinks\n\n@synalinks.saving.register_synalinks_serializable()\nasync def my_custom_metric(y_true, y_pred):\n    # ...\n    # my custom logic\n    # your custom logic, should return a float usually between 0.0 and 1.0 (not always e.g. cost)\n    return 1.0\n\n\nasync def main():\n\n    # ...\n    # Your program definition\n\n    program.compile(\n        reward=synalinks.rewards.MeanMetricWrapper(\n            fn=my_custom_reward,\n        )\n        optimizer=...\n    )\n\n    history = await program.fit(...)\n</code></pre>"},{"location":"CheatSheet/#how-to-use-the-chainofthought","title":"How to use the ChainOfThought?","text":"<pre><code>import synalinks\n\nclass UserQuery(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass Answer(synalinks.DataModel):\n    answer: str = syalinks.Field(\n        descriptino=\"The final answer\",\n    )\n\n\nasync def main():\n\n    inputs = synalinks.Input(\n        data_model=UserQuery,\n    )\n\n    outputs = synalinks.ChainOfThought(\n        data_model=Answer,\n        language_model=language_model,\n    )(inputs)\n\n    program = Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"chain_of_thought_answer\",\n        description=\"A program using chain of thought to answer\",\n    )\n</code></pre>"},{"location":"CheatSheet/#how-to-use-the-selfconsistency","title":"How to use the SelfConsistency?","text":"<p>SelfConsistency can be described as a functional program.  This technique can be quite expensive at inference time, so check first that a ChainOfThought + optimization don't yield the expected results.</p> <pre><code>import synalinks\n\nclass UserQuery(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass Answer(synalinks.DataModel):\n    answer: str = syalinks.Field(\n        descriptino=\"The final answer\",\n    )\n\nasync def main():\n\n    inputs = synalinks.Input(data_model=UserQuery)\n\n    x0 = synalinks.Generator(\n        data_model=Answer,\n        language_model=language_model,\n        return_inputs=False,\n    )(inputs)\n    x1 = synalinks.Generator(\n        data_model=Answer,\n        language_model=language_model,\n        return_inputs=False,\n    )(inputs)\n\n    x2 = inputs &amp; x1 &amp; x2 # concatenate the inputs with each answer (robust to provider failure)\n\n    # This stage (optional) critique the two previous answers\n    x3 = synalinks.SelfCritique(\n        return_reward=False,\n        language_model=language_model,\n        return_inputs=True,\n    )(x2)\n\n    # Compute the final answer based on the previous answers and critique \n    outputs = synalinks.Generator(\n        data_model=Answer,\n        language_model=language_model,\n        return_inputs=True,\n    )(x3)\n\n    program = Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"self_consistency\",\n        description=\"A self-consistency program\"\n    )\n</code></pre>"},{"location":"Differences%20with%20DSPy/","title":"Differences with DSPy","text":""},{"location":"Differences%20with%20DSPy/#differences-with-dspy","title":"Differences with DSPy","text":""},{"location":"Differences%20with%20DSPy/#a-complete-guide","title":"A Complete Guide","text":"<p>This document highlights the key differences between DSPy and Synalinks. While both frameworks enable in-context learning and neuro-symbolic programming, they differ significantly in design philosophy, reliability, and production readiness. We assume you have a basic understanding of in-context learning frameworks to be able to compare them.</p>"},{"location":"Differences%20with%20DSPy/#fundamental-differences","title":"Fundamental Differences","text":""},{"location":"Differences%20with%20DSPy/#dspy-pytorch-inspired","title":"DSPy: PyTorch-Inspired","text":"<ul> <li>Purpose: Build intelligent applications combining LMs with symbolic reasoning.</li> <li>Memory: Natively supports vector-only databases.</li> <li>Reliability: DSPy relies on brittle parsing logic in <code>Adapter</code> classes. While optimization reduces errors, exceptions due to LM output format failures remain common in production. Also, still DSPy rely on pipelines exceptions and formatting failures to optimize internal modules, which is higly problematic in any production settings.</li> <li>Async: Offers both async and sync code, which can lead to inconsistent practices in production environments.</li> <li>String Variables: Like TextGrad and other in-context learning frameworks, DSPy represents variables as strings. This limits the ability to handle complex structured variables (e.g., graphs, plans), making them less suitable for advanced neuro-symbolic systems that require learning to plan or structure working memory.</li> </ul>"},{"location":"Differences%20with%20DSPy/#synalinks-keras-inspired","title":"Synalinks: Keras-Inspired","text":"<ul> <li>Purpose: Build intelligent applications combining LMs with symbolic reasoning.</li> <li>Memory: Natively supports vector databases for flexible memory structures.</li> <li>Reliability: Uses constrained structured output by default, eliminating brittle parsing and ensuring robust, predictable behavior whith 100% success output formatting.</li> <li>Async: Async by default, enforcing production-ready practices and consistent performance.</li> <li>Strict Module Typing: Modules in Synalinks are strictly typed using JSON schemas (defined in <code>compute_output_spec()</code>). This allows the system to compute output contracts end-to-end before any computation, ensuring type safety and clarity.</li> <li>JSON Variables: Variables are JSON objects with associated schemas. The optimizer uses constrained structured output to guarantee 100% correct variable structure, enabling complex, nested, and graph-like data handling.</li> <li>Arithmetic &amp; Logical Operators: Implements JSON-level concatenation and logical operators (OR, AND, XOR) via Python operators, allowing rapid architecture changes without additional class implementations.</li> <li>Robust Branching and Merging: Dynamically creates schemas on the fly for branching, and handles merging via JSON operators. This enables complex workflows without requiring custom classes.</li> <li>Observable by Default: Every LM call within a Module can be returned, allowing reward computation based on internal processes, not just outputs, enabling finer-grained optimization and debugging.</li> </ul>"},{"location":"Differences%20with%20DSPy/#key-concept-mapping","title":"Key Concept Mapping","text":"DSPy Concept Synalinks Equivalent Key Difference <code>Adapter</code> - No brittle parsing; uses JSON schemas for robust I/O <code>GEPA</code> <code>OMEGA</code> Use a SOTA algorithm (2025) in evolutionary AI instead of a 10 years old method String-based variables JSON-based variables Supports complex structures (graphs, plans) and strict validation Sync/Async choice Async by default Enforces production best practices Vector-only memory Vector database memory Flexible memory structures with DuckDB Custom branching logic JSON operators for branching Dynamic schema creation and merging; no need for custom classes Limited observability Observable by default Full visibility into LM calls for reward computation and debugging"},{"location":"Differences%20with%20DSPy/#when-to-use-each-framework","title":"When to Use Each Framework","text":""},{"location":"Differences%20with%20DSPy/#use-dspy-when","title":"Use DSPy when:","text":"<ul> <li>You are in a research environment and need rapid prototyping.</li> <li>Your use case is simple and does not require complex structured variables.</li> <li>You prefer the flexibility of choosing between sync and async code.</li> <li>You are comfortable managing parsing logic and potential LM output format issues (and don't mind about failures in production).</li> </ul>"},{"location":"Differences%20with%20DSPy/#use-synalinks-when","title":"Use Synalinks when:","text":"<ul> <li>You need a production-ready, reliable system with robust error handling.</li> <li>Your application requires complex structured variables (e.g., graphs, plans).</li> <li>You want strict typing and end-to-end contract validation.</li> <li>You need flexible vector memory with DuckDB for data relationships.</li> <li>You want to observe and optimize internal LM processes, not just outputs.</li> <li>You need to rapidly change architectures using built-in JSON operators.</li> </ul>"},{"location":"Differences%20with%20DSPy/#summary","title":"Summary","text":"<p>While DSPy is a powerful research tool inspired by PyTorch\u2019s flexibility, Synalinks is designed for production use, inspired by Keras user-friendliness and reliability. Synalinks use of JSON schemas, strict typing, async-by-default design, robust branching/merging makes it ideal for building complex, reliable neuro-symbolic systems that can learn, plan, and reason with structured data.</p>"},{"location":"Differences%20with%20Keras/","title":"Differences with Keras","text":""},{"location":"Differences%20with%20Keras/#differences-with-keras","title":"Differences with Keras","text":""},{"location":"Differences%20with%20Keras/#a-complete-guide","title":"A Complete Guide","text":"<p>This document provides a comprehensive guide for translating Keras concepts into Synalinks. While Keras is designed for building traditional neural networks with tensor operations, Synalinks is a framework for creating neuro-symbolic programs that combine language models with structured reasoning.</p>"},{"location":"Differences%20with%20Keras/#fundamental-paradigm-shift","title":"Fundamental Paradigm Shift","text":""},{"location":"Differences%20with%20Keras/#keras-neural-network-framework","title":"Keras: Neural Network Framework","text":"<ul> <li>Purpose: Build and train deep neural networks using tensor operations</li> <li>Core abstraction: Mathematical tensors flowing through differentiable layers</li> <li>Training: Gradient-based optimization (backpropagation)</li> <li>Computation: Matrix multiplications and activation functions</li> <li>Use cases: Computer vision, time series, traditional ML tasks</li> </ul>"},{"location":"Differences%20with%20Keras/#synalinks-neuro-symbolic-lm-framework","title":"Synalinks: Neuro-Symbolic LM Framework","text":"<ul> <li>Purpose: Build intelligent applications combining LMs with symbolic reasoning</li> <li>Core abstraction: Structured JSON data flowing through modular programs</li> <li>Training: Reinforcement learning with LM-based optimizers</li> <li>Computation: Language model inference + symbolic operations</li> <li>Use cases: AI agents, reasoning systems, structured generation, API orchestration</li> </ul>"},{"location":"Differences%20with%20Keras/#quick-concept-mapping","title":"Quick Concept Mapping","text":"Keras Concept Synalinks Equivalent Key Difference <code>Layer</code> <code>Module</code> Processes JSON instead of tensors <code>Model</code> <code>Program</code> DAG of modules with conditional logic <code>Tensor</code> <code>Data Model</code> JSON object with schema validation Tensor shape JSON schema Explicit structure definition Weights/biases <code>Trainable Variable</code> JSON objects, not floating-point arrays Loss function Reward function Maximize reward vs minimize loss Backpropagation LM-based optimization No gradients; uses language model reasoning <code>model.compile()</code> <code>program.compile()</code> Sets up LM optimizer instead of SGD/Adam <code>model.fit()</code> <code>program.fit()</code> Reinforcement learning loop"},{"location":"Differences%20with%20Keras/#core-concepts-explained","title":"Core Concepts Explained","text":""},{"location":"Differences%20with%20Keras/#module-layer-equivalent","title":"Module (Layer Equivalent)","text":"<p>A Module is a self-contained computational unit that: - Receives: JSON Data Models conforming to input schemas - Processes: Via LM calls, symbolic operations, or hybrid approaches - Outputs: JSON Data Models conforming to output schemas</p>"},{"location":"Differences%20with%20Keras/#key-module-properties","title":"Key Module Properties:","text":"<ul> <li>Keras Layer: Receives tensors, performs matrix operations, outputs tensors</li> <li>Synalinks Module: Receives JSON, performs LM/symbolic operations, outputs JSON with schema validation</li> </ul>"},{"location":"Differences%20with%20Keras/#program-model-equivalent","title":"Program (Model Equivalent)","text":"<p>A Program orchestrates multiple Modules into a directed acyclic graph (DAG):</p>"},{"location":"Differences%20with%20Keras/#keras-model-vs-synalinks-program","title":"Keras Model vs Synalinks Program:","text":"<ul> <li>Keras: Fixed computation graph of tensor operations</li> <li>Synalinks: Dynamic graph with conditional branching based on LM decisions</li> </ul> <p>In Keras, you build sequential or functional models with fixed layer connections. In Synalinks, you create programs that can include conditional branches, where different modules execute based on LM-driven decisions or data conditions.</p>"},{"location":"Differences%20with%20Keras/#data-models-json-schemas","title":"Data Models &amp; JSON Schemas","text":"<p>Instead of implicit tensor shapes, Synalinks uses explicit JSON schemas:</p> <ul> <li>Keras: Input shape defined as dimensions (e.g., 784-dimensional vector)</li> <li>Synalinks: Input structure defined as JSON schema with explicit fields, types, and validation rules</li> </ul> <p>Why JSON? - Interpretability: Human-readable intermediate states - Validation: Built-in schema validation - Interoperability: Native compatibility with APIs and web services - Debugging: Easy to inspect and modify - Structured generation: Natural fit for LM constrained output</p>"},{"location":"Differences%20with%20Keras/#training-paradigm-differences","title":"Training Paradigm Differences","text":""},{"location":"Differences%20with%20Keras/#keras-gradient-descent","title":"Keras: Gradient Descent","text":"<ul> <li>Defines differentiable loss function (e.g., categorical crossentropy)</li> <li>Computes gradients via automatic differentiation (backpropagation)</li> <li>Updates weights using gradient information through optimizers like Adam or SGD</li> <li>Requires continuous, differentiable operations throughout</li> </ul>"},{"location":"Differences%20with%20Keras/#synalinks-lm-based-optimization","title":"Synalinks: LM-Based Optimization","text":"<ul> <li>Defines reward function (higher values indicate better performance)</li> <li>No gradient computation\u2014uses language models to reason about improvements</li> <li>LM analyzes current performance and proposes better configurations</li> <li>Updates trainable variables (JSON objects) based on LM suggestions</li> <li>Can incorporate discrete decisions and non-differentiable operations</li> </ul>"},{"location":"Differences%20with%20Keras/#key-training-differences","title":"Key Training Differences:","text":"Aspect Keras Synalinks Objective Minimize loss Maximize reward Optimization Gradient descent Reinforcement learning Update mechanism Mathematical derivatives LM-generated improvements Trainable params Float tensors Structured JSON objects"},{"location":"Differences%20with%20Keras/#when-to-use-each-framework","title":"When to Use Each Framework","text":""},{"location":"Differences%20with%20Keras/#use-keras-when","title":"Use Keras when:","text":"<ul> <li>Working with numerical data (images, signals, time series)</li> <li>Need fast, vectorized computations</li> <li>Have large labeled datasets</li> <li>Problem has clear differentiable objectives</li> <li>Deploying to edge devices with limited resources</li> </ul>"},{"location":"Differences%20with%20Keras/#use-synalinks-when","title":"Use Synalinks when:","text":"<ul> <li>Building LM-powered applications</li> <li>Need symbolic reasoning and logic</li> <li>Working with structured/semi-structured text data</li> <li>Require interpretable intermediate steps</li> <li>Building API orchestration or agent systems</li> <li>Need adaptive, context-aware processing</li> <li>Want to combine multiple LMs and tools</li> </ul>"},{"location":"Differences%20with%20Keras/#summary","title":"Summary","text":"<p>While Keras excels at building traditional neural networks with tensor operations and gradient-based training, Synalinks is designed for a fundamentally different purpose: creating neuro-symbolic applications that combine the reasoning capabilities of language models with structured, validated data processing.</p> <p>The shift from tensors to JSON, from gradients to LM-based optimization, and from fixed architectures to adaptive programs represents not just a technical change, but a paradigm shift in how we think about building intelligent systems. Synalinks is ideal when you need interpretability, flexibility, and the ability to integrate language understanding with symbolic reasoning, making it perfect for next-generation AI applications.</p>"},{"location":"FAQ/","title":"FAQ","text":""},{"location":"FAQ/#faq","title":"FAQ","text":""},{"location":"FAQ/#general-questions","title":"General questions","text":"<ul> <li>What makes Synalinks revolutionary compared to DSPy?</li> <li>Why do you focus on in-context techniques?</li> <li>I already use structured output, why would I use Synalinks?</li> <li>Can Synalinks be used for non-LMs applications</li> <li>What are my options for saving programs?</li> <li>How to do hyperparameter tuning with Synalinks?</li> <li>Where is the Synalinks configuration file stored?</li> <li>How should I cite Synalinks?</li> <li>Do you provide help or support?</li> </ul>"},{"location":"FAQ/#training-related-questions","title":"Training related questions","text":"<ul> <li>What do \"sample\", \"batch\", and \"epoch\" mean?</li> <li>What's the difference between the <code>training</code> argument in <code>call()</code> and the <code>trainable</code> attribute?</li> <li>In <code>fit()</code>, how is the validation split computed?</li> <li>In <code>fit()</code>, is the data shuffled during training?</li> <li>What's the difference between Program methods <code>predict()</code> and <code>__call__()</code>?</li> </ul>"},{"location":"FAQ/#what-makes-synalinks-revolutionary-compared-to-dspy","title":"What makes Synalinks revolutionary compared to DSPy?","text":"<p>While DSPy wrestles with PyTorch complexity, Synalinks delivers the elegant simplicity of Keras with enterprise-grade power. We're the only framework featuring logical flows inspired by logical circuits and parrallel function calling agents. Synalinks transforms AI workflow design into an intuitive, natural process that accelerates development cycles and reduces implementation complexity.</p>"},{"location":"FAQ/#why-do-you-focus-on-in-context-techniques","title":"Why do you focus on in-context techniques?","text":"<p>In the real-world, most problems that companies face, are not labelled in public datasets for ML engineers to train their networks against. Meaning that their is a big discrepency between the results annonced on public benchmarks and real-world problems. Making adoption difficult when companies face the reality and complexity of real-world.</p> <p>Training a whole Language Model (LM) from scratch is out of the reach of most companies, so adapting them to real-world tasks is essential.</p> <p>LMs have the capability to leverage their prompt to mimick the examples given, but it means that one have to update the examples/prompts each time you change the pipelines as you experiment. Making it cumberstone, but even with that, their is no guaranty that the examples/prompts you gave yield to the best results.</p> <p>To select the best examples and instructions to give to the LMs, it needs a complex system like Synalinks that automate the generation and selection.</p>"},{"location":"FAQ/#i-already-use-structured-output-why-would-i-use-synalinks","title":"I already use structured output, why would I use Synalinks?","text":"<p>While structured output ensure a correct format, ready to parse, it doesn't guaranty the content of the LMs answers. Synalinks use constrained structured output in conjunction with in-context techniques to ensure both format and content correctness.</p>"},{"location":"FAQ/#can-synalinks-be-used-for-non-lms-applications","title":"Can Synalinks be used for non-LMs applications?","text":"<p>While Synalinks provide everything to work with LMs, we emphasize that you can create modules (or entire pipelines) that don't use them. In fact, many neuro-symbolic systems use a conjunction of LMs modules with non-LMs modules. Synalinks backend can suit any algorithm that works with any kind of data-structure as long as they are formalized in JSON.</p>"},{"location":"FAQ/#what-are-my-options-for-saving-programs","title":"What are my options for saving programs?","text":"<p>1. Whole-program saving (configuration + variables)</p> <p>Whole program saving means creating a file that will contain:</p> <ul> <li>The architecture of the program, allowing you to re-create the program.</li> <li>The variables of the program</li> <li>The training configuration (reward, optimizer, metrics)</li> <li>The state of the optimizer, allowing you to resume the training exactly where you left off.</li> </ul> <p>The default and recommended way to save the whole program is to do:</p> <p><code>program.save(\"my_program.json\")</code></p> <p>After saving a program you can re-instanciate it via:</p> <p><code>program = synalinks.Program.load(\"my_program.json\")</code></p> <p>2. Variables-only saving</p> <p>If you need to save the variables of a program, you can it using:</p> <p><code>program.save_variables(\"my_program.variables.json\")</code></p> <p>You can then load the variables into a program with the same architecture:</p> <p><code>program.load_variables(\"my_program.variables.json\")</code></p> <p>Note: All programs and/or variables are saved in JSON format, we removed the pickle format for obvious security concerns.</p>"},{"location":"FAQ/#how-to-do-hyperparameter-tuning-with-synalinks","title":"How to do hyperparameter tuning with Synalinks?","text":"<p>Synalinks is compatible with KerasTuner which allows you to find hyperparameters.</p>"},{"location":"FAQ/#where-is-the-synalinks-configuration-file-stored","title":"Where is the Synalinks configuration file stored?","text":"<p>The default directory where all Synalinks data is stored is:</p> <pre><code>$HOME/.synalinks/\n</code></pre> <p>Note that Windows users should replace <code>$HOME</code> with <code>%USERPROFILE%</code>.</p> <p>In case Synalinks cannot create the above directory (e.g. due to permission issues), <code>/tmp/.synalinks/</code> is used as a backup.</p> <p>The Synalinks configuration file is a JSON file stored at $HOME/.synalinks/synalinks.json. The default configuration file looks like this:</p> <pre><code>{\n    \"backend\": \"pydantic\",\n    \"floatx\": \"float32\",\n    \"epsilon\": 1e-07\n}\n</code></pre> <p>Likewise, cached dataset files, such as those downloaded with <code>get_file()</code>, are stored by default in <code>$HOME/.synalinks/datasets/</code>.</p>"},{"location":"FAQ/#how-should-i-cite-synalinks","title":"How should I cite Synalinks?","text":"<p>Please cite Synalinks if it is useful in your research. Here is the bibtex entry to use:</p> <pre><code>@misc{sallami2025synalinks,\n  title={Synalinks},\n  author={Sallami, Yoan and Chollet, Fran\\c{c}ois},\n  year={2025},\n  howpublished={\\url{https://github.com/SynaLinks/Synalinks}},\n}\n</code></pre>"},{"location":"FAQ/#do-you-provide-help-or-support","title":"Do you provide help or support?","text":"<p>We provide consulting, development and technical support for companies that want to implement any neuro-symbolic systems. Using a framework is one thing, having a complete view of possible neuro-symbolic applications and the knowledge to create such complex systems is another. If you can't afford our services, you can find help in our public Discord channel.</p>"},{"location":"FAQ/#what-do-sample-batch-and-epoch-mean","title":"What do \"sample\", \"batch\", and \"epoch\" mean?","text":"<ul> <li>Sample: A sample is one element of a dataset. For example, one DataModel is one sample.</li> <li>Batch: A batch is a set of N samples. The samples in a batch are processed independently, in parallel. During training, a batch result in only one program update. A batch approximates the input distribution better than a single input. The larger the batch, the better the approximation; however a larger batch will take longer to process and still result in only one update.</li> <li>Epochs: A epochs is an arbitrarly cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. When using <code>validation_split</code> or <code>validation_data</code> with the <code>fit</code> method of Synalinks programs, evaluation will be run at the end of every epoch.</li> </ul>"},{"location":"FAQ/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute","title":"What's the difference between the <code>training</code> argument in <code>call()</code> and the <code>trainable</code> attribute?","text":"<p><code>training</code> is a boolean argument in <code>call</code> that determines whether the call should be run in inference mode or training mode. For example, in training mode, a <code>Generator</code> module save each LM prediction for later backpropagation. In inference mode, the <code>Generator</code> doesn't save the predictions.</p> <p><code>trainable</code> is a boolean module attribute that determines the trainable variables of the module should be updated to maximize the reward during training. If <code>module.trainable</code> is set to False, then <code>module.trainable_variables</code> will always be an empty list. </p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    class ThinkingWithAnswer(synalinks.DataModel):\n        thinking: str\n        answer: str\n\n    language_model = synalinks.LanguageModel(\n        \"ollama_chat/deepseek-r1\",\n    )\n\n    program = synalinks.Sequential(\n        [\n            synalinks.Generator(\n                data_model=ThinkingWithAnswer,\n                language_model=language_model,\n            ),\n            synalinks.Generator(\n                data_model=ThinkingWithAnswer,\n                language_model=language_model,\n            ),\n        ]\n    )\n\n    program.modules[0].trainable = False # Freeze the first generator\n\n    assert program.modules[0].trainable_variables == []\n\n    program.compile(...)\n    history = await program.fit(...) # Train only the second generator\n\nif __main__ == \"__name__\":\n    asyncio.run(main())\n</code></pre> <p>In essence, \"inference mode vs training mode\" and \"module variable trainability\" are two very different concepts.</p>"},{"location":"FAQ/#in-fit-how-is-the-validation-split-computed","title":"In <code>fit()</code>, how is the validation split computed?","text":"<p>If you set the <code>validation_split</code> argument in <code>program.fit</code> to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed.</p> <p>The same validation set is used for all epochs (within the same call to fit).</p> <p>Note that the validation_split option is only available if your data is passed as Numpy arrays.</p>"},{"location":"FAQ/#in-fit-is-the-data-shuffled-during-training","title":"In <code>fit()</code>, is the data shuffled during training?","text":"<p>If you pass your data as NumPy arrays and if the <code>shuffle</code> argument in <code>program.fit()</code> is set to True (which is the default), the training data will be globally randomly shuffled at each epoch.</p> <p>Validation data is never shuffled.</p>"},{"location":"FAQ/#whats-the-difference-between-program-methods-predict-and-__call__","title":"What's the difference between <code>Program</code> methods <code>predict()</code> and <code>__call__()</code>?","text":"<p><code>predict()</code> loops over the data in batches (you can specify the batch size via <code>predict(x, batch_size=64)</code>) and returns a Numpy array of predictions.</p> <p>While <code>program(x)</code> perform a single prediction, and is used to create APIs that process a single user request at a time.</p> <p>This means that <code>predict()</code> calls can perform prediction on a dataset. While <code>program(x)</code> perform a single prediction.</p>"},{"location":"Introduction/","title":"Introduction","text":""},{"location":"Introduction/#introduction","title":"Introduction","text":""},{"location":"Introduction/#what-is-synalinks","title":"What is Synalinks?","text":"<p>Synalinks is an open-source framework that makes it easy to create, evaluate, train, and deploy industry-standard Language Models (LMs) applications. Synalinks follows the principle of progressive disclosure of complexity: meaning that simple workflows should be quick and easy, while arbitrarily advanced ones should be possible via a clear path that builds upon what you've already learned.</p> <p>Synalinks is an adaptation of Keras 3 focused on neuro-symbolic systems and in-context reinforcement learning, an ensemble of techniques that enhance the LMs predictions and accuracy without changing the weights of the model. The goal of Synalinks is to facilitate the rapid setup of simple applications while providing the flexibility for researchers and advanced users to develop sophisticated systems.</p> <p>Info</p> <p>Too busy to read the documentation? Give the llms.txt or llms-full.txt to you favorite LMs or AI coding tools. Or better, use Synalinks Claude Skills with Claude Code to use Synalinks right away!</p>"},{"location":"Introduction/#who-is-synalinks-for","title":"Who is Synalinks for?","text":"<p>Synalinks is designed for a diverse range of users, from professionals and AI researchers to students, independent developers, and hobbyists. It is suitable for anyone who wants to learn about AI by building/composing blocks or build solid foundations for enterprise-grade products. While a background in Machine Learning and Deep Learning can be advantageous \u2014 as Synalinks leverages design patterns from Keras, one of the most user-friendly and popular Deep Learning frameworks \u2014 it is not a prerequisite. Synalinks is designed to be accessible to anyone with programming skills in Python, making it a versatile and inclusive platform for AI development.</p>"},{"location":"Introduction/#why-use-synalinks","title":"Why use Synalinks?","text":"<p>Developping a successful LM application in a profesional context, beyond stateless chatbots, is difficult and typically include:</p> <ul> <li>Building optimized prompts with examples/instructions at each step: Synalinks uses advanced In-Context Reinforcement Learning techniques to optimize each prompt.</li> <li>Pipelines that change over time: Easily edit your pipelines, re-run your training, and you're good to go.</li> <li>Ensuring the correctness of the LMs output: Synalinks combines constrained structured output with In-Context RL to ensure both format and content correctness.</li> <li>Async Optimization: Synalinks automatically optimizes your pipelines by detecting parallel processes.</li> <li>Assessing the performance of your application: Synalinks provides built-in metrics and rewards to evaluate your workflows.</li> <li>Configuring Language &amp; Embedding Models: Seamlessly integrate multiple LM providers like Ollama, OpenAI, Anthropic, Mistral or Groq.</li> <li>Documenting your ML workflows: Plot your workflows, training history, and evaluations; document everything.</li> <li>Versioning the prompts/pipelines: Each program is serializable into JSON so you can version it with git.</li> <li>Deploying REST APIs: Compatible out-of-the-box with FastAPI so your Data Scientists and Web Developers can stop tearing each other apart.</li> </ul> <p>Synalinks can help you simplify these tasks by leveraging decade old practices in Deep Learning frameworks. We provide a comprehensive suite of tools and features designed to streamline the development process, making it easier to create, evaluate, train, document and deploy robust neuro-symbolic LMs applications.</p>"},{"location":"Code%20Examples/Autonomous%20Agent/","title":"Autonomous Agent","text":""},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent--autonomous-agents","title":"Autonomous Agents","text":"<p>Autonomous agents represent a significant advancement in AI system design, combining the power of language models with the ability to perform tasks autonomously. This tutorial will guide you through building an autonomous agent using Synalinks, capable of processing mathematical queries and returning numerical answers.</p>"},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent--understanding-the-foundation","title":"Understanding the Foundation","text":"<p>Autonomous agents address a fundamental limitation of traditional systems by enabling them to perform tasks without constant human intervention. While language models excel at generating coherent text, they often require additional tools and logic to perform specific tasks autonomously. Autonomous agents bridge this gap by dynamically processing information and executing tasks based on predefined tools.</p> <p>The architecture of an autonomous agent follows several core stages:</p> <ul> <li>The input stage captures the user's query or command.</li> <li>The processing stage uses predefined tools and logic to process the input   and generate a response.</li> <li>The output stage returns the result to the user.</li> </ul> <pre><code>graph LR\n    A[Query] --&gt; B[Agent]\n    B --&gt; C{Need Tool?}\n    C --&gt;|Yes| D[Tool Call]\n    D --&gt; E[Tool Result]\n    E --&gt; B\n    C --&gt;|No| F[Final Answer]</code></pre> <p>Synalinks streamlines this complex process through its modular architecture, allowing you to compose components with precision while maintaining flexibility for different use cases.</p>"},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent--creating-an-autonomous-agent","title":"Creating an Autonomous Agent","text":"<p>Define tools as async functions with complete docstrings, then wrap them with <code>synalinks.Tool</code>:</p> <pre><code>@synalinks.utils.register_synalinks_serializable()\nasync def calculate(expression: str):\n    \"\"\"Calculate the result of a mathematical expression.\n\n    Args:\n        expression (str): The mathematical expression to calculate.\n    \"\"\"\n    result = eval(expression)\n    return {\"result\": result, \"log\": \"Successfully executed\"}\n\ntools = [synalinks.Tool(calculate)]\n</code></pre> <p>Important Tool Constraints:</p> <ul> <li> <p>No Optional Parameters: All parameters must be required. OpenAI and   other providers require all tool parameters to be required in JSON schemas.</p> </li> <li> <p>Complete Docstring Required: Every parameter must have a description   in the <code>Args:</code> section of the docstring. The Tool uses these to build the   JSON schema sent to the LLM.</p> </li> </ul> <p>Create the agent with <code>FunctionCallingAgent</code> in autonomous mode:</p> <pre><code>inputs = synalinks.Input(data_model=Query)\noutputs = await synalinks.FunctionCallingAgent(\n    data_model=NumericalFinalAnswer,\n    tools=tools,\n    language_model=language_model,\n    max_iterations=5,\n    return_inputs_with_trajectory=True,\n    autonomous=True,  # Agent runs until completion\n)(inputs)\n</code></pre>"},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Autonomous Task Execution: Autonomous agents solve the fundamental     problem of performing tasks without constant human intervention.</li> <li>Synalinks Modular Implementation: The framework simplifies the development     of autonomous agents through composable components like <code>FunctionCallingAgent</code>.</li> <li>Explicit Data Model Contracts: Using structured <code>Query</code> and output models     ensures type safety and predictable behavior.</li> <li>Tool Integration: Integrate tools like the calculate function into your     autonomous agent for processing specific types of queries.</li> <li>Dynamic Processing: Autonomous agents dynamically process information     and execute tasks based on predefined tools and logic.</li> </ul>"},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent--api-references","title":"API References","text":"<ul> <li>FunctionCallingAgent</li> <li>DataModel</li> <li>LanguageModel</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Autonomous%20Agent/#examples.10_autonomous_agent.calculate","title":"<code>calculate(expression)</code>  <code>async</code>","text":"<p>Calculate the result of a mathematical expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>The mathematical expression to calculate, such as '2 + 2'. The expression can contain numbers, operators (+, -, *, /), parentheses, and spaces.</p> required Source code in <code>examples/10_autonomous_agent.py</code> <pre><code>@synalinks.utils.register_synalinks_serializable()\nasync def calculate(expression: str):\n    \"\"\"Calculate the result of a mathematical expression.\n\n    Args:\n        expression (str): The mathematical expression to calculate, such as\n            '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n            parentheses, and spaces.\n    \"\"\"\n    if not all(char in \"0123456789+-*/(). \" for char in expression):\n        return {\n            \"result\": None,\n            \"log\": (\n                \"Error: invalid characters in expression. \"\n                \"The expression can only contain numbers, operators (+, -, *, /),\"\n                \" parentheses, and spaces NOT letters.\"\n            ),\n        }\n    try:\n        # Evaluate the mathematical expression safely\n        result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n        return {\n            \"result\": result,\n            \"log\": \"Successfully executed\",\n        }\n    except Exception as e:\n        return {\n            \"result\": None,\n            \"log\": f\"Error: {e}\",\n        }\n</code></pre>"},{"location":"Code%20Examples/Conditional%20Branches/","title":"Conditional Branches","text":""},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--conditional-branches","title":"Conditional Branches","text":"<p>In Lesson 3, you learned to make decisions. Now, let's use those decisions to create conditional branches - programs that take different paths based on the input.</p>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--why-conditional-branches","title":"Why Conditional Branches?","text":"<p>Real-world applications often need different processing for different cases:</p> <ul> <li>Simple questions \u2192 Quick answer</li> <li>Complex questions \u2192 Detailed reasoning</li> <li>Urgent tickets \u2192 Fast track processing</li> <li>Normal tickets \u2192 Standard queue</li> </ul>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--how-conditional-branches-work","title":"How Conditional Branches Work","text":"<p>The <code>Branch</code> module combines decision-making with conditional execution:</p> <pre><code>graph LR\n    Input --&gt; Decision\n    Decision --&gt;|easy| A[Easy Branch]\n    Decision --&gt;|difficult| B[Hard Branch]\n    A --&gt; Outputs\n    B --&gt; Outputs</code></pre> <p>Only ONE branch executes - the others output <code>None</code>.</p> <pre><code>(easy_result, hard_result) = await synalinks.Branch(\n    question=\"How complex is this query?\",\n    labels=[\"easy\", \"difficult\"],\n    branches=[\n        synalinks.Generator(data_model=SimpleAnswer, ...),    # For \"easy\"\n        synalinks.Generator(data_model=DetailedAnswer, ...),  # For \"difficult\"\n    ],\n    language_model=language_model,\n)(inputs)\n\n# If query was \"easy\": easy_result has data, hard_result is None\n# If query was \"difficult\": easy_result is None, hard_result has data\n</code></pre>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--handling-none-outputs","title":"Handling None Outputs","text":"<p>Since unselected branches return <code>None</code>, you can use logical operators to merge results (covered in Lesson 5):</p> <pre><code>final_result = easy_result | hard_result  # Gets the non-None result\n</code></pre>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    inputs = synalinks.Input(data_model=Query)\n\n    # Branch routes to different generators based on decision\n    (easy_branch, hard_branch) = await synalinks.Branch(\n        question=\"Evaluate the difficulty to answer the provided query\",\n        labels=[\"easy\", \"difficult\"],\n        language_model=language_model,\n        branches=[\n            synalinks.Generator(data_model=Answer, language_model=language_model),\n            synalinks.Generator(data_model=AnswerWithThinking, language_model=language_model),\n        ],\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=[easy_branch, hard_branch],\n        name=\"conditional_branches\",\n    )\n\n    # One output will be None depending on which branch was taken\n    results = await program(Query(query=\"What is 2 + 2?\"))\n    print(f\"Easy branch: {results[0]}\")  # Has data\n    print(f\"Hard branch: {results[1]}\")  # None\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Branch Module: Combines Decision with multiple processing paths -     routes inputs to different modules based on classification.</li> <li>Exclusive Execution: Only one branch executes per input; others     return <code>None</code>.</li> <li>Tuple Output: Branch returns a tuple of outputs, one per label,     where only the selected branch has data.</li> <li>OR Operator: Use <code>result1 | result2</code> to merge branch outputs and     get the non-None result.</li> </ul>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Branch</li> <li>Generator</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A simple, direct answer (for easy questions).</p> Source code in <code>examples/4_conditional_branches.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"A simple, direct answer (for easy questions).\"\"\"\n\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A detailed answer with reasoning (for hard questions).</p> Source code in <code>examples/4_conditional_branches.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"A detailed answer with reasoning (for hard questions).\"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Conditional%20Branches/#examples.4_conditional_branches.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A user query to process.</p> Source code in <code>examples/4_conditional_branches.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A user query to process.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Conversational%20Applications/","title":"Conversational Applications","text":""},{"location":"Code%20Examples/Conversational%20Applications/#examples.6_conversational_applications--conversational-applications","title":"Conversational Applications","text":"<p>Synalinks is designed to handle conversational applications as well as query-based systems. In the case of a conversational applications, the input data model is a list of chat messages, and the output an individual chat message. The <code>Program</code> is in that case responsible of handling a single conversation turn.</p> <pre><code>sequenceDiagram\n    participant User\n    participant Program\n    participant LLM\n\n    User-&gt;&gt;Program: ChatMessages [msg1, msg2, ...]\n    Program-&gt;&gt;LLM: Full conversation context\n    LLM--&gt;&gt;Program: New response\n    Program--&gt;&gt;User: ChatMessage (assistant)\n    Note over User,Program: Add response to history\n    User-&gt;&gt;Program: ChatMessages [..., new_msg]</code></pre> <pre><code>inputs = synalinks.Input(data_model=synalinks.ChatMessages)\noutputs = await synalinks.Generator(\n    language_model=language_model,\n    streaming=False,\n)(inputs)\n\nprogram = synalinks.Program(\n    inputs=inputs,\n    outputs=outputs,\n    name=\"simple_chatbot\",\n)\n</code></pre> <p>By default, if no data_model/schema is provided to the <code>Generator</code> it will output a <code>ChatMessage</code> like output. If the data model is <code>None</code>, then you can enable streaming.</p> <p>To use the chatbot, pass a <code>ChatMessages</code> object with the conversation history:</p> <pre><code>input_messages = synalinks.ChatMessages(\n    messages=[\n        synalinks.ChatMessage(\n            role=\"user\",\n            content=\"Hello! What is the capital of France?\",\n        )\n    ]\n)\nresponse = await program(input_messages)\n</code></pre> <p>Note: Streaming is disabled during training and should only be used in the last <code>Generator</code> of your pipeline.</p>"},{"location":"Code%20Examples/Conversational%20Applications/#examples.6_conversational_applications--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Conversational Flow Management: Synalinks effectively manages     conversational applications by handling inputs as a list of chat messages     and generating individual chat messages as outputs.</li> <li>Streaming and Real-Time Interaction: Synalinks supports streaming for     real-time interactions. However, streaming is disabled during training     and should be used only in the final <code>Generator</code>.</li> <li>Simple Setup: Just use <code>ChatMessages</code> as input data model and the     <code>Generator</code> will handle the conversation context automatically.</li> </ul>"},{"location":"Code%20Examples/Conversational%20Applications/#examples.6_conversational_applications--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Conversational%20Applications/#examples.6_conversational_applications--api-references","title":"API References","text":"<ul> <li>ChatMessages (Base DataModels)</li> <li>LanguageModel</li> <li>Generator</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Data%20Model%20Operators/","title":"Data Model Operators","text":""},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--data-model-operators","title":"Data Model Operators","text":"<p>In previous lessons, you learned to create branches where some outputs can be <code>None</code>. How do we combine these outputs? This lesson introduces operators for merging and manipulating data models.</p>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--the-five-operators","title":"The Five Operators","text":"<pre><code>graph LR\n    subgraph Operators\n        CONCAT[A + B: Concatenate]\n        AND[A &amp; B: Safe Merge]\n        OR[A | B: First Non-None]\n        XOR[A ^ B: Exclusive]\n        NOT[~A: Cancel]\n    end</code></pre> <p>Synalinks provides five Python operators for data models:</p> Operator Name Behavior <code>+</code> Concatenation Merge fields (fails if either is None) <code>&amp;</code> Logical AND Safe merge (returns None if either is None) <code>|</code> Logical OR Returns first non-None, or merges if both exist <code>^</code> Logical XOR Returns data only if exactly ONE is non-None <code>~</code> NOT (Invert) Converts data to None"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--1-concatenation","title":"1. Concatenation (<code>+</code>)","text":"<p>Combines fields from two data models into one:</p> <pre><code>answer = Answer(answer=\"42\")           # {\"answer\": \"42\"}\nthinking = Thinking(thinking=\"...\")    # {\"thinking\": \"...\"}\n\ncombined = answer + thinking  # {\"answer\": \"42\", \"thinking\": \"...\"}\n</code></pre> <p>Warning: Raises an exception if either operand is <code>None</code>!</p>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--2-logical-and","title":"2. Logical AND (<code>&amp;</code>)","text":"<p>Safe concatenation - returns <code>None</code> if either input is <code>None</code>:</p> <pre><code>result = data &amp; possibly_none  # None if possibly_none is None\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--3-logical-or","title":"3. Logical OR (<code>|</code>)","text":"<p>Returns non-None value, or merges if both have data:</p> <pre><code>(easy_result, hard_result) = await Branch(...)(inputs)\nfinal = easy_result | hard_result  # Gets whichever has data\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--4-logical-xor","title":"4. Logical XOR (<code>^</code>)","text":"<p>Returns data only if exactly one operand is non-None:</p> <pre><code>result = a ^ b\n# If a has data and b is None: returns a\n# If a is None and b has data: returns b\n# If both have data: returns None\n# If both are None: returns None\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--5-not-invert","title":"5. NOT / Invert (<code>~</code>)","text":"<p>Converts any data to <code>None</code>:</p> <pre><code>result = ~data  # Always returns None\n</code></pre> <p>Useful for conditional flows where you want to \"cancel\" a path.</p>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--truth-table","title":"Truth Table","text":"A B A <code>+</code> B A <code>&amp;</code> B A <code>|</code> B A <code>^</code> B Data Data Merged Merged Merged <code>None</code> Data <code>None</code> <code>ERROR</code> <code>None</code> A A <code>None</code> Data <code>ERROR</code> <code>None</code> B B <code>None</code> <code>None</code> <code>ERROR</code> <code>None</code> <code>None</code> <code>None</code> A <code>~</code>A Data <code>None</code> None <code>None</code>* <p>*Note: <code>~None</code> behavior depends on context (symbolic vs json)</p>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--complete-example-merging-branch-outputs","title":"Complete Example: Merging Branch Outputs","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    inputs = synalinks.Input(data_model=Query)\n\n    # Branch returns (easy_result, hard_result) - only ONE is non-None\n    (easy, hard) = await synalinks.Branch(\n        question=\"Evaluate difficulty\",\n        labels=[\"easy\", \"difficult\"],\n        language_model=language_model,\n        branches=[\n            synalinks.Generator(data_model=Answer, language_model=language_model),\n            synalinks.Generator(data_model=Answer, language_model=language_model),\n        ],\n    )(inputs)\n\n    # Use | to get whichever branch was active\n    outputs = easy | hard\n\n    program = synalinks.Program(inputs=inputs, outputs=outputs)\n\n    result = await program(Query(query=\"What is 2 + 2?\"))\n    print(f\"Answer: {result['answer']}\")  # Gets the non-None result\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Concat (<code>+</code>): Merge all fields from two data models into one. Both     inputs must have data (no <code>None</code>).</li> <li>OR (<code>|</code>): Returns the first non-None input. Perfect for merging     conditional branch outputs.</li> <li>AND (<code>&amp;</code>): Returns data only if both inputs have data; otherwise     returns <code>None</code>.</li> <li>XOR (<code>^</code>): Returns data only if exactly one input has data; returns     <code>None</code> if both or neither have data.</li> <li>NOT (<code>~</code>): Always returns <code>None</code>. Useful for canceling paths in     conditional flows.</li> </ul>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--program-visualizations","title":"Program Visualizations","text":""},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Branch</li> <li>Generator</li> <li>Merging Modules (And, Or, Xor, Concat)</li> </ul>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A simple answer.</p> Source code in <code>examples/5a_data_model_operators.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"A simple answer.\"\"\"\n\n    answer: str = synalinks.Field(description=\"The correct answer\")\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An answer with reasoning.</p> Source code in <code>examples/5a_data_model_operators.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"An answer with reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators.Critique","title":"<code>Critique</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A critique of an answer.</p> Source code in <code>examples/5a_data_model_operators.py</code> <pre><code>class Critique(synalinks.DataModel):\n    \"\"\"A critique of an answer.\"\"\"\n\n    critique: str = synalinks.Field(description=\"The critique of the answer\")\n</code></pre>"},{"location":"Code%20Examples/Data%20Model%20Operators/#examples.5a_data_model_operators.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A user query.</p> Source code in <code>examples/5a_data_model_operators.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A user query.\"\"\"\n\n    query: str = synalinks.Field(description=\"The user query\")\n</code></pre>"},{"location":"Code%20Examples/Decisions/","title":"Decisions","text":""},{"location":"Code%20Examples/Decisions/#examples.3_decisions--making-decisions","title":"Making Decisions","text":"<p>Sometimes your AI needs to make a choice: Is this email spam or not? Is this query simple or complex? Should we route to department A or B?</p> <p>This lesson introduces the Decision module - a powerful way to classify inputs into predefined categories.</p>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions--what-is-a-decision","title":"What is a Decision?","text":"<p>A Decision is essentially single-label classification. You provide:</p> <ol> <li>A question - what you want to decide</li> <li>Labels - the possible choices</li> </ol> <p>The LLM will pick exactly ONE label from your choices.</p> <pre><code>decision = await synalinks.Decision(\n    question=\"Is this email spam?\",\n    labels=[\"spam\", \"not_spam\"],\n    language_model=language_model,\n)(email_input)\n\nprint(decision[\"choice\"])  # Either \"spam\" or \"not_spam\"\n</code></pre>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions--how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    Input --&gt; Decision\n    Decision --&gt; |thinking| T[Reasoning]\n    Decision --&gt; |choice| C{Labels}\n    C --&gt; L1[easy]\n    C --&gt; L2[difficult]</code></pre> <p>Behind the scenes, Synalinks:</p> <ol> <li>Creates an Enum schema from your labels</li> <li>Uses constrained generation to force the LLM to pick one label</li> <li>Returns a structured output with <code>thinking</code> and <code>choice</code> fields</li> </ol> <p>This guarantees you get exactly one of your labels - no ambiguity!</p>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions--use-cases","title":"Use Cases","text":"<ul> <li>Routing: Send queries to different handlers based on type</li> <li>Filtering: Spam detection, content moderation</li> <li>Triage: Prioritize tasks by urgency</li> <li>Classification: Categorize documents, tickets, etc.</li> </ul>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    inputs = synalinks.Input(data_model=Query)\n\n    # Decision module classifies input into one of the labels\n    outputs = await synalinks.Decision(\n        question=\"Evaluate the difficulty to answer the provided query\",\n        labels=[\"easy\", \"difficult\"],\n        language_model=language_model,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"decision_making\",\n    )\n\n    # Test with different queries\n    result = await program(Query(query=\"What is 2 + 2?\"))\n    print(f\"Query: 'What is 2 + 2?'\")\n    print(f\"Choice: {result['choice']}\")  # Output: \"easy\"\n\n    result = await program(Query(query=\"Explain quantum entanglement\"))\n    print(f\"Query: 'Explain quantum entanglement'\")\n    print(f\"Choice: {result['choice']}\")  # Output: \"difficult\"\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Decision Module: Single-label classification that forces the LLM to     pick exactly one label from your predefined choices.</li> <li>Constrained Output: Uses enum schemas and constrained generation to     guarantee a valid label - no ambiguous responses.</li> <li>Routing: Use decisions to route inputs to different processing paths     based on their characteristics.</li> <li>Thinking Field: Decision outputs include a <code>thinking</code> field showing     the LLM's reasoning for its choice.</li> </ul>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Decisions/#examples.3_decisions--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Decision</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Decisions/#examples.3_decisions.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A user query to classify.</p> Source code in <code>examples/3_decisions.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A user query to classify.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Document%20RAG/","title":"Document RAG","text":""},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--document-rag-retrieval-augmented-generation","title":"Document RAG (Retrieval-Augmented Generation)","text":"<p>This example demonstrates how to build a classic RAG system using Synalinks. RAG combines document retrieval with language model generation to answer questions based on your own documents.</p>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--how-rag-works","title":"How RAG Works","text":"<pre><code>graph LR\n    subgraph Indexing\n        A[Documents] --&gt; B[Embeddings]\n        B --&gt; C[(KnowledgeBase)]\n    end\n    subgraph Query Time\n        D[Question] --&gt; E[RetrieveKnowledge]\n        C --&gt; E\n        E --&gt; F[Relevant Docs]\n        F --&gt; G[Generator]\n        G --&gt; H[Answer]\n    end</code></pre> <ol> <li>Index: Store documents in a knowledge base with embeddings</li> <li>Retrieve: When a question is asked, find relevant documents</li> <li>Generate: Use the retrieved context to generate an accurate answer</li> </ol>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--creating-a-document-store","title":"Creating a Document Store","text":"<pre><code>class Document(synalinks.DataModel):\n    id: str = synalinks.Field(description=\"Document ID\")\n    title: str = synalinks.Field(description=\"Document title\")\n    content: str = synalinks.Field(description=\"Document content\")\n\nknowledge_base = synalinks.KnowledgeBase(\n    uri=\"duckdb://./documents.db\",\n    data_models=[Document],\n    embedding_model=embedding_model,  # For semantic search\n)\n</code></pre>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--building-the-rag-pipeline","title":"Building the RAG Pipeline","text":"<pre><code>inputs = synalinks.Input(data_model=Query)\n\n# Retrieve relevant documents\nretrieved = await synalinks.RetrieveKnowledge(\n    knowledge_base=knowledge_base,\n    language_model=language_model,\n    search_type=\"hybrid\",\n    k=3,\n)(inputs)\n\n# Generate answer from retrieved context\nanswer = await synalinks.Generator(\n    data_model=Answer,\n    language_model=language_model,\n    instructions=\"Answer based on the retrieved documents.\",\n)(retrieved)\n</code></pre>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Hybrid Search: Combines keyword (BM25) and semantic (vector) search     for better retrieval accuracy.</li> <li>Chunking: For large documents, split into smaller chunks for better     retrieval granularity.</li> <li>Context Window: Retrieved documents are passed as context to the LM     for grounded generation.</li> <li>Trainable: The retrieval and generation modules can be optimized     using Synalinks training.</li> </ul>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag--api-references","title":"API References","text":"<ul> <li>KnowledgeBase</li> <li>RetrieveKnowledge</li> <li>Generator</li> <li>EmbeddingModel</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An answer generated from retrieved documents.</p> Source code in <code>examples/13_document_rag.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"An answer generated from retrieved documents.\"\"\"\n\n    answer: str = synalinks.Field(\n        description=\"The answer to the question based on retrieved documents\",\n    )\n    sources: str = synalinks.Field(\n        description=\"The document titles used to generate the answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A document stored in the knowledge base.</p> Source code in <code>examples/13_document_rag.py</code> <pre><code>class Document(synalinks.DataModel):\n    \"\"\"A document stored in the knowledge base.\"\"\"\n\n    id: str = synalinks.Field(\n        description=\"Unique document identifier\",\n    )\n    title: str = synalinks.Field(\n        description=\"Document title\",\n    )\n    content: str = synalinks.Field(\n        description=\"The main text content of the document\",\n    )\n    source: str = synalinks.Field(\n        description=\"Source or category of the document\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Document%20RAG/#examples.13_document_rag.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A user question.</p> Source code in <code>examples/13_document_rag.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A user question.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user's question\",\n    )\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/","title":"First Steps","text":""},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--first-steps-with-synalinks","title":"First Steps with Synalinks","text":"<p>Welcome to Synalinks! This lesson covers the essential concepts you need to understand before building AI applications.</p>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--installation","title":"Installation","text":"<pre><code># Using pip\npip install synalinks\n\n# Or using uv (recommended)\nuv pip install synalinks\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--key-concepts","title":"Key Concepts","text":""},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--1-no-traditional-prompting","title":"1. No Traditional Prompting","text":"<p>In Synalinks, you don't write prompts manually. Instead, you define:</p> <ul> <li>Input Data Models: What data goes into your program</li> <li>Output Data Models: What data comes out</li> </ul> <pre><code>graph LR\n    A[Input DataModel] --&gt; B[Synalinks]\n    B --&gt; C[Auto-Generated Prompt]\n    C --&gt; D[LLM]\n    D --&gt; E[Output DataModel]</code></pre> <p>The framework automatically constructs prompts from your data model definitions.</p>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--2-data-models-and-fields","title":"2. Data Models and Fields","text":"<p>Data models define the structure of your inputs and outputs. Use <code>Field</code> to add descriptions that help the LLM understand what each field should contain:</p> <pre><code>class Answer(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step reasoning\"\n    )\n    answer: str = synalinks.Field(\n        description=\"The final answer\"\n    )\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--3-constrained-structured-output","title":"3. Constrained Structured Output","text":"<p>Synalinks uses constrained structured output to ensure LLM responses always match your data model specification. No parsing errors!</p>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--4-session-management","title":"4. Session Management","text":"<p>Always clear the session at the start of scripts to ensure reproducible module naming:</p> <pre><code>synalinks.clear_session()\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--building-a-simple-program","title":"Building a Simple Program","text":"<p>Here's a complete example that creates a question-answering program:</p> <pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\n# Define input data model\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query to answer\")\n\n# Define output data model with chain-of-thought\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking process\")\n    answer: str = synalinks.Field(description=\"The correct answer based on your thinking\")\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    # Initialize a language model\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Build the program using the Functional API\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"chain_of_thought_qa\",\n    )\n\n    # Run the program\n    result = await program(Query(query=\"What is 2 + 2?\"))\n    print(f\"Thinking: {result['thinking']}\")\n    print(f\"Answer: {result['answer']}\")\n\nasyncio.run(main())\n</code></pre> <p>By adding a <code>thinking</code> field to our output model, we instruct the LLM to show its reasoning - this is called \"Chain of Thought\" prompting, achieved simply by defining the output structure!</p>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--key-takeaways","title":"Key Takeaways","text":"<ul> <li>No Prompt Engineering: Define data models instead of writing prompts -     the framework generates prompts automatically from your schemas.</li> <li>Structured Output: All LLM responses are guaranteed to match your     data model specification through constrained generation.</li> <li>Field Descriptions: Use descriptive <code>Field</code> annotations to guide the     LLM on what each field should contain.</li> <li>Chain of Thought: Add a \"thinking\" field to your output model to get     step-by-step reasoning from the LLM.</li> </ul>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A simple answer from the LLM.</p> Source code in <code>examples/0_first_steps.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"A simple answer from the LLM.\"\"\"\n\n    answer: str = synalinks.Field(\n        description=\"The correct answer to the query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An answer with step-by-step reasoning.</p> <p>By adding a 'thinking' field, we instruct the LLM to show its work. This is called \"Chain of Thought\" prompting - but we achieve it simply by defining the output structure!</p> Source code in <code>examples/0_first_steps.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"An answer with step-by-step reasoning.\n\n    By adding a 'thinking' field, we instruct the LLM to show its work.\n    This is called \"Chain of Thought\" prompting - but we achieve it\n    simply by defining the output structure!\n    \"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking process\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer based on your thinking\",\n    )\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The input to our program - a user's question.</p> <p>The docstring becomes part of the schema description.</p> Source code in <code>examples/0_first_steps.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"The input to our program - a user's question.\n\n    The docstring becomes part of the schema description.\n    \"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query to answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps.setup","title":"<code>setup()</code>","text":"<p>Setup Synalinks for use.</p> Source code in <code>examples/0_first_steps.py</code> <pre><code>def setup():\n    \"\"\"Setup Synalinks for use.\"\"\"\n    # Check version\n    print(f\"Synalinks version: {synalinks.__version__}\")\n\n    # Clear the global context for reproducible naming\n    # This ensures modules get consistent names across runs\n    synalinks.clear_session()\n</code></pre>"},{"location":"Code%20Examples/First%20Steps/#examples.0_first_steps.show_prompt_template","title":"<code>show_prompt_template()</code>","text":"<p>Display the default prompt template.</p> Source code in <code>examples/0_first_steps.py</code> <pre><code>def show_prompt_template():\n    \"\"\"Display the default prompt template.\"\"\"\n    print(\"=\" * 60)\n    print(\"Default Prompt Template\")\n    print(\"=\" * 60)\n    print()\n    print(\"Synalinks automatically constructs prompts using this template:\")\n    print()\n    print(synalinks.default_prompt_template())\n    print()\n    print(\"-\" * 60)\n    print(\"The template uses Markdown headers for structure.\")\n    print(\"Your data model schemas are automatically inserted!\")\n    print()\n</code></pre>"},{"location":"Code%20Examples/Functional%20API/","title":"Functional API","text":""},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--the-functional-api","title":"The Functional API","text":"<p>Welcome to your first Synalinks lesson! In this tutorial, you will learn how to build AI applications using the Functional API - the most intuitive and recommended approach for creating programs.</p>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--what-is-synalinks","title":"What is Synalinks?","text":"<p>Synalinks is a framework for building AI applications powered by Large Language Models (LLMs). Think of it like building with LEGO blocks - you connect different pieces (called \"modules\") together to create something useful.</p>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--core-concepts","title":"Core Concepts","text":""},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--1-programs-and-modules","title":"1. Programs and Modules","text":"<p>A Program in Synalinks is like a recipe - it defines the steps your AI application will follow. Each step is performed by a Module, which is a reusable building block.</p> <pre><code>graph LR\n    Input --&gt; Module1[Module 1] --&gt; Module2[Module 2] --&gt; Output</code></pre>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--2-data-models","title":"2. Data Models","text":"<p>Data flows through your program in structured formats called DataModels. Think of them as blueprints that define what information looks like:</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user's question\")\n</code></pre>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--3-the-functional-api","title":"3. The Functional API","text":"<p>The Functional API lets you build programs by:</p> <ol> <li>Creating an Input placeholder</li> <li>Passing it through modules (like connecting pipes)</li> <li>Wrapping everything in a Program</li> </ol> <pre><code># Step 1: Define where data enters\ninputs = synalinks.Input(data_model=Query)\n\n# Step 2: Pass through a module (Generator uses an LLM to create output)\noutputs = await synalinks.Generator(\n    data_model=Answer,\n    language_model=language_model,\n)(inputs)\n\n# Step 3: Create the program\nprogram = synalinks.Program(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--complete-example","title":"Complete Example","text":"<p>Here's a complete Chain of Thought program that shows reasoning before answering:</p> <pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\n# Define input and output data models\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nasync def main():\n    load_dotenv()\n\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    # Build with Functional API\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\",\n    )\n\n    # Run the program\n    result = await program(Query(query=\"What are the key aspects of human cognition?\"))\n    print(f\"Thinking: {result['thinking']}\")\n    print(f\"Answer: {result['answer']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Functional API: Build programs by connecting modules like pipes - data     flows from <code>Input</code> through modules to create outputs.</li> <li>Three Steps: (1) Create an Input, (2) Pass through modules, (3) Wrap in     a Program.</li> <li>Generator Module: The core module that uses an LLM to transform input     data into structured output matching your data model.</li> <li>Reusable Programs: Once built, programs can be called like functions     with your input data model.</li> </ul>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The output from our program - reasoning + final answer.</p> <p>By asking the LLM to show its thinking, we get better answers. This is called \"Chain of Thought\" prompting.</p> Source code in <code>examples/1a_functional_api.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"The output from our program - reasoning + final answer.\n\n    By asking the LLM to show its thinking, we get better answers.\n    This is called \"Chain of Thought\" prompting.\n    \"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Functional%20API/#examples.1a_functional_api.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The input to our program - a user's question.</p> Source code in <code>examples/1a_functional_api.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"The input to our program - a user's question.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/","title":"Implementing Custom Modules Via Subclassing","text":""},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--implementing-custom-modules-via-subclassing","title":"Implementing Custom Modules Via Subclassing","text":"<p>This tutorial is for more advanced users, it will cover how to create custom modules/programs via subclassing.</p> <p>In this tutorial, we will cover the following themes:</p> <ul> <li>The <code>Module</code> class</li> <li>The <code>add_variable()</code> method</li> <li>Trainable and non-trainable variables</li> <li>The <code>compute_output_spec()</code> and <code>build()</code> method</li> <li>The training argument in <code>call()</code></li> <li>Making sure your module/program can be serialized</li> </ul> <p>One of the main abstraction of Synalinks is the <code>Module</code> class. A <code>Module</code> encapsulate both a state (the module's variables) and a transformation from inputs to outputs (the <code>call()</code> method).</p> <p>For this tutorial, we are going to make a simple neuro-symbolic component called <code>BacktrackingOfThought</code>. This component is an adaptation of the famous backtracking algorithm, used a lot in symbolic planning/reasoning, combined with chain of thought, nowadays most used technique to enhance the LMs predicitons.</p> <p>The principle is straitforward, the component will have to \"think\" then we will critique at runtime the thinking and aggregate it to the current chain of thinking only if it is above the given threshold. This mechanism will allow the system to discard bad thinking to resume at the previsous step. Additionally we will add a stop condition.</p> <pre><code>graph TD\n    A[Input] --&gt; B[Think]\n    B --&gt; C[Critique]\n    C --&gt; D{Score &gt;= Threshold?}\n    D --&gt;|Yes| E[Add to Chain]\n    D --&gt;|No| B\n    E --&gt; F{Stop Condition?}\n    F --&gt;|No| B\n    F --&gt;|Yes| G[Output]</code></pre> <p>This algorithm a simplified version of the popular <code>TreeOfThought</code> that instead of being a tree strucutre, is only a sequential chain of thinking.</p>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--module-structure","title":"Module Structure","text":"<p>A custom module inherits from <code>synalinks.Module</code> and implements key methods:</p> <pre><code>class MyCustomModule(synalinks.Module):\n    def __init__(self, language_model=None, ...):\n        super().__init__(name=name, description=description, trainable=trainable)\n        # Initialize your generators and components\n        self.generator = synalinks.Generator(...)\n\n    async def call(self, inputs, training=False):\n        # Define the forward pass logic\n        return await self.generator(inputs, training=training)\n\n    async def compute_output_spec(self, inputs, training=False):\n        # Define how to compute output specification\n        return await self.generator(inputs)\n\n    def get_config(self):\n        # Return configuration for serialization\n        return {\"language_model\": synalinks.saving.serialize_synalinks_object(...)}\n\n    @classmethod\n    def from_config(cls, config):\n        # Reconstruct the module from config\n        return cls(...)\n</code></pre>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--the-__init__-function","title":"The <code>__init__()</code> function","text":"<p>When implementing modules that use a <code>Generator</code>, you want to externalize the generator's parameters (<code>prompt_template</code>, <code>instructions</code>, <code>examples</code>, <code>use_inputs_schema</code>, <code>use_outputs_schema</code>) to give maximum flexibility to your module when possible.</p>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--how-to-know-when-using-a-variable","title":"How to know when using a <code>Variable</code>?","text":"<p>As a rule of thumb, the variables should be anything that evolve over time during inference/training. These variables could be updated by the module itself, or by the optimizer if you have an optimizer designed for that.</p>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--the-call-function","title":"The <code>call()</code> function","text":"<p>The <code>call()</code> function is the core of the <code>Module</code> class. It defines the computation performed at every call of the module.</p>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--the-compute_output_spec-function","title":"The <code>compute_output_spec()</code> function","text":"<p>The <code>compute_output_spec()</code> function is responsible for defining the output data model of the module/program. Its inputs is always a <code>SymbolicDataModel</code>, a placeholder that only contains a JSON schema that serve as data specification.</p>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--serialization-and-deserialization","title":"Serialization and Deserialization","text":"<p>To ensure that your module can be saved and loaded correctly, you need to implement serialization and deserialization methods (<code>get_config()</code> and <code>from_config()</code>).</p>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Module Class: Encapsulates both state (variables) and transformation     logic (<code>call()</code> method).</li> <li>Initialization and Variables: Externalize generator parameters for     flexibility. Use <code>add_variables</code> for state management.</li> <li>Call Function: Defines the core computation of the module.</li> <li>Output Specification: <code>compute_output_spec()</code> defines the output data model.</li> <li>Serialization: Implement <code>get_config()</code> and <code>from_config()</code> for saving     and loading.</li> </ul>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules--api-references","title":"API References","text":"<ul> <li>Module (Base Class)</li> <li>ChainOfThought</li> <li>SelfCritique</li> <li>Generator</li> <li>Program Saving API</li> </ul>"},{"location":"Code%20Examples/Implementing%20Custom%20Modules%20Via%20Subclassing/#examples.9_custom_modules.BacktrackingOfThought","title":"<code>BacktrackingOfThought</code>","text":"<p>               Bases: <code>Module</code></p> <p>A Backtracking of Thought algorithm.</p> <p>This component combines the backtracking algorithm with chain of thought to enhance LMs predictions through iterative self-critique.</p> Source code in <code>examples/9_custom_modules.py</code> <pre><code>class BacktrackingOfThought(synalinks.Module):\n    \"\"\"A Backtracking of Thought algorithm.\n\n    This component combines the backtracking algorithm with chain of thought\n    to enhance LMs predictions through iterative self-critique.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        language_model=None,\n        backtracking_threshold=0.5,\n        stop_threshold=0.9,\n        max_iterations=5,\n        return_inputs=False,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n        self.language_model = language_model\n        self.backtracking_threshold = backtracking_threshold\n        self.stop_threshold = stop_threshold\n        self.max_iterations = max_iterations\n        self.return_inputs = return_inputs\n        self.prompt_template = prompt_template\n        self.examples = examples\n        self.instructions = instructions\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n\n        self.thinking = []\n        for i in range(self.max_iterations):\n            self.thinking.append(\n                synalinks.ChainOfThought(\n                    schema=self.schema,\n                    language_model=self.language_model,\n                    prompt_template=self.prompt_template,\n                    examples=self.examples,\n                    return_inputs=False,\n                    instructions=self.instructions,\n                    use_inputs_schema=self.use_inputs_schema,\n                    use_outputs_schema=self.use_outputs_schema,\n                    name=f\"thinking_generator_{i}_\" + self.name,\n                )\n            )\n        self.critique = []\n        for i in range(self.max_iterations):\n            self.critique.append(\n                synalinks.SelfCritique(\n                    language_model=self.language_model,\n                    prompt_template=self.prompt_template,\n                    examples=self.examples,\n                    return_inputs=True,\n                    instructions=self.instructions,\n                    use_inputs_schema=self.use_inputs_schema,\n                    use_outputs_schema=self.use_outputs_schema,\n                    name=f\"critique_generator_{i}\" + self.name,\n                )\n            )\n        # This is going to be the final generator\n        self.generator = synalinks.Generator(\n            schema=self.schema,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            return_inputs=self.return_inputs,\n            instructions=self.instructions,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            name=\"generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            # This is to allow logical flows\n            # (e.g. don't run the module if no inputs provided)\n            return None\n        for i in range(self.max_iterations):\n            thinking = await self.thinking[i](\n                inputs,\n                training=training,\n            )\n            critique = await self.critique[i](\n                thinking,\n                training=training,\n            )\n            reward = critique.get(\"reward\")\n            if reward &gt; self.backtracking_threshold:\n                inputs = await synalinks.ops.concat(\n                    inputs,\n                    critique,\n                    name=f\"_inputs_with_thinking_{i}_\" + self.name,\n                )\n                if reward &gt; self.stop_threshold:\n                    break\n        return await self.generator(\n            inputs,\n            training=training,\n        )\n\n    async def compute_output_spec(self, inputs, training=False):\n        for i in range(self.max_iterations):\n            inputs = await self.thinking[i](inputs)\n            inputs = await self.critique[i](inputs)\n        return await self.generator(inputs)\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"backtracking_threshold\": self.backtracking_threshold,\n            \"stop_threshold\": self.stop_threshold,\n            \"max_iterations\": self.max_iterations,\n            \"return_inputs\": self.return_inputs,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": synalinks.saving.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        return {**language_model_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = synalinks.saving.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(\n            language_model=language_model,\n            **config,\n        )\n</code></pre>"},{"location":"Code%20Examples/Interactive%20Agent/","title":"Interactive Agent","text":""},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--interactive-agents","title":"Interactive Agents","text":"<p>Interactive agents represent a significant advancement in AI system design, enabling dynamic interactions with users through iterative processing and validation of inputs. This tutorial will guide you through building an interactive agent using Synalinks, capable of processing mathematical queries and returning numerical answers through a series of interactions.</p>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--understanding-the-foundation","title":"Understanding the Foundation","text":"<p>Interactive agents address the need for dynamic and iterative processing of user inputs, allowing for more flexible and responsive AI systems. Unlike autonomous agents, interactive agents require user validation at each step, ensuring accuracy and relevance in their responses.</p> <p>The architecture of an interactive agent follows several core stages:</p> <ul> <li>The input stage captures the user's query or command.</li> <li>The processing stage uses predefined tools and logic to process the input   and generate a response.</li> <li>The validation stage requires user input to validate the tool calls and   their arguments.</li> <li>The output stage returns the result to the user.</li> </ul> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n\n    User-&gt;&gt;Agent: Query\n    Agent-&gt;&gt;User: Propose tool call\n    User-&gt;&gt;Agent: Validate/Edit args\n    Agent-&gt;&gt;Tool: Execute\n    Tool--&gt;&gt;Agent: Result\n    Agent-&gt;&gt;User: Response\n    Note over User,Agent: Repeat until done</code></pre> <p>Interactive agents are transforming the landscape of AI by facilitating dynamic and iterative interactions with users.</p>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--exploring-interactive-agent-architecture","title":"Exploring Interactive Agent Architecture","text":"<p>Synalinks offers a streamlined approach to building interactive agents, thanks to its modular and flexible architecture. Each user interaction initiates a new cycle in the Directed Acyclic Graph (DAG), and tool calls are executed only after receiving user validation.</p> <p>In non-autonomous mode (also called human in the loop or interactive mode), the user needs to validate/edit the tool arguments and send it back to the agent. In this mode, the agent requires a <code>ChatMessages</code> data model as input and outputs <code>ChatMessages</code> back to the user (containing both tool results and assistant response). The agent ignores the <code>max_iterations</code> argument, as it will only perform one step at a time.</p>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--creating-an-interactive-agent","title":"Creating an Interactive Agent","text":"<p>Define tools as async functions with complete docstrings. Important Tool Constraints:</p> <ul> <li> <p>No Optional Parameters: All parameters must be required. OpenAI and   other providers require all tool parameters in their JSON schemas.</p> </li> <li> <p>Complete Docstring Required: Every parameter must be documented in the   <code>Args:</code> section. The Tool extracts descriptions to build the JSON schema.</p> </li> </ul> <p>Use <code>ChatMessages</code> as input and set <code>autonomous=False</code>:</p> <pre><code>inputs = synalinks.Input(data_model=synalinks.ChatMessages)\noutputs = await synalinks.FunctionCallingAgent(\n    tools=tools,\n    language_model=language_model,\n    return_inputs_with_trajectory=True,\n    autonomous=False,  # Human-in-the-loop mode\n)(inputs)\n</code></pre>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--running-the-conversation-loop","title":"Running the Conversation Loop","text":"<p>Process messages iteratively, validating tool calls at each step:</p> <pre><code>input_messages = synalinks.ChatMessages(\n    messages=[synalinks.ChatMessage(role=\"user\", content=\"Calculate 2 + 2\")]\n)\n\nfor iteration in range(MAX_ITERATIONS):\n    response = await agent(input_messages)\n    assistant_message = response.get(\"messages\")[-1]\n\n    tool_calls = assistant_message.get(\"tool_calls\")\n    if not tool_calls:\n        print(assistant_message.get(\"content\"))  # Final response\n        break\n\n    # Display tool calls for user validation\n    for tool_call in tool_calls:\n        print(f\"Tool: {tool_call.get('name')}({tool_call.get('arguments')})\")\n\n    # In a real app: let user approve/modify/reject here\n    # After validation, append and continue\n    input_messages.messages.append(assistant_message)\n</code></pre>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Dynamic Interaction: Interactive agents facilitate dynamic and iterative     processing of user inputs.</li> <li>Modular Design: Synalinks modular architecture simplifies the development     of interactive agents.</li> <li>Structured Data Models: The use of structured ChatMessages models ensures     consistency and predictability.</li> <li>Tool Integration and Validation: Integration of tools and validation of     their arguments provide a robust foundation.</li> <li>User-Driven Processing: Interactive agents dynamically process information     and execute tasks based on user interactions.</li> </ul>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent--api-references","title":"API References","text":"<ul> <li>FunctionCallingAgent</li> <li>ChatMessages (Base DataModels)</li> <li>LanguageModel</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Interactive%20Agent/#examples.11_interactive_agent.calculate","title":"<code>calculate(expression)</code>  <code>async</code>","text":"<p>Perform calculations based on a mathematical expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>The mathematical expression to calculate, such as '2 + 2'. The expression can contain numbers, operators (+, -, *, /), parentheses, and spaces.</p> required Source code in <code>examples/11_interactive_agent.py</code> <pre><code>@synalinks.utils.register_synalinks_serializable()\nasync def calculate(expression: str):\n    \"\"\"Perform calculations based on a mathematical expression.\n\n    Args:\n        expression (str): The mathematical expression to calculate, such as\n            '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n            parentheses, and spaces.\n    \"\"\"\n    # Check for valid characters in the expression\n    if not all(char in \"0123456789+-*/(). \" for char in expression):\n        return {\n            \"result\": None,\n            \"log\": (\n                \"Invalid characters detected in the expression. \"\n                \"Only numbers, operators (+, -, *, /), parentheses, and spaces \"\n                \"are allowed.\"\n            ),\n        }\n    try:\n        # Safely evaluate the mathematical expression\n        result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n        return {\n            \"result\": result,\n            \"log\": \"Calculation successful\",\n        }\n    except Exception as e:\n        return {\n            \"result\": None,\n            \"log\": f\"Calculation error: {e}\",\n        }\n</code></pre>"},{"location":"Code%20Examples/JSON%20Ops/","title":"JSON Ops","text":""},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--json-operations","title":"JSON Operations","text":"<p>In Lesson 5a, you learned about data model operators (<code>+</code>, <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>~</code>). This lesson covers JSON operations - functions for transforming, filtering, and reshaping data models.</p>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--categories-of-operations","title":"Categories of Operations","text":""},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--1-masking-operations-filtering-fields","title":"1. Masking Operations (Filtering Fields)","text":"Operation Description Example <code>in_mask</code> Keep only specified fields <code>ops.in_mask(data, mask=[\"answer\"])</code> <code>out_mask</code> Remove specified fields <code>ops.out_mask(data, mask=[\"thinking\"])</code>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--2-renaming-operations","title":"2. Renaming Operations","text":"Operation Description Example <code>prefix</code> Add prefix to field names <code>ops.prefix(data, prefix=\"v1_\")</code> <code>suffix</code> Add suffix to field names <code>ops.suffix(data, suffix=\"_draft\")</code>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--3-aggregation-operations","title":"3. Aggregation Operations","text":"Operation Description Example <code>factorize</code> Group similar fields into lists <code>ops.factorize(combined)</code>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--4-logical-operations-function-form","title":"4. Logical Operations (Function Form)","text":"Operation Equivalent Description <code>ops.concat</code> <code>+</code> Merge fields with custom naming <code>ops.logical_and</code> <code>&amp;</code> Safe merge <code>ops.logical_or</code> <code>|</code> First non-None <code>ops.logical_xor</code> <code>^</code> Exactly one non-None"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--why-use-these-operations","title":"Why Use These Operations?","text":"<pre><code>graph LR\n    subgraph Before\n        A[thinking: ...&lt;br/&gt;answer: 42]\n    end\n    subgraph in_mask\n        B[answer: 42]\n    end\n    A --&gt;|in_mask| B</code></pre> <ol> <li>Data Preparation: Transform data before passing to next module</li> <li>Field Selection: Keep only relevant fields for downstream processing</li> <li>Conflict Resolution: Rename fields to avoid collisions when merging</li> <li>Aggregation: Combine multiple similar outputs into lists</li> </ol>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--complete-example-filtering-fields","title":"Complete Example: Filtering Fields","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    inputs = synalinks.Input(data_model=Query)\n    x = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n    )(inputs)\n\n    # Keep only the \"answer\" field, discard \"thinking\"\n    outputs = await synalinks.ops.in_mask(x, mask=[\"answer\"])\n\n    program = synalinks.Program(inputs=inputs, outputs=outputs)\n\n    result = await program(Query(query=\"What is 2 + 2?\"))\n    print(f\"Fields: {list(result.keys())}\")  # Only ['answer']\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--key-takeaways","title":"Key Takeaways","text":"<ul> <li>in_mask: Keep only specified fields from a data model. Useful for     filtering out intermediate fields like \"thinking\".</li> <li>out_mask: Remove specified fields, keeping all others.</li> <li>prefix/suffix: Add constant text before/after field values.</li> <li>factorize: Split a data model into multiple single-field data models     for independent processing.</li> <li>Training Integration: Use masks to evaluate only relevant fields     when computing rewards during training.</li> </ul>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--program-visualizations","title":"Program Visualizations","text":""},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>JSON Ops (in_mask, out_mask, prefix, suffix, factorize)</li> <li>Masking Modules</li> </ul>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A simple answer.</p> Source code in <code>examples/5b_json_ops.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"A simple answer.\"\"\"\n\n    answer: str = synalinks.Field(description=\"The correct answer\")\n</code></pre>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An answer with reasoning.</p> Source code in <code>examples/5b_json_ops.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"An answer with reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n</code></pre>"},{"location":"Code%20Examples/JSON%20Ops/#examples.5b_json_ops.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A user query.</p> Source code in <code>examples/5b_json_ops.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A user query.\"\"\"\n\n    query: str = synalinks.Field(description=\"The user query\")\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/","title":"Knowledge Extraction and Storage","text":""},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--knowledge-extraction-and-storage","title":"Knowledge Extraction and Storage","text":"<p>Synalinks provides a powerful knowledge base system for extracting, storing, and retrieving structured knowledge. This example demonstrates extracting structured information from invoices and documents, storing them, and querying them later.</p> <pre><code>graph LR\n    subgraph Extraction\n        A[Document] --&gt; B[Generator]\n        B --&gt; C[Structured Data]\n    end\n    subgraph Storage\n        C --&gt; D[UpdateKnowledge]\n        D --&gt; E[(KnowledgeBase)]\n    end\n    subgraph Retrieval\n        F[Query] --&gt; G[RetrieveKnowledge]\n        E --&gt; G\n        G --&gt; H[Results]\n    end</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--creating-a-knowledge-base","title":"Creating a Knowledge Base","text":"<p>The <code>KnowledgeBase</code> uses DuckDB as the underlying storage engine, providing full-text search and optional vector similarity search:</p> <pre><code># Define your data model\nclass Invoice(synalinks.DataModel):\n    invoice_number: str = synalinks.Field(description=\"Invoice number\")\n    vendor: str = synalinks.Field(description=\"Vendor name\")\n    total: float = synalinks.Field(description=\"Total amount\")\n    description: str = synalinks.Field(description=\"Description of items\")\n\n# Create a knowledge base\nknowledge_base = synalinks.KnowledgeBase(\n    uri=\"duckdb://./invoices.db\",\n    data_models=[Invoice],\n    embedding_model=embedding_model,  # Optional, for similarity search\n)\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--extracting-information-with-generator","title":"Extracting Information with Generator","text":"<p>Use a <code>Generator</code> to extract structured information from unstructured text:</p> <pre><code>inputs = synalinks.Input(data_model=DocumentText)\nextracted = await synalinks.Generator(\n    data_model=Invoice,\n    language_model=language_model,\n)(inputs)\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--storing-data-with-updateknowledge","title":"Storing Data with UpdateKnowledge","text":"<p>The <code>UpdateKnowledge</code> module stores data models in the knowledge base:</p> <pre><code>stored = await synalinks.UpdateKnowledge(\n    knowledge_base=knowledge_base,\n)(extracted)\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--retrieving-data-with-retrieveknowledge","title":"Retrieving Data with RetrieveKnowledge","text":"<p>The <code>RetrieveKnowledge</code> module uses hybrid search to find relevant records:</p> <pre><code>results = await synalinks.RetrieveKnowledge(\n    knowledge_base=knowledge_base,\n    language_model=language_model,\n    search_type=\"hybrid\",\n    k=5,\n)(query)\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--key-takeaways","title":"Key Takeaways","text":"<ul> <li>KnowledgeBase: Unified interface for storing and searching structured data     using DuckDB with full-text and vector search capabilities.</li> <li>UpdateKnowledge: Module for inserting/upserting data models into the     knowledge base using the first field as primary key.</li> <li>RetrieveKnowledge: Module for intelligent retrieval using LM-generated     search queries with hybrid search (full-text + vector).</li> <li>Structured Extraction: Use Generators to extract typed data from     unstructured text like invoices, receipts, or documents.</li> </ul>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--program-visualizations","title":"Program Visualizations","text":""},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--invoice-extraction-pipeline","title":"Invoice Extraction Pipeline","text":""},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--business-qa-system","title":"Business Q&amp;A System","text":""},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage--api-references","title":"API References","text":"<ul> <li>KnowledgeBase</li> <li>UpdateKnowledge</li> <li>RetrieveKnowledge</li> <li>Generator</li> <li>EmbeddingModel</li> </ul>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An answer based on retrieved information.</p> Source code in <code>examples/12_knowledge_extraction_and_storage.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"An answer based on retrieved information.\"\"\"\n\n    answer: str = synalinks.Field(\n        description=\"The answer to the user's question based on retrieved data\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage.Customer","title":"<code>Customer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Extracted customer information.</p> Source code in <code>examples/12_knowledge_extraction_and_storage.py</code> <pre><code>class Customer(synalinks.DataModel):\n    \"\"\"Extracted customer information.\"\"\"\n\n    customer_id: str = synalinks.Field(\n        description=\"Unique customer identifier\",\n    )\n    name: str = synalinks.Field(\n        description=\"Customer name (person or company)\",\n    )\n    email: str = synalinks.Field(\n        description=\"Customer email address\",\n    )\n    description: str = synalinks.Field(\n        description=\"Additional notes about the customer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage.DocumentText","title":"<code>DocumentText</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Raw document text to extract information from.</p> Source code in <code>examples/12_knowledge_extraction_and_storage.py</code> <pre><code>class DocumentText(synalinks.DataModel):\n    \"\"\"Raw document text to extract information from.\"\"\"\n\n    text: str = synalinks.Field(\n        description=\"The raw text content of the document\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage.Invoice","title":"<code>Invoice</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Extracted invoice information.</p> Source code in <code>examples/12_knowledge_extraction_and_storage.py</code> <pre><code>class Invoice(synalinks.DataModel):\n    \"\"\"Extracted invoice information.\"\"\"\n\n    invoice_number: str = synalinks.Field(\n        description=\"The unique invoice number or ID\",\n    )\n    vendor: str = synalinks.Field(\n        description=\"The name of the vendor or supplier\",\n    )\n    date: str = synalinks.Field(\n        description=\"The invoice date (YYYY-MM-DD format)\",\n    )\n    total_amount: float = synalinks.Field(\n        description=\"The total amount due\",\n    )\n    currency: str = synalinks.Field(\n        description=\"The currency (e.g., USD, EUR)\",\n    )\n    description: str = synalinks.Field(\n        description=\"A brief description of the invoice items or services\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Knowledge%20Extraction%20and%20Storage/#examples.12_knowledge_extraction_and_storage.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A user query for searching the knowledge base.</p> Source code in <code>examples/12_knowledge_extraction_and_storage.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A user query for searching the knowledge base.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The search query or question\",\n    )\n</code></pre>"},{"location":"Code%20Examples/MCP%20Agent/","title":"MCP Agent","text":""},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--mcp-agent","title":"MCP Agent","text":"<p>This example demonstrates how to build an autonomous agent that uses tools from an MCP (Model Context Protocol) server. MCP is a standard protocol for connecting AI models to external tools and data sources.</p>"},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--understanding-mcp-integration","title":"Understanding MCP Integration","text":"<pre><code>graph LR\n    subgraph Synalinks\n        A[Agent] --&gt; B[MultiServerMCPClient]\n    end\n    subgraph MCP Servers\n        B --&gt; C[Math Server]\n        B --&gt; D[Search Server]\n        B --&gt; E[...]\n    end\n    C --&gt; F[add, subtract, ...]\n    D --&gt; G[search, fetch, ...]</code></pre> <p>MCP enables seamless integration between language models and external tools through a standardized protocol. Synalinks provides the <code>MultiServerMCPClient</code> class to connect to one or more MCP servers and load their tools as native Synalinks Tool modules.</p> <p>Key benefits of using MCP:</p> <ul> <li>Standardized Protocol: Use tools from any MCP-compatible server</li> <li>Multiple Servers: Connect to multiple MCP servers simultaneously</li> <li>Namespace Support: Avoid tool name collisions with namespacing</li> <li>Transport Flexibility: Support for stdio, HTTP, SSE, and WebSocket</li> </ul>"},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--setting-up-the-mcp-server","title":"Setting Up the MCP Server","text":"<p>First, you need an MCP server. This example uses a simple math server (<code>mcp_math_server.py</code>) that provides basic arithmetic operations.</p> <p>To run the server standalone (for testing): </p><pre><code>uv run python examples/mcp_math_server.py\n</code></pre><p></p>"},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--creating-an-mcp-agent","title":"Creating an MCP Agent","text":"<p>Connect to MCP servers using <code>MultiServerMCPClient</code>:</p> <pre><code>client = synalinks.MultiServerMCPClient(\n    {\n        \"math\": {\n            \"command\": \"python\",\n            \"args\": [\"examples/mcp_math_server.py\"],\n            \"transport\": \"stdio\",\n        },\n    }\n)\n\n# Load all tools from connected servers\ntools = await client.get_tools()\n</code></pre> <p>Then use the tools with a <code>FunctionCallingAgent</code>:</p> <pre><code>inputs = synalinks.Input(data_model=Query)\noutputs = await synalinks.FunctionCallingAgent(\n    data_model=NumericalFinalAnswer,\n    tools=tools,\n    language_model=language_model,\n    autonomous=True,\n)(inputs)\n</code></pre>"},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--key-takeaways","title":"Key Takeaways","text":"<ul> <li>MCP Protocol: Synalinks supports the Model Context Protocol for     standardized tool integration.</li> <li>MultiServerMCPClient: Connect to multiple MCP servers and load tools     with automatic namespacing.</li> <li>Transport Options: Support for stdio (subprocess), HTTP, SSE, and     WebSocket transports.</li> <li>Seamless Integration: MCP tools work identically to native Synalinks     tools with full observability support.</li> </ul>"},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/MCP%20Agent/#examples.15_mcp_agent--api-references","title":"API References","text":"<ul> <li>FunctionCallingAgent</li> <li>DataModel</li> <li>LanguageModel</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Mixing%20Strategy/","title":"Mixing Strategy","text":""},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--the-mixing-strategy-recommended","title":"The Mixing Strategy (Recommended)","text":"<p>You've learned three ways to build programs:</p> <ul> <li>Functional API (1a): Flexible graph building</li> <li>Subclassing (1b): Full control but more boilerplate</li> <li>Sequential (1c): Simplest for linear pipelines</li> </ul> <p>Now, let's explore the recommended approach: mixing subclassing with the Functional API. This gives you the best of both worlds!</p>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--why-use-the-mixing-strategy","title":"Why Use the Mixing Strategy?","text":"Approach Encapsulation Boilerplate Flexibility Functional API Low Low High Subclassing High High High Sequential Medium Very Low Low Mixing High Low High <p>The mixing strategy provides:</p> <ol> <li>Encapsulation: Your program is a reusable class</li> <li>No boilerplate: No need for <code>call()</code>, <code>get_config()</code>, or <code>from_config()</code></li> <li>Flexibility: Full power of the Functional API inside</li> </ol>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--the-pattern","title":"The Pattern","text":"<pre><code># Step 1: Define a reusable module using the mixing strategy\nclass MyModule(synalinks.Program):\n\n    def __init__(self, language_model=None, ...):\n        super().__init__()  # Initialize without inputs/outputs\n        self.language_model = language_model  # Store config\n\n    async def build(self, inputs):\n        # Use Functional API to create the graph\n        # `inputs` is a SymbolicDataModel (from the outer program)\n        outputs = await synalinks.Generator(...)(inputs)\n\n        # Re-initialize with inputs and outputs\n        super().__init__(\n            inputs=inputs,\n            outputs=outputs,\n            name=self.name,\n        )\n\n# Step 2: Use the module inside a functional program\nmy_module = MyModule(language_model=lm)\n\ninputs = synalinks.Input(data_model=Query)  # Creates symbolic input\noutputs = await my_module(inputs)           # Triggers build() with symbolic input\n\nprogram = synalinks.Program(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant User\n    participant MyModule\n    participant FunctionalAPI\n    participant Graph\n\n    User-&gt;&gt;MyModule: Create instance with config\n    User-&gt;&gt;FunctionalAPI: inputs = Input(data_model)\n    User-&gt;&gt;MyModule: await my_module(inputs)\n    MyModule-&gt;&gt;MyModule: build(inputs) triggered\n    MyModule-&gt;&gt;FunctionalAPI: Create internal graph\n    MyModule-&gt;&gt;Graph: Re-initialize as Program\n    Graph--&gt;&gt;User: outputs (SymbolicDataModel)</code></pre> <ol> <li>Define your class: Implement <code>__init__()</code> and <code>build()</code> only</li> <li>Create an instance: Store configuration (language models, settings)</li> <li>Use in Functional API: Call the module with a symbolic <code>Input</code></li> <li>build() is triggered: Receives symbolic data, creates the graph</li> </ol> <p>The key insight: the mixing strategy creates reusable modules that you compose using the Functional API. The <code>build()</code> method receives symbolic inputs when called during graph construction.</p>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nclass ChainOfThought(synalinks.Program):\n    \"\"\"Reusable module using the mixing strategy.\"\"\"\n\n    def __init__(self, language_model=None, name=None):\n        super().__init__(name=name)\n        self.language_model = language_model\n\n    async def build(self, inputs):\n        # Use Functional API inside build()\n        outputs = await synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=self.language_model,\n        )(inputs)\n\n        # Re-initialize as a Functional program\n        super().__init__(inputs=inputs, outputs=outputs, name=self.name)\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    # Use the mixed module in a functional program\n    chain_of_thought = ChainOfThought(language_model=language_model)\n\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await chain_of_thought(inputs)  # Triggers build()\n\n    program = synalinks.Program(inputs=inputs, outputs=outputs)\n\n    result = await program(Query(query=\"What is 15% of 80?\"))\n    print(f\"Answer: {result['answer']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Mixing Strategy: Combine subclassing with the Functional API for the     best of both worlds - encapsulation without boilerplate.</li> <li>build() Method: Override <code>build()</code> to use the Functional API inside     your class, receiving symbolic inputs during graph construction.</li> <li>Automatic Serialization: No need for <code>get_config()</code> or <code>from_config()</code>     when using the mixing strategy.</li> <li>Reusable Components: Create library-quality modules that can be     composed into larger programs.</li> </ul>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The output from our program - reasoning + final answer.</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"The output from our program - reasoning + final answer.\"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Program</code></p> <p>A program that answers questions with step-by-step reasoning.</p> <p>This uses the MIXING STRATEGY: subclassing + Functional API. Notice how we DON'T implement call(), get_config(), or from_config()!</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>class ChainOfThought(synalinks.Program):\n    \"\"\"A program that answers questions with step-by-step reasoning.\n\n    This uses the MIXING STRATEGY: subclassing + Functional API.\n    Notice how we DON'T implement call(), get_config(), or from_config()!\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model=None,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        # Step 1: Initialize the base Program (without inputs/outputs yet)\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n\n        # Step 2: Store configuration for later use in build()\n        # These are NOT modules yet - just configuration!\n        self.language_model = language_model\n\n    async def build(self, inputs: synalinks.SymbolicDataModel) -&gt; None:\n        \"\"\"Build the program graph using the Functional API.\n\n        This method is called AUTOMATICALLY when the program is first used.\n        You don't need to call it yourself!\n\n        Args:\n            inputs (SymbolicDataModel): A SymbolicDataModel representing\n                    the input data model.\n        \"\"\"\n        # Step 3: Use Functional API to create the computation graph\n        # This is exactly like the Functional API (Lesson 1a)!\n        outputs = await synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=self.language_model,\n        )(inputs)\n\n        # Step 4: Re-initialize as a Functional program\n        # This tells Synalinks the complete graph structure\n        super().__init__(\n            inputs=inputs,\n            outputs=outputs,\n            name=self.name,\n            description=self.description,\n            trainable=self.trainable,\n        )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.ChainOfThought.build","title":"<code>build(inputs)</code>  <code>async</code>","text":"<p>Build the program graph using the Functional API.</p> <p>This method is called AUTOMATICALLY when the program is first used. You don't need to call it yourself!</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>SymbolicDataModel</code> <p>A SymbolicDataModel representing     the input data model.</p> required Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>async def build(self, inputs: synalinks.SymbolicDataModel) -&gt; None:\n    \"\"\"Build the program graph using the Functional API.\n\n    This method is called AUTOMATICALLY when the program is first used.\n    You don't need to call it yourself!\n\n    Args:\n        inputs (SymbolicDataModel): A SymbolicDataModel representing\n                the input data model.\n    \"\"\"\n    # Step 3: Use Functional API to create the computation graph\n    # This is exactly like the Functional API (Lesson 1a)!\n    outputs = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=self.language_model,\n    )(inputs)\n\n    # Step 4: Re-initialize as a Functional program\n    # This tells Synalinks the complete graph structure\n    super().__init__(\n        inputs=inputs,\n        outputs=outputs,\n        name=self.name,\n        description=self.description,\n        trainable=self.trainable,\n    )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.Critique","title":"<code>Critique</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A critique of an answer.</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>class Critique(synalinks.DataModel):\n    \"\"\"A critique of an answer.\"\"\"\n\n    issues: str = synalinks.Field(\n        description=\"Any issues or problems with the answer\",\n    )\n    is_correct: bool = synalinks.Field(\n        description=\"Whether the answer appears correct\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The input to our program - a user's question.</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"The input to our program - a user's question.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.RefinedAnswer","title":"<code>RefinedAnswer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A refined answer after self-critique.</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>class RefinedAnswer(synalinks.DataModel):\n    \"\"\"A refined answer after self-critique.\"\"\"\n\n    original_answer: str = synalinks.Field(\n        description=\"The original answer\",\n    )\n    refinement: str = synalinks.Field(\n        description=\"Any refinements or corrections\",\n    )\n    final_answer: str = synalinks.Field(\n        description=\"The final, refined answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.SelfCritiquingReasoner","title":"<code>SelfCritiquingReasoner</code>","text":"<p>               Bases: <code>Program</code></p> <p>A more complex program: reason, critique, then refine.</p> <p>This demonstrates building a multi-step pipeline with the mixing strategy.</p> <p>Flow: Query -&gt; Think+Answer -&gt; Critique -&gt; Refine</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>class SelfCritiquingReasoner(synalinks.Program):\n    \"\"\"A more complex program: reason, critique, then refine.\n\n    This demonstrates building a multi-step pipeline with the mixing strategy.\n\n    Flow: Query -&gt; Think+Answer -&gt; Critique -&gt; Refine\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model=None,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.language_model = language_model\n\n    async def build(self, inputs: synalinks.SymbolicDataModel) -&gt; None:\n        \"\"\"Build a multi-step reasoning pipeline.\"\"\"\n\n        # Step 1: Generate initial answer with thinking\n        initial_answer = await synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=self.language_model,\n            name=\"initial_reasoner\",\n        )(inputs)\n\n        # Step 2: Critique the answer\n        # Combine query + answer for context\n        critique_input = inputs + initial_answer\n        critique = await synalinks.Generator(\n            data_model=Critique,\n            language_model=self.language_model,\n            name=\"self_critic\",\n        )(critique_input)\n\n        # Step 3: Refine based on critique\n        refine_input = critique_input + critique\n        refined = await synalinks.Generator(\n            data_model=RefinedAnswer,\n            language_model=self.language_model,\n            name=\"refiner\",\n        )(refine_input)\n\n        # Initialize as Functional program\n        super().__init__(\n            inputs=inputs,\n            outputs=refined,\n            name=self.name,\n            description=self.description,\n            trainable=self.trainable,\n        )\n</code></pre>"},{"location":"Code%20Examples/Mixing%20Strategy/#examples.1d_mixing_strategy.SelfCritiquingReasoner.build","title":"<code>build(inputs)</code>  <code>async</code>","text":"<p>Build a multi-step reasoning pipeline.</p> Source code in <code>examples/1d_mixing_strategy.py</code> <pre><code>async def build(self, inputs: synalinks.SymbolicDataModel) -&gt; None:\n    \"\"\"Build a multi-step reasoning pipeline.\"\"\"\n\n    # Step 1: Generate initial answer with thinking\n    initial_answer = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=self.language_model,\n        name=\"initial_reasoner\",\n    )(inputs)\n\n    # Step 2: Critique the answer\n    # Combine query + answer for context\n    critique_input = inputs + initial_answer\n    critique = await synalinks.Generator(\n        data_model=Critique,\n        language_model=self.language_model,\n        name=\"self_critic\",\n    )(critique_input)\n\n    # Step 3: Refine based on critique\n    refine_input = critique_input + critique\n    refined = await synalinks.Generator(\n        data_model=RefinedAnswer,\n        language_model=self.language_model,\n        name=\"refiner\",\n    )(refine_input)\n\n    # Initialize as Functional program\n    super().__init__(\n        inputs=inputs,\n        outputs=refined,\n        name=self.name,\n        description=self.description,\n        trainable=self.trainable,\n    )\n</code></pre>"},{"location":"Code%20Examples/Parallel%20Branches/","title":"Parallel Branches","text":""},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--parallel-branches","title":"Parallel Branches","text":"<p>In Lesson 1, you learned to build simple linear programs. But what if you need to do multiple things at once? This lesson introduces parallel branches - running multiple modules simultaneously for better performance.</p>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--why-parallel-execution","title":"Why Parallel Execution?","text":"<p>Imagine you're writing an essay and need to:</p> <ol> <li>Research the topic</li> <li>Find relevant quotes</li> <li>Check for similar existing essays</li> </ol> <p>You could do these sequentially (one after another), but it's much faster to do them all at the same time - that's parallel execution!</p>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--how-parallel-branches-work","title":"How Parallel Branches Work","text":"<p>In Synalinks, creating parallel branches is automatic. When multiple modules use the same input, they run in parallel:</p> <pre><code>graph LR\n    Input --&gt; Fork\n    Fork --&gt; A[Module A]\n    Fork --&gt; B[Module B]\n    Fork --&gt; C[Module C]\n    A --&gt; Merge\n    B --&gt; Merge\n    C --&gt; Merge\n    Merge --&gt; Outputs</code></pre> <p>The syntax is simple - just connect multiple modules to the same input:</p> <pre><code>inputs = synalinks.Input(data_model=Query)\n\n# Both generators share the same input -&gt; they run in parallel!\nanswer1 = await synalinks.Generator(data_model=Answer1, ...)(inputs)\nanswer2 = await synalinks.Generator(data_model=Answer2, ...)(inputs)\n\n# Pass multiple outputs as a list\nprogram = synalinks.Program(inputs=inputs, outputs=[answer1, answer2])\n</code></pre>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--use-cases-for-parallel-branches","title":"Use Cases for Parallel Branches","text":"<ol> <li>Ensemble Methods: Get multiple answers and pick the best one</li> <li>Multi-perspective Analysis: Analyze input from different angles</li> <li>Redundancy: Run the same task multiple times for reliability</li> <li>Speed: Process independent tasks concurrently</li> </ol>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    inputs = synalinks.Input(data_model=Query)\n\n    # Two generators sharing the same input -&gt; parallel execution!\n    branch_1 = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n        name=\"branch_1\",\n    )(inputs)\n\n    branch_2 = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n        name=\"branch_2\",\n    )(inputs)\n\n    # Program with multiple outputs (as a list)\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=[branch_1, branch_2],\n        name=\"parallel_branches\",\n    )\n\n    # Result is a LIST of outputs\n    results = await program(Query(query=\"What is the meaning of life?\"))\n    for i, result in enumerate(results, 1):\n        print(f\"Branch {i}: {result['answer'][:50]}...\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Automatic Parallelism: When multiple modules share the same input,     Synalinks automatically runs them in parallel.</li> <li>Multiple Outputs: Pass a list of outputs to <code>Program</code> to get multiple     results from parallel branches.</li> <li>Performance: Parallel execution significantly speeds up programs that     need multiple independent operations.</li> <li>Ensemble Methods: Use parallel branches to get multiple perspectives     or answers and combine them.</li> </ul>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>Input</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An answer with step-by-step reasoning.</p> Source code in <code>examples/2_parallel_branches.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"An answer with step-by-step reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Parallel%20Branches/#examples.2_parallel_branches.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The input query to analyze.</p> Source code in <code>examples/2_parallel_branches.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"The input query to analyze.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/RAG%20Agent/","title":"RAG Agent","text":""},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent--rag-agent","title":"RAG Agent","text":"<p>This example demonstrates how to build a RAG Agent - an autonomous agent that can search a knowledge base and use other tools to answer complex questions. Unlike a simple RAG pipeline, a RAG agent can decide when to search, what to search for, and can combine multiple searches with other tools.</p>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent--why-rag-agents","title":"Why RAG Agents?","text":"<pre><code>graph TD\n    A[Question] --&gt; B[Agent]\n    B --&gt; C{Need to Search?}\n    C --&gt;|Yes| D[search_documents]\n    D --&gt; E[Results]\n    E --&gt; B\n    C --&gt;|No| F{Use Calculator?}\n    F --&gt;|Yes| G[calculate]\n    G --&gt; B\n    F --&gt;|No| H[Final Answer]</code></pre> <p>Traditional RAG pipelines always retrieve documents, even for simple questions. A RAG agent is smarter:</p> <ul> <li>Decides IF retrieval is needed</li> <li>Can reformulate queries for better search results</li> <li>Can perform multiple searches and combine results</li> <li>Can use other tools (calculator, web search, etc.) alongside retrieval</li> </ul>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent--building-a-rag-agent","title":"Building a RAG Agent","text":"<p>Important Tool Constraints:</p> <ul> <li> <p>No Optional Parameters: All tool parameters must be required. OpenAI   and other LLM providers require all parameters in their JSON schemas.</p> </li> <li> <p>Complete Docstring Required: Every parameter must be documented in the   <code>Args:</code> section. The Tool extracts descriptions to build the JSON schema   sent to the LLM. Missing descriptions raise a ValueError.</p> </li> </ul> <pre><code># Define the search tool\n@synalinks.saving.register_synalinks_serializable()\nasync def search_knowledge_base(query: str):\n    \"\"\"Search the knowledge base for documents relevant to the query.\n\n    Use this tool to find information about company policies, procedures,\n    products, or any documented knowledge.\n\n    Args:\n        query (str): The search query describing what information you need.\n    \"\"\"\n    results = await knowledge_base.hybrid_search(query, k=3)\n    return {\"documents\": results}\n\n# Create the agent\ninputs = synalinks.Input(data_model=synalinks.ChatMessages)\noutputs = await synalinks.FunctionCallingAgent(\n    tools=[synalinks.Tool(search_knowledge_base)],\n    language_model=language_model,\n    autonomous=True,\n    max_iterations=5,\n)(inputs)\n</code></pre>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Autonomous Decision Making: The agent decides when to search and what     queries to use.</li> <li>Multi-Tool Support: Combine document search with other tools like     calculators or APIs.</li> <li>Iterative Reasoning: The agent can search multiple times and refine     its understanding.</li> <li>Conversational: Maintains context across multiple turns.</li> </ul>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent--api-references","title":"API References","text":"<ul> <li>FunctionCallingAgent</li> <li>KnowledgeBase</li> <li>ChatMessages (Base DataModels)</li> <li>EmbeddingModel</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A document stored in the knowledge base.</p> Source code in <code>examples/14_rag_agent.py</code> <pre><code>class Document(synalinks.DataModel):\n    \"\"\"A document stored in the knowledge base.\"\"\"\n\n    id: str = synalinks.Field(\n        description=\"Unique document identifier\",\n    )\n    title: str = synalinks.Field(\n        description=\"Document title\",\n    )\n    content: str = synalinks.Field(\n        description=\"The main text content of the document\",\n    )\n    category: str = synalinks.Field(\n        description=\"Document category\",\n    )\n</code></pre>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent.calculate","title":"<code>calculate(expression)</code>  <code>async</code>","text":"<p>Perform mathematical calculations.</p> <p>Use this for any math operations like addition, multiplication, percentages, or complex expressions.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>A mathematical expression to evaluate, e.g., '100 * 0.15' or '(50 + 30) / 2'.</p> required Source code in <code>examples/14_rag_agent.py</code> <pre><code>@synalinks.saving.register_synalinks_serializable()\nasync def calculate(expression: str):\n    \"\"\"Perform mathematical calculations.\n\n    Use this for any math operations like addition, multiplication,\n    percentages, or complex expressions.\n\n    Args:\n        expression (str): A mathematical expression to evaluate,\n            e.g., '100 * 0.15' or '(50 + 30) / 2'.\n    \"\"\"\n    # Validate expression\n    allowed_chars = \"0123456789+-*/().% \"\n    if not all(c in allowed_chars for c in expression):\n        return {\n            \"status\": \"error\",\n            \"message\": \"Invalid characters in expression\",\n            \"result\": None,\n        }\n\n    try:\n        # Handle percentage\n        expr = expression.replace(\"%\", \"/100\")\n        result = eval(expr, {\"__builtins__\": {}}, {})\n        return {\n            \"status\": \"success\",\n            \"expression\": expression,\n            \"result\": round(float(result), 2),\n        }\n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"message\": str(e),\n            \"result\": None,\n        }\n</code></pre>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent.get_current_date","title":"<code>get_current_date()</code>  <code>async</code>","text":"<p>Get the current date and time.</p> <p>Use this when you need to know today's date or the current time. This function takes no arguments.</p> Source code in <code>examples/14_rag_agent.py</code> <pre><code>@synalinks.saving.register_synalinks_serializable()\nasync def get_current_date():\n    \"\"\"Get the current date and time.\n\n    Use this when you need to know today's date or the current time.\n    This function takes no arguments.\n    \"\"\"\n    from datetime import datetime\n\n    now = datetime.now()\n    return {\n        \"date\": now.strftime(\"%Y-%m-%d\"),\n        \"time\": now.strftime(\"%H:%M:%S\"),\n        \"day_of_week\": now.strftime(\"%A\"),\n    }\n</code></pre>"},{"location":"Code%20Examples/RAG%20Agent/#examples.14_rag_agent.search_knowledge_base","title":"<code>search_knowledge_base(query)</code>  <code>async</code>","text":"<p>Search the knowledge base for documents relevant to the query.</p> <p>Use this tool to find information about company policies, procedures, products, or any documented knowledge. Returns the most relevant documents.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query describing what information you need. Be specific and use relevant keywords.</p> required Source code in <code>examples/14_rag_agent.py</code> <pre><code>@synalinks.saving.register_synalinks_serializable()\nasync def search_knowledge_base(query: str):\n    \"\"\"Search the knowledge base for documents relevant to the query.\n\n    Use this tool to find information about company policies, procedures,\n    products, or any documented knowledge. Returns the most relevant documents.\n\n    Args:\n        query (str): The search query describing what information you need.\n            Be specific and use relevant keywords.\n    \"\"\"\n    global _knowledge_base\n\n    if _knowledge_base is None:\n        return {\"error\": \"Knowledge base not initialized\"}\n\n    results = await _knowledge_base.hybrid_search(query, k=30)\n    return {\"documents\": results}\n</code></pre>"},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/","title":"Reward Metrics And Optimizers","text":""},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--rewards-metrics-optimizers","title":"Rewards, Metrics &amp; Optimizers","text":""},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--understanding-rewards","title":"Understanding Rewards","text":"<p><code>Reward</code>s are an essential part of reinforcement learning frameworks. They are scalar values (between 0.0 and 1.0 for synalinks) that guide the process into making more efficient decisions or predictions. During training, the goal is to maximize the reward function. The reward gives the system an indication of how well it performed for that task.</p> <p>All rewards consist of a function or program that takes two inputs:</p> <ul> <li><code>y_pred</code>: The prediction of the program.</li> <li><code>y_true</code>: The ground truth/target value provided by the training data.</li> </ul>"},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--understanding-metrics","title":"Understanding Metrics","text":"<p><code>Metric</code>s are scalar values that are monitored during training and evaluation. These values are used to know which program is best, in order to save it. Or to provide additional information to compare different architectures with each others. Unlike <code>Reward</code>s, a <code>Metric</code> is not used during training, meaning the metric value is not backpropagated. Additionaly every reward function can be used as metric.</p>"},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--predictions-filtering","title":"Predictions Filtering","text":"<p>Sometimes, your program have to output a complex JSON but you want to evaluate just part of it. This could be because your training data only include a subset of the JSON, or because the additonal fields were added only to help the LMs. In that case, you have to filter out or filter in your predictions and ground truth using <code>out_mask</code> or <code>in_mask</code> list parameter.</p>"},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--understanding-optimizers","title":"Understanding Optimizers","text":"<pre><code>graph LR\n    subgraph Training Loop\n        A[Input] --&gt; B[Program]\n        B --&gt; C[y_pred]\n        D[y_true] --&gt; E[Reward]\n        C --&gt; E\n        E --&gt; F[Optimizer]\n        F --&gt; |update| B\n    end\n    E --&gt; G[Metrics]</code></pre> <p>Optimizers are systems that handle the update of the module's state in order to make them more performant. They are in charge of backpropagating the rewards from the training process and select or generate examples and instructions for the LMs.</p> <pre><code>program.compile(\n    reward=synalinks.rewards.CosineSimilarity(\n        embedding_model=embedding_model,\n        in_mask=[\"answer\"],  # Only evaluate the \"answer\" field\n    ),\n    optimizer=synalinks.optimizers.RandomFewShot(),\n    metrics=[\n        synalinks.metrics.F1Score(in_mask=[\"answer\"]),\n    ],\n)\n</code></pre>"},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Rewards: Guide the reinforcement learning process by providing feedback     on the system's performance.</li> <li>Metrics: Scalar values monitored during training and evaluation to     determine the best-performing program.</li> <li>Optimizers: Update the module's state to improve performance.</li> <li>Filtering Outputs: Use <code>out_mask</code> or <code>in_mask</code> to evaluate only relevant     fields of complex JSON outputs.</li> </ul>"},{"location":"Code%20Examples/Reward%20Metrics%20And%20Optimizers/#examples.7_reward_metrics_and_optimizers--api-references","title":"API References","text":"<ul> <li>Rewards</li> <li>Metrics</li> <li>Optimizers</li> <li>Program Training API</li> </ul>"},{"location":"Code%20Examples/SQL%20Agent/","title":"SQL Agent","text":""},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--sql-agent","title":"SQL Agent","text":"<p>An SQL Agent combines the reasoning capabilities of LLMs with structured database access. Instead of writing SQL manually, users ask questions in natural language and the agent autonomously discovers the schema, constructs queries, and returns answers.</p>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--why-sql-agents-matter","title":"Why SQL Agents Matter","text":"<p>Traditional database access requires SQL knowledge. An SQL Agent bridges this gap:</p> <pre><code>graph LR\n    subgraph Traditional\n        A[User] --&gt; B[Write SQL]\n        B --&gt; C[Execute Query]\n        C --&gt; D[Interpret Results]\n    end\n    subgraph SQL Agent\n        E[User Question] --&gt; F[Agent Reasons]\n        F --&gt; G[Auto-generates SQL]\n        G --&gt; H[Natural Language Answer]\n    end</code></pre> <p>SQL Agents provide:</p> <ol> <li>Natural Language Interface: Ask questions without knowing SQL</li> <li>Schema Discovery: Agent explores database structure automatically</li> <li>Query Generation: Constructs correct SQL based on user intent</li> <li>Safe Execution: Read-only queries prevent accidental modifications</li> </ol>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--architecture","title":"Architecture","text":"<p>The SQL Agent uses three specialized tools to interact with the Knowledge Base:</p> <pre><code>flowchart TD\n    A[User Question] --&gt; B[FunctionCallingAgent]\n    B --&gt; C{Select Tool}\n    C --&gt;|Discover structure| D[get_database_schema]\n    C --&gt;|View sample data| E[get_table_sample]\n    C --&gt;|Execute query| F[run_sql_query]\n    D --&gt; G[Schema Info]\n    E --&gt; H[Sample Rows]\n    F --&gt; I[Query Results]\n    G --&gt; B\n    H --&gt; B\n    I --&gt; B\n    B --&gt; J[Natural Language Answer + SQL]</code></pre> <p>The agent follows an autonomous loop: it first discovers the schema, optionally samples data to understand the format, then constructs and executes SQL queries. This process repeats until the agent has enough information to answer.</p>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--available-tools","title":"Available Tools","text":"Tool Description <code>get_database_schema</code> Returns all tables and their columns with types <code>get_table_sample</code> Fetches sample rows from a table with pagination <code>run_sql_query</code> Executes read-only SELECT queries safely"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--defining-input-and-output-models","title":"Defining Input and Output Models","text":"<p>First, define DataModels for the agent's input (user query) and output (answer with SQL):</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    \"\"\"A natural language query about the database.\"\"\"\n    query: str = synalinks.Field(\n        description=\"The user's question about the data in natural language\"\n    )\n\nclass SQLResult(synalinks.DataModel):\n    \"\"\"The result of the SQL agent's analysis.\"\"\"\n    answer: str = synalinks.Field(\n        description=\"A clear, natural language answer to the user's question\"\n    )\n    sql_query: str = synalinks.Field(\n        description=\"The SQL query that was executed to get the answer\"\n    )\n</code></pre> <p>The <code>Query</code> model captures the user's natural language question. The <code>SQLResult</code> model structures the output to include both a human-readable answer and the SQL query used, providing transparency into the agent's reasoning.</p>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--tool-design-for-database-access","title":"Tool Design for Database Access","text":"<p>Each tool uses <code>kb.get_symbolic_data_models()</code> to dynamically discover available tables. This makes the agent adaptable to any database schema without hardcoding table names.</p> <p>Important Tool Constraints:</p> <ul> <li> <p>No Optional Parameters: All tool parameters must be required. OpenAI   and other LLM providers require all parameters in their JSON schemas. Do   not use default values for parameters.</p> </li> <li> <p>Complete Docstring Required: Every parameter must be documented in the   <code>Args:</code> section. The Tool extracts descriptions from the docstring to build   the JSON schema sent to the LLM. Missing descriptions raise a ValueError.</p> </li> </ul> <p>Example tool definition:</p> <pre><code>from synalinks.src.saving.object_registration import register_synalinks_serializable\n\n@register_synalinks_serializable()\nasync def get_database_schema():\n    \"\"\"Get the complete database schema including all tables and columns.\"\"\"\n    kb = get_knowledge_base()\n    symbolic_models = kb.get_symbolic_data_models()\n\n    schema_info = []\n    for model in symbolic_models:\n        schema = model.get_schema()\n        table_name = schema.get(\"title\", \"Unknown\")\n        properties = schema.get(\"properties\", {})\n\n        columns = []\n        for col_name, col_info in properties.items():\n            col_type = col_info.get(\"type\", \"unknown\")\n            columns.append(f\"  - {col_name} ({col_type})\")\n\n        schema_info.append(f\"Table: {table_name}\\n\" + \"\\n\".join(columns))\n\n    return {\"schema\": \"\\n\\n\".join(schema_info), \"table_count\": len(symbolic_models)}\n</code></pre> <p>The <code>@register_synalinks_serializable()</code> decorator enables the tool to be saved and loaded with the program. The tool extracts table names and column information from the JSON schema of each symbolic data model.</p>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--building-the-sql-agent","title":"Building the SQL Agent","text":"<p>Wrap the tool functions with <code>synalinks.Tool()</code> and create the agent using <code>FunctionCallingAgent</code>:</p> <pre><code># Create Knowledge Base with your data models\nkb = synalinks.KnowledgeBase(\n    uri=\"duckdb://my_database.db\",\n    data_models=[Customer, Product, SalesOrder],\n)\n\n# Configure language model\nlm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n# Wrap async functions as Tool objects\nschema_tool = synalinks.Tool(get_database_schema)\nsample_tool = synalinks.Tool(get_table_sample)\nquery_tool = synalinks.Tool(run_sql_query)\n\n# Build the agent using Functional API\ninputs = synalinks.Input(data_model=Query)\noutputs = await synalinks.FunctionCallingAgent(\n    data_model=SQLResult,\n    language_model=lm,\n    tools=[schema_tool, sample_tool, query_tool],\n    autonomous=True,                    # Run until agent decides it's done\n    max_iterations=10,                  # Safety limit to prevent infinite loops\n    return_inputs_with_trajectory=True, # Include full tool call history\n)(inputs)\n\nsql_agent = synalinks.Program(\n    inputs=inputs,\n    outputs=outputs,\n    name=\"sql_agent\",\n)\n</code></pre> <p>The <code>autonomous=True</code> setting allows the agent to make multiple tool calls until it has gathered enough information. The <code>max_iterations</code> parameter prevents runaway execution.</p>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--safety-considerations","title":"Safety Considerations","text":"<p>The <code>run_sql_query</code> tool enforces read-only access through multiple layers:</p> <ol> <li>SELECT Only: Rejects queries not starting with SELECT</li> <li>Keyword Filtering: Blocks DROP, DELETE, INSERT, UPDATE, ALTER, etc.</li> <li>Read-Only Mode: Uses <code>kb.query(sql, read_only=True)</code></li> </ol> <pre><code>@register_synalinks_serializable()\nasync def run_sql_query(sql_query: str):\n    \"\"\"Execute a read-only SQL query.\"\"\"\n    kb = get_knowledge_base()\n\n    # Validate query is read-only\n    query_upper = sql_query.strip().upper()\n    if not query_upper.startswith(\"SELECT\"):\n        return {\"error\": \"Only SELECT queries are allowed.\", \"success\": False}\n\n    # Check for dangerous keywords\n    dangerous = [\"DROP\", \"DELETE\", \"INSERT\", \"UPDATE\", \"ALTER\", \"CREATE\"]\n    for keyword in dangerous:\n        if keyword in query_upper:\n            return {\"error\": f\"Forbidden keyword: {keyword}\", \"success\": False}\n\n    results = await kb.query(sql_query, read_only=True)\n    return {\"success\": True, \"results\": results, \"row_count\": len(results)}\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--example-usage","title":"Example Usage","text":"<pre><code># Ask a natural language question\nresult = await sql_agent(Query(query=\"Who are the top 3 customers by orders?\"))\n\nprint(result[\"answer\"])\n# Output: \"The top 3 customers are: Carlos Garcia ($1539.96),\n#          Alice Johnson ($1489.96), and Diana Chen ($379.94)\"\n\nprint(result[\"sql_query\"])\n# Output: \"SELECT c.name, SUM(o.total_amount) as total\n#          FROM Customer c JOIN SalesOrder o ON c.id = o.customer_id\n#          GROUP BY c.name ORDER BY total DESC LIMIT 3\"\n</code></pre> <p>The agent automatically: 1. Discovered the Customer and SalesOrder tables 2. Understood the relationship via customer_id 3. Wrote a proper JOIN with aggregation 4. Returned both the answer and the SQL for transparency</p>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Dynamic Schema Discovery: Use <code>kb.get_symbolic_data_models()</code> to make   tools adaptable to any database structure without hardcoding.</p> </li> <li> <p>Autonomous Reasoning: The <code>FunctionCallingAgent</code> with <code>autonomous=True</code>   iteratively calls tools until it can answer the question.</p> </li> <li> <p>Safety First: Always validate SQL queries and use read-only mode to   prevent accidental data modifications.</p> </li> <li> <p>Transparent Outputs: Include the generated SQL in the output so users   can verify and learn from the agent's reasoning.</p> </li> <li> <p>Tool Serialization: Use <code>@register_synalinks_serializable()</code> on tool   functions to enable program saving and loading.</p> </li> </ul>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent--api-references","title":"API References","text":"<ul> <li>FunctionCallingAgent</li> <li>KnowledgeBase</li> <li>Tool</li> </ul>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.Customer","title":"<code>Customer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A customer in the database.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>class Customer(synalinks.DataModel):\n    \"\"\"A customer in the database.\"\"\"\n\n    id: str = synalinks.Field(description=\"Customer ID\")\n    name: str = synalinks.Field(description=\"Customer name\")\n    email: str = synalinks.Field(description=\"Customer email\")\n    country: str = synalinks.Field(description=\"Customer country\")\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.Product","title":"<code>Product</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A product in the database.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>class Product(synalinks.DataModel):\n    \"\"\"A product in the database.\"\"\"\n\n    id: str = synalinks.Field(description=\"Product ID\")\n    name: str = synalinks.Field(description=\"Product name\")\n    category: str = synalinks.Field(description=\"Product category\")\n    price: float = synalinks.Field(description=\"Product price\")\n    stock: int = synalinks.Field(description=\"Stock quantity\")\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A natural language query about the database.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"A natural language query about the database.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user's question about the data in natural language\"\n    )\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.SQLResult","title":"<code>SQLResult</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The result of the SQL agent's analysis.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>class SQLResult(synalinks.DataModel):\n    \"\"\"The result of the SQL agent's analysis.\"\"\"\n\n    answer: str = synalinks.Field(\n        description=\"A clear, natural language answer to the user's question\"\n    )\n    sql_query: str = synalinks.Field(\n        description=\"The SQL query that was executed to get the answer\"\n    )\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.SalesOrder","title":"<code>SalesOrder</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An order in the database.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>class SalesOrder(synalinks.DataModel):\n    \"\"\"An order in the database.\"\"\"\n\n    id: str = synalinks.Field(description=\"Order ID\")\n    customer_id: str = synalinks.Field(description=\"Customer ID\")\n    product_id: str = synalinks.Field(description=\"Product ID\")\n    quantity: int = synalinks.Field(description=\"Quantity ordered\")\n    total_amount: float = synalinks.Field(description=\"Total order amount\")\n    status: str = synalinks.Field(description=\"Order status\")\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.get_database_schema","title":"<code>get_database_schema()</code>  <code>async</code>","text":"<p>Get the complete database schema including all tables and their columns.</p> <p>Returns a list of all tables with their column names and types. Use this tool first to understand what data is available before writing queries.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>@register_synalinks_serializable()\nasync def get_database_schema():\n    \"\"\"Get the complete database schema including all tables and their columns.\n\n    Returns a list of all tables with their column names and types.\n    Use this tool first to understand what data is available before writing queries.\n    \"\"\"\n    kb = get_knowledge_base()\n    symbolic_models = kb.get_symbolic_data_models()\n\n    schema_info = []\n    for model in symbolic_models:\n        schema = model.get_schema()\n        table_name = schema.get(\"title\", \"Unknown\")\n        properties = schema.get(\"properties\", {})\n\n        columns = []\n        for col_name, col_info in properties.items():\n            col_type = col_info.get(\"type\", \"unknown\")\n            col_desc = col_info.get(\"description\", \"\")\n            columns.append(f\"  - {col_name} ({col_type}): {col_desc}\")\n\n        schema_info.append(f\"Table: {table_name}\\n\" + \"\\n\".join(columns))\n\n    return {\n        \"schema\": \"\\n\\n\".join(schema_info),\n        \"table_count\": len(symbolic_models),\n    }\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.get_knowledge_base","title":"<code>get_knowledge_base()</code>","text":"<p>Get the global knowledge base instance.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>def get_knowledge_base():\n    \"\"\"Get the global knowledge base instance.\"\"\"\n    global _knowledge_base\n    return _knowledge_base\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.get_table_sample","title":"<code>get_table_sample(table_name, limit, offset)</code>  <code>async</code>","text":"<p>Get a sample of rows from a specific table to understand the data format.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table to sample.</p> required <code>limit</code> <code>int</code> <p>Number of sample rows to return (recommended: 3-5).</p> required <code>offset</code> <code>int</code> <p>Number of rows to skip before returning results (use 0 for start).</p> required Source code in <code>examples/16_sql_agent.py</code> <pre><code>@register_synalinks_serializable()\nasync def get_table_sample(table_name: str, limit: int, offset: int):\n    \"\"\"Get a sample of rows from a specific table to understand the data format.\n\n    Args:\n        table_name (str): The name of the table to sample.\n        limit (int): Number of sample rows to return (recommended: 3-5).\n        offset (int): Number of rows to skip before returning results (use 0 for start).\n    \"\"\"\n    kb = get_knowledge_base()\n    symbolic_models = kb.get_symbolic_data_models()\n\n    # Find the matching symbolic model by table name\n    target_model = None\n    available_tables = []\n    for model in symbolic_models:\n        schema = model.get_schema()\n        name = schema.get(\"title\", \"Unknown\")\n        available_tables.append(name)\n        if name == table_name:\n            target_model = model\n            break\n\n    if target_model is None:\n        return {\"error\": f\"Table '{table_name}' not found. Available: {available_tables}\"}\n\n    try:\n        results = await kb.getall(target_model, limit=limit, offset=offset)\n        return {\n            \"table\": table_name,\n            \"sample_data\": [dict(r) for r in results],\n            \"row_count\": len(results),\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.main","title":"<code>main()</code>  <code>async</code>","text":"<p>Demonstrate the SQL agent with natural language queries.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>async def main():\n    \"\"\"Demonstrate the SQL agent with natural language queries.\"\"\"\n    global _knowledge_base\n\n    load_dotenv()\n    synalinks.clear_session()\n\n    # -------------------------------------------------------------------------\n    # Create Knowledge Base\n    # -------------------------------------------------------------------------\n    print(\"=\" * 60)\n    print(\"SQL Agent with Knowledge Base\")\n    print(\"=\" * 60)\n\n    db_path = \"guides/sql_agent.db\"\n    if os.path.exists(db_path):\n        os.remove(db_path)\n\n    print(\"\\nCreating knowledge base...\")\n    _knowledge_base = synalinks.KnowledgeBase(\n        uri=f\"duckdb://{db_path}\",\n        data_models=[Customer, Product, SalesOrder],\n        wipe_on_start=True,\n        name=\"sql_agent_kb\",\n    )\n\n    print(\"Populating with sample data...\")\n    await populate_knowledge_base(_knowledge_base)\n\n    # -------------------------------------------------------------------------\n    # Create SQL Agent\n    # -------------------------------------------------------------------------\n    print(\"\\nBuilding SQL agent...\")\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Wrap tools\n    schema_tool = synalinks.Tool(get_database_schema)\n    sample_tool = synalinks.Tool(get_table_sample)\n    query_tool = synalinks.Tool(run_sql_query)\n\n    # Build the SQL agent\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.FunctionCallingAgent(\n        data_model=SQLResult,\n        language_model=lm,\n        tools=[schema_tool, sample_tool, query_tool],\n        autonomous=True,\n        max_iterations=10,\n        return_inputs_with_trajectory=True,\n    )(inputs)\n\n    sql_agent = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"sql_agent\",\n        description=\"An agent that answers questions about data using SQL queries\",\n    )\n\n    sql_agent.summary()\n\n    # -------------------------------------------------------------------------\n    # Demo Queries\n    # -------------------------------------------------------------------------\n    example_queries = [\n        \"What tables are available in the database?\",\n        \"Who are the top 3 customers by total order amount?\",\n        \"What is the most popular product category?\",\n        \"Show me all pending orders with customer names\",\n    ]\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SQL Agent Demo\")\n    print(\"=\" * 60)\n\n    for query_text in example_queries:\n        print(f\"\\nQuestion: {query_text}\")\n        print(\"-\" * 40)\n\n        try:\n            result = await sql_agent(Query(query=query_text))\n\n            # Show trajectory (tool calls and results)\n            messages = result.get(\"messages\", [])\n            tool_calls_count = 0\n            for msg in messages:\n                if msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n                    for tool_call in msg[\"tool_calls\"]:\n                        tool_calls_count += 1\n                        args = tool_call.get(\"arguments\", {})\n                        args_str = \", \".join(f\"{k}={repr(v)}\" for k, v in args.items())\n                        print(\n                            f\"Tool Call {tool_calls_count}: {tool_call['name']}({args_str})\"\n                        )\n                elif msg.get(\"role\") == \"tool\":\n                    content = msg.get(\"content\", \"\")\n                    # Truncate long results for readability\n                    if len(content) &gt; 200:\n                        content = content[:200] + \"...\"\n                    print(f\"Tool Result: {content}\")\n\n            print(f\"\\nAnswer: {result['answer']}\")\n            print(f\"SQL: {result['sql_query']}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n        print()\n\n    # Cleanup\n    if os.path.exists(db_path):\n        os.remove(db_path)\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.populate_knowledge_base","title":"<code>populate_knowledge_base(kb)</code>  <code>async</code>","text":"<p>Populate the knowledge base with sample data.</p> Source code in <code>examples/16_sql_agent.py</code> <pre><code>async def populate_knowledge_base(kb):\n    \"\"\"Populate the knowledge base with sample data.\"\"\"\n\n    # Sample customers\n    customers = [\n        Customer(\n            id=\"C001\", name=\"Alice Johnson\", email=\"alice@example.com\", country=\"USA\"\n        ),\n        Customer(\n            id=\"C002\",\n            name=\"Bob Smith\",\n            email=\"bob@example.com\",\n            country=\"UK\",\n        ),\n        Customer(\n            id=\"C003\",\n            name=\"Carlos Garcia\",\n            email=\"carlos@example.com\",\n            country=\"Spain\",\n        ),\n        Customer(\n            id=\"C004\",\n            name=\"Diana Chen\",\n            email=\"diana@example.com\",\n            country=\"China\",\n        ),\n        Customer(\n            id=\"C005\",\n            name=\"Emma Wilson\",\n            email=\"emma@example.com\",\n            country=\"Canada\",\n        ),\n    ]\n\n    # Sample products\n    products = [\n        Product(\n            id=\"P001\",\n            name=\"Laptop Pro\",\n            category=\"Electronics\",\n            price=1299.99,\n            stock=50,\n        ),\n        Product(\n            id=\"P002\",\n            name=\"Wireless Mouse\",\n            category=\"Electronics\",\n            price=49.99,\n            stock=200,\n        ),\n        Product(\n            id=\"P003\",\n            name=\"Mechanical Keyboard\",\n            category=\"Electronics\",\n            price=149.99,\n            stock=100,\n        ),\n        Product(\n            id=\"P004\",\n            name=\"USB-C Hub\",\n            category=\"Accessories\",\n            price=79.99,\n            stock=150,\n        ),\n        Product(\n            id=\"P005\",\n            name=\"Monitor Stand\",\n            category=\"Accessories\",\n            price=89.99,\n            stock=75,\n        ),\n        Product(\n            id=\"P006\",\n            name=\"Webcam HD\",\n            category=\"Electronics\",\n            price=129.99,\n            stock=80,\n        ),\n        Product(\n            id=\"P007\",\n            name=\"Desk Lamp\",\n            category=\"Office\",\n            price=45.99,\n            stock=120,\n        ),\n        Product(\n            id=\"P008\",\n            name=\"Notebook Set\",\n            category=\"Office\",\n            price=12.99,\n            stock=500,\n        ),\n    ]\n\n    # Sample orders\n    orders = [\n        SalesOrder(\n            id=\"O001\",\n            customer_id=\"C001\",\n            product_id=\"P001\",\n            quantity=1,\n            total_amount=1299.99,\n            status=\"completed\",\n        ),\n        SalesOrder(\n            id=\"O002\",\n            customer_id=\"C001\",\n            product_id=\"P002\",\n            quantity=2,\n            total_amount=99.98,\n            status=\"completed\",\n        ),\n        SalesOrder(\n            id=\"O003\",\n            customer_id=\"C002\",\n            product_id=\"P003\",\n            quantity=1,\n            total_amount=149.99,\n            status=\"completed\",\n        ),\n        SalesOrder(\n            id=\"O004\",\n            customer_id=\"C003\",\n            product_id=\"P001\",\n            quantity=1,\n            total_amount=1299.99,\n            status=\"shipped\",\n        ),\n        SalesOrder(\n            id=\"O005\",\n            customer_id=\"C003\",\n            product_id=\"P004\",\n            quantity=3,\n            total_amount=239.97,\n            status=\"shipped\",\n        ),\n        SalesOrder(\n            id=\"O006\",\n            customer_id=\"C004\",\n            product_id=\"P002\",\n            quantity=5,\n            total_amount=249.95,\n            status=\"pending\",\n        ),\n        SalesOrder(\n            id=\"O007\",\n            customer_id=\"C004\",\n            product_id=\"P006\",\n            quantity=1,\n            total_amount=129.99,\n            status=\"completed\",\n        ),\n        SalesOrder(\n            id=\"O008\",\n            customer_id=\"C005\",\n            product_id=\"P007\",\n            quantity=2,\n            total_amount=91.98,\n            status=\"completed\",\n        ),\n        SalesOrder(\n            id=\"O009\",\n            customer_id=\"C005\",\n            product_id=\"P008\",\n            quantity=10,\n            total_amount=129.90,\n            status=\"completed\",\n        ),\n        SalesOrder(\n            id=\"O010\",\n            customer_id=\"C001\",\n            product_id=\"P005\",\n            quantity=1,\n            total_amount=89.99,\n            status=\"pending\",\n        ),\n    ]\n\n    # Store all data\n    for customer in customers:\n        await kb.update(customer.to_json_data_model())\n    for product in products:\n        await kb.update(product.to_json_data_model())\n    for order in orders:\n        await kb.update(order.to_json_data_model())\n\n    print(f\"  Stored {len(customers)} customers\")\n    print(f\"  Stored {len(products)} products\")\n    print(f\"  Stored {len(orders)} orders\")\n</code></pre>"},{"location":"Code%20Examples/SQL%20Agent/#examples.16_sql_agent.run_sql_query","title":"<code>run_sql_query(sql_query)</code>  <code>async</code>","text":"<p>Execute a read-only SQL query and return the results.</p> <p>Parameters:</p> Name Type Description Default <code>sql_query</code> <code>str</code> <p>A SELECT SQL query to execute. Only SELECT queries are allowed.</p> required Important <ul> <li>Only SELECT queries are permitted for safety</li> <li>Use get_database_schema first to discover available tables</li> <li>Use proper JOIN syntax for multi-table queries</li> <li>Include LIMIT clause for large result sets</li> </ul> Source code in <code>examples/16_sql_agent.py</code> <pre><code>@register_synalinks_serializable()\nasync def run_sql_query(sql_query: str):\n    \"\"\"Execute a read-only SQL query and return the results.\n\n    Args:\n        sql_query (str): A SELECT SQL query to execute. Only SELECT queries are allowed.\n\n    Important:\n        - Only SELECT queries are permitted for safety\n        - Use get_database_schema first to discover available tables\n        - Use proper JOIN syntax for multi-table queries\n        - Include LIMIT clause for large result sets\n    \"\"\"\n    kb = get_knowledge_base()\n\n    # Validate query is read-only\n    query_upper = sql_query.strip().upper()\n    if not query_upper.startswith(\"SELECT\"):\n        return {\n            \"error\": \"Only SELECT queries are allowed.\",\n            \"success\": False,\n        }\n\n    # Check for dangerous patterns\n    dangerous = [\"DROP\", \"DELETE\", \"INSERT\", \"UPDATE\", \"ALTER\", \"CREATE\", \"TRUNCATE\"]\n    for keyword in dangerous:\n        if keyword in query_upper:\n            return {\"error\": f\"Forbidden keyword: {keyword}\", \"success\": False}\n\n    try:\n        results = await kb.query(sql_query, read_only=True)\n        return {\n            \"success\": True,\n            \"query\": sql_query,\n            \"row_count\": len(results),\n            \"results\": results,\n        }\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e), \"query\": sql_query}\n</code></pre>"},{"location":"Code%20Examples/Sequential%20API/","title":"Sequential API","text":""},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--the-sequential-api","title":"The Sequential API","text":"<p>You've learned two ways to build programs: the Functional API (Lesson 1a) and Subclassing (Lesson 1b). Now, let's explore the simplest approach: the Sequential API.</p>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--when-to-use-sequential","title":"When to Use Sequential","text":"<p>The Sequential API is perfect when your program is a simple pipeline - data flows through modules one after another, like water through pipes:</p> <pre><code>graph LR\n    Input --&gt; A[Module A] --&gt; B[Module B] --&gt; C[Module C] --&gt; Output</code></pre> <p>No branching, no conditionals, just a straight line of transformations.</p>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--comparison-three-ways-to-build-programs","title":"Comparison: Three Ways to Build Programs","text":"API Use Case Complexity Sequential Simple linear pipelines Easiest Functional Graphs with branches/merges Medium Subclassing Custom logic, full control Advanced"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--the-sequential-pattern","title":"The Sequential Pattern","text":"<p>Building with Sequential is as simple as making a list:</p> <pre><code>program = synalinks.Sequential(\n    [\n        synalinks.Input(data_model=InputType),     # First: where data enters\n        synalinks.SomeModule(...),                  # Middle: transformations\n        synalinks.Generator(data_model=OutputType), # Last: final output\n    ],\n    name=\"my_program\",\n)\n</code></pre> <p>Think of it like a recipe:</p> <ol> <li>Start with ingredients (Input)</li> <li>Apply steps in order (Modules)</li> <li>Get the final dish (Output)</li> </ol>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    # Create a sequential program - just a list of modules!\n    program = synalinks.Sequential(\n        [\n            synalinks.Input(data_model=Query),\n            synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=language_model,\n            ),\n        ],\n        name=\"chain_of_thought\",\n    )\n\n    result = await program(Query(query=\"What is the capital of France?\"))\n    print(f\"Answer: {result['answer']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Sequential API: The simplest way to build programs - just provide a     list of modules that execute in order.</li> <li>Linear Pipelines: Best for simple data flows with no branching or     conditional logic.</li> <li>Minimal Boilerplate: No need to manually connect inputs/outputs -     Sequential handles the wiring automatically.</li> <li>Quick Prototyping: Great for testing ideas quickly before moving to     more complex architectures.</li> </ul>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>Input</li> <li>Sequential</li> </ul>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The output from our program - reasoning + final answer.</p> Source code in <code>examples/1c_sequential.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"The output from our program - reasoning + final answer.\"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Sequential%20API/#examples.1c_sequential.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The input to our program - a user's question.</p> Source code in <code>examples/1c_sequential.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"The input to our program - a user's question.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Subclassing/","title":"Subclassing","text":""},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--building-programs-by-subclassing","title":"Building Programs by Subclassing","text":"<p>In Lesson 1a, you learned to build programs using the Functional API. Now, let's explore a more advanced approach: subclassing the Program class.</p>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--when-to-use-subclassing","title":"When to Use Subclassing","text":"<p>Subclassing is useful when you need:</p> <ul> <li>Custom logic in your program's execution flow</li> <li>Stateful behavior that persists across calls</li> <li>Reusable components that can be shared across projects</li> <li>Full control over serialization and deserialization</li> </ul> <p>Think of it like the difference between using a pre-built function vs writing your own class in object-oriented programming.</p>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--the-subclassing-pattern","title":"The Subclassing Pattern","text":"<p>When you subclass <code>synalinks.Program</code>, you must implement:</p> <ol> <li><code>__init__()</code>: Define your modules and initialize state</li> <li><code>call()</code>: Define how data flows through your modules</li> <li><code>get_config()</code>: Define how to save your program (serialization)</li> <li><code>from_config()</code>: Define how to load your program (deserialization)</li> </ol> <pre><code>classDiagram\n    class Program {\n        +__init__()\n        +call(inputs)\n        +get_config()\n        +from_config(config)\n    }\n    class MyProgram {\n        +generator\n        +__init__(language_model)\n        +call(inputs)\n        +get_config()\n        +from_config(config)\n    }\n    Program &lt;|-- MyProgram</code></pre> <pre><code>class MyProgram(synalinks.Program):\n\n    def __init__(self, language_model=None):\n        super().__init__()  # Always call super().__init__()!\n        self.generator = synalinks.Generator(\n            data_model=OutputModel,\n            language_model=language_model,\n        )\n\n    async def call(self, inputs, training=False):\n        # Define the forward pass\n        return await self.generator(inputs)\n\n    def get_config(self):\n        # Return a dict with everything needed to recreate this program\n        return {\"language_model\": serialize(self.language_model)}\n\n    @classmethod\n    def from_config(cls, config):\n        # Recreate the program from the config dict\n        return cls(language_model=deserialize(config[\"language_model\"]))\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--important-the-build-method","title":"Important: The <code>build()</code> Method","text":"<p>Unlike the Functional API, subclassed programs need to be built before first use. This tells Synalinks what input type to expect:</p> <pre><code>program = MyProgram(language_model=lm)\nawait program.build(InputDataModel)  # &lt;-- Required before first call!\n</code></pre> <p>If you used a subclassed module inside a functional API program, your module is built automatically!</p>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"The user query\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Your step by step thinking\")\n    answer: str = synalinks.Field(description=\"The correct answer\")\n\nclass ChainOfThought(synalinks.Program):\n    def __init__(self, language_model=None):\n        super().__init__()\n        self.answer_generator = synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=language_model,\n        )\n\n    async def call(self, inputs, training=False):\n        return await self.answer_generator(inputs)\n\n    def get_config(self):\n        return {\n            \"language_model\": synalinks.saving.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = synalinks.saving.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(language_model=language_model)\n\nasync def main():\n    load_dotenv()\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    program = ChainOfThought(language_model=language_model)\n    await program.build(Query)  # Required before first call!\n\n    result = await program(Query(query=\"What is 15% of 80?\"))\n    print(f\"Answer: {result['answer']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Subclassing: Inherit from <code>synalinks.Program</code> for full control over     program behavior and custom logic.</li> <li>Four Methods: Implement <code>__init__()</code>, <code>call()</code>, <code>get_config()</code>, and     <code>from_config()</code> for a complete subclassed program.</li> <li>Build Required: Call <code>await program.build(InputDataModel)</code> before     first use when using standalone subclassed programs.</li> <li>Serialization: <code>get_config()</code> and <code>from_config()</code> enable saving and     loading your custom programs.</li> </ul>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>LanguageModel</li> <li>Generator</li> <li>Program</li> </ul>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The output from our program - reasoning + final answer.</p> Source code in <code>examples/1b_subclassing.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"The output from our program - reasoning + final answer.\"\"\"\n\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Program</code></p> <p>A program that answers questions with step-by-step reasoning.</p> <p>Note: The first line of the docstring becomes the program's description if not explicitly provided in super().init().</p> Source code in <code>examples/1b_subclassing.py</code> <pre><code>class ChainOfThought(synalinks.Program):\n    \"\"\"A program that answers questions with step-by-step reasoning.\n\n    Note: The first line of the docstring becomes the program's description\n    if not explicitly provided in super().__init__().\n    \"\"\"\n\n    def __init__(self, language_model=None):\n        # Always call super().__init__() first!\n        super().__init__()\n\n        # Define the modules your program will use\n        # These are like instance variables in regular Python classes\n        self.answer_generator = synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=language_model,\n        )\n\n    async def call(\n        self, inputs: synalinks.JsonDataModel, training: bool = False\n    ) -&gt; synalinks.JsonDataModel:\n        \"\"\"Define how data flows through your program.\n\n        This method is called when you do `await program(input_data)`.\n\n        Args:\n            inputs (JsonDataModel): The input data (will be a Query instance)\n            training (bool): Whether we're in training mode (for optimization)\n\n        Returns:\n            JsonDataModel: The output data (will be an AnswerWithThinking instance)\n        \"\"\"\n        # In this simple case, we just pass inputs through one module\n        # More complex programs might have multiple steps, conditionals, etc.\n        result = await self.answer_generator(inputs)\n        return result\n\n    def get_config(self):\n        \"\"\"Return configuration needed to recreate this program.\n\n        This is called when saving the program to disk.\n        \"\"\"\n        config = {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        # Serialize the language model so it can be saved\n        language_model_config = {\n            \"language_model\": synalinks.saving.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n        return {**config, **language_model_config}\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Recreate the program from a configuration dict.\n\n        This is called when loading the program from disk.\n        \"\"\"\n        # Deserialize the language model first\n        language_model = synalinks.saving.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(language_model=language_model, **config)\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing.ChainOfThought.call","title":"<code>call(inputs, training=False)</code>  <code>async</code>","text":"<p>Define how data flows through your program.</p> <p>This method is called when you do <code>await program(input_data)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>JsonDataModel</code> <p>The input data (will be a Query instance)</p> required <code>training</code> <code>bool</code> <p>Whether we're in training mode (for optimization)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>JsonDataModel</code> <code>JsonDataModel</code> <p>The output data (will be an AnswerWithThinking instance)</p> Source code in <code>examples/1b_subclassing.py</code> <pre><code>async def call(\n    self, inputs: synalinks.JsonDataModel, training: bool = False\n) -&gt; synalinks.JsonDataModel:\n    \"\"\"Define how data flows through your program.\n\n    This method is called when you do `await program(input_data)`.\n\n    Args:\n        inputs (JsonDataModel): The input data (will be a Query instance)\n        training (bool): Whether we're in training mode (for optimization)\n\n    Returns:\n        JsonDataModel: The output data (will be an AnswerWithThinking instance)\n    \"\"\"\n    # In this simple case, we just pass inputs through one module\n    # More complex programs might have multiple steps, conditionals, etc.\n    result = await self.answer_generator(inputs)\n    return result\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing.ChainOfThought.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Recreate the program from a configuration dict.</p> <p>This is called when loading the program from disk.</p> Source code in <code>examples/1b_subclassing.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Recreate the program from a configuration dict.\n\n    This is called when loading the program from disk.\n    \"\"\"\n    # Deserialize the language model first\n    language_model = synalinks.saving.deserialize_synalinks_object(\n        config.pop(\"language_model\")\n    )\n    return cls(language_model=language_model, **config)\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing.ChainOfThought.get_config","title":"<code>get_config()</code>","text":"<p>Return configuration needed to recreate this program.</p> <p>This is called when saving the program to disk.</p> Source code in <code>examples/1b_subclassing.py</code> <pre><code>def get_config(self):\n    \"\"\"Return configuration needed to recreate this program.\n\n    This is called when saving the program to disk.\n    \"\"\"\n    config = {\n        \"name\": self.name,\n        \"description\": self.description,\n        \"trainable\": self.trainable,\n    }\n    # Serialize the language model so it can be saved\n    language_model_config = {\n        \"language_model\": synalinks.saving.serialize_synalinks_object(\n            self.language_model\n        )\n    }\n    return {**config, **language_model_config}\n</code></pre>"},{"location":"Code%20Examples/Subclassing/#examples.1b_subclassing.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The input to our program - a user's question.</p> Source code in <code>examples/1b_subclassing.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"The input to our program - a user's question.\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n</code></pre>"},{"location":"Code%20Examples/Training%20Programs/","title":"Training Programs","text":""},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--training-programs","title":"Training Programs","text":"<p>Like in machine learning, a LM application needs to be trained. In that case, we don't update the weights of the model, but optimize the prompts by automatically picking the best examples or generate instructions in order to help the program to perform better on your dataset.</p> <p>In production settings, this means that you can use smaller and more cost-effective models from your preferred provider while enhancing their accuracy with Synalinks.</p>"},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--training-flow","title":"Training Flow","text":"<pre><code>graph TB\n    subgraph Data\n        A[x_train, y_train]\n        B[x_test, y_test]\n    end\n    subgraph Training\n        C[program.compile] --&gt; D[program.fit]\n        D --&gt; E{val_reward improved?}\n        E --&gt;|Yes| F[Save Checkpoint]\n        E --&gt;|No| D\n    end\n    subgraph Evaluation\n        G[program.evaluate]\n    end\n    A --&gt; D\n    B --&gt; G\n    F --&gt; G</code></pre>"},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--loading-a-dataset","title":"Loading a Dataset","text":"<p>Synalinks provides built-in datasets for training and evaluation:</p> <pre><code>(x_train, y_train), (x_test, y_test) = synalinks.datasets.gsm8k.load_data()\n</code></pre>"},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--training-with-fit","title":"Training with <code>fit()</code>","text":"<p>Training a program is similar to Keras. Use the <code>fit()</code> method with your data:</p> <pre><code>history = await program.fit(\n    x=x_train,\n    y=y_train,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=32,\n    callbacks=[program_checkpoint_callback],\n)\n</code></pre>"},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--saving-and-loading-checkpoints","title":"Saving and Loading Checkpoints","text":"<p>Use callbacks to save the best performing program during training:</p> <pre><code>program_checkpoint_callback = synalinks.callbacks.ProgramCheckpoint(\n    filepath=\"checkpoint.program.json\",\n    monitor=\"val_reward\",\n    mode=\"max\",\n    save_best_only=True,\n)\n\n# Load the best checkpoint after training\nprogram.load(\"checkpoint.program.json\")\n</code></pre>"},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--key-takeaways","title":"Key Takeaways","text":"<ul> <li>Dataset Loading: Use built-in datasets or create your own for training.</li> <li>Training Loop: The <code>fit()</code> method handles the training process with     configurable epochs, batch size, and validation split.</li> <li>Checkpointing: Save the best performing model during training using     <code>ProgramCheckpoint</code> callback.</li> <li>Evaluation: Use <code>evaluate()</code> to measure performance before and after training.</li> </ul>"},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--program-visualization","title":"Program Visualization","text":""},{"location":"Code%20Examples/Training%20Programs/#examples.8_training_programs--api-references","title":"API References","text":"<ul> <li>Program Training API</li> <li>Rewards</li> <li>Optimizers</li> <li>Callbacks</li> <li>Built-in Datasets (GSM8K)</li> </ul>"},{"location":"Deployment/Building%20a%20REST%20API/","title":"Building a REST API","text":""},{"location":"Deployment/Building%20a%20REST%20API/#building-a-rest-api","title":"Building a REST API","text":"<p>The optimal approach to developing web-apps or micro-services using Synalinks involves building REST APIs and deploying them. You can deploy these APIs locally to test your system or on a cloud provider of your choice to scale to millions of users.</p> <p>For this purpose, you will need to use FastAPI, a Python library that makes it easy and straightforward to create REST APIs. If you use the default backend, the DataModel will be compatible with FastAPI as they both use Pydantic.</p> <p>In this tutorial we are going to make a backend that runs locally to test our system.</p>"},{"location":"Deployment/Building%20a%20REST%20API/#project-structure","title":"Project structure","text":"<p>Your project structure should look like this:</p> <pre><code>demo/\n\u251c\u2500\u2500 backend/\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 programs/\n\u2502   \u2502   \u2514\u2500\u2500 checkpoint.program.json\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 frontend/\n\u2502   \u2514\u2500\u2500 ... (your frontend code)\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 train.py (refer to the code examples to learn how to train programs)\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 .env.backend\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"Deployment/Building%20a%20REST%20API/#your-envbackend-file","title":"Your <code>.env.backend</code> file","text":"<p>This file contains your API keys and configuration:</p> .env.backend<pre><code>OPENAI_API_KEY=your-openai-api-key\n# Add other provider keys as needed\n</code></pre>"},{"location":"Deployment/Building%20a%20REST%20API/#your-requirementstxt-file","title":"Your <code>requirements.txt</code> file","text":"<p>Import additionally any necessary dependency:</p> requirements.txt<pre><code>fastapi[standard]\nuvicorn\npython-dotenv\nsynalinks\n</code></pre>"},{"location":"Deployment/Building%20a%20REST%20API/#creating-your-endpoint-using-fastapi-and-synalinks","title":"Creating your endpoint using FastAPI and Synalinks","text":"<p>Now you can create your endpoint using FastAPI.</p> main.py<pre><code>import argparse\nimport logging\nimport os\nimport uvicorn\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI\n\nimport synalinks\n\n# Load the environment variables\nload_dotenv()\n\n# Enable Synalinks built-in observability (uses MLflow)\nsynalinks.enable_observability(\n    tracking_uri=os.environ.get(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\"),\n    experiment_name=os.environ.get(\"EXPERIMENT_NAME\", \"production\"),\n)\n\n# Set up logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n# Set up FastAPI\napp = FastAPI()\n\n# The dictionary mapping the name of your custom modules to their class\ncustom_modules = {}\n\n# Load your program\nprogram = synalinks.Program.load(\n    \"programs/checkpoint.program.json\",\n    custom_modules=custom_modules,\n)\n\n\n@app.post(\"/v1/chat_completion\")\nasync def chat_completion(messages: synalinks.ChatMessages):\n    logger.info(messages.prettify_json())\n    try:\n        result = await program(messages)\n        if result:\n            logger.info(result.prettify_json())\n            return result.get_json()\n        else:\n            return None\n    except Exception as e:\n        logger.error(f\"Error occurred: {str(e)}\")\n        return None\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"127.0.0.1\")\n    parser.add_argument(\"--port\", type=int, default=8000)\n    args = parser.parse_args()\n    uvicorn.run(app, host=args.host, port=args.port)\n</code></pre>"},{"location":"Deployment/Building%20a%20REST%20API/#creating-the-dockerfile","title":"Creating the Dockerfile","text":"<p>Here is the Dockerfile to use according to FastAPI documentation.</p> Dockerfile<pre><code>FROM python:3.13\n\nWORKDIR /code\n\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\nCOPY ./app /code/app\nCOPY ./programs /code/programs\n\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"Deployment/Building%20a%20REST%20API/#the-docker-compose-file","title":"The docker compose file","text":"<p>And finally your docker compose file.</p> docker-compose.yml<pre><code>services:\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:latest\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./mlflow-data:/mlflow\n    command: &gt;\n      mlflow server\n      --host 0.0.0.0\n      --port 5000\n      --backend-store-uri sqlite:///mlflow/mlflow.db\n      --default-artifact-root mlflow-artifacts:/\n      --serve-artifacts\n      --artifacts-destination /mlflow/artifacts\n    restart: unless-stopped\n  backend:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n    ports:\n      - \"8000:8000\"\n    env_file:\n      - .env.backend\n    environment:\n      - MLFLOW_TRACKING_URI=http://mlflow:5000\n      - EXPERIMENT_NAME=production\n    depends_on:\n      - mlflow\n    restart: unless-stopped\n</code></pre>"},{"location":"Deployment/Building%20a%20REST%20API/#launching-your-backend","title":"Launching your backend","text":"<p>Launch your backend using <code>docker compose</code>:</p> <pre><code>cd demo\ndocker compose up\n</code></pre> <p>Open your browser to <code>http://0.0.0.0:8000/docs</code> and test your API with the FastAPI UI.</p> <p>You can view traces and metrics at <code>http://0.0.0.0:5000</code> (MLflow UI).</p>"},{"location":"Observability/MLflow/","title":"Observability with MLflow","text":""},{"location":"Observability/MLflow/#observability-with-mlflow","title":"Observability with MLflow","text":"<p>Synalinks provides built-in observability through MLflow, enabling you to trace and monitor your LM programs in production.</p>"},{"location":"Observability/MLflow/#overview","title":"Overview","text":"<p>The observability system automatically creates spans for each module call, capturing:</p> <ul> <li>Inputs and outputs of each module</li> <li>Duration of each call</li> <li>Cost information (when available from the language model)</li> <li>Success/failure status</li> <li>Parent-child relationships between nested module calls</li> </ul>"},{"location":"Observability/MLflow/#quick-start","title":"Quick Start","text":""},{"location":"Observability/MLflow/#enable-observability","title":"Enable Observability","text":"<p>Important: You must call <code>enable_observability()</code> BEFORE creating any modules. Hooks are registered when modules are instantiated, so enabling observability after module creation will not trace those modules.</p> <pre><code>import synalinks\n\n# Enable FIRST, before creating any modules\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"my_experiment\"\n)\n\n# Now create your modules - they will be automatically traced\ninputs = synalinks.Input(data_model=Question)\noutputs = await synalinks.Generator(...)(inputs)\n</code></pre> <p>Once enabled, all module calls in your program will be automatically traced.</p>"},{"location":"Observability/MLflow/#example-usage","title":"Example Usage","text":"<pre><code>import asyncio\nimport synalinks\n\n# Enable observability before creating your program\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"question_answering\"\n)\n\n\nclass Question(synalinks.DataModel):\n    question: str = synalinks.Field(description=\"The user's question\")\n\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The answer to the question\")\n\n\nasync def main():\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1\")\n\n    # Create a simple question-answering program\n    inputs = synalinks.Input(data_model=Question)\n    outputs = await synalinks.Generator(\n        data_model=Answer,\n        language_model=language_model,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"qa_program\",\n        description=\"A simple QA program\",\n    )\n\n    # Run the program - traces will be sent to MLflow\n    result = await program(Question(question=\"What is the capital of France?\"))\n    if result:\n        print(result.prettify_json())\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"Observability/MLflow/#running-mlflow-with-docker","title":"Running MLflow with Docker","text":""},{"location":"Observability/MLflow/#using-docker","title":"Using Docker","text":"<p>Run MLflow tracking server locally with artifact proxying enabled:</p> <pre><code>docker run -d \\\n    --name mlflow \\\n    -p 5000:5000 \\\n    -v mlflow-data:/mlflow \\\n    ghcr.io/mlflow/mlflow:latest \\\n    mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri sqlite:///mlflow/mlflow.db \\\n    --default-artifact-root mlflow-artifacts:/ \\\n    --serve-artifacts \\\n    --artifacts-destination /mlflow/artifacts\n</code></pre> <p>Important flags:</p> <ul> <li><code>--serve-artifacts</code>: Enables the MLflow server to proxy artifact uploads from clients</li> <li><code>--default-artifact-root mlflow-artifacts:/</code>: Tells clients to use the server as an artifact proxy</li> <li><code>--artifacts-destination /mlflow/artifacts</code>: Where the server stores artifacts on disk</li> </ul> <p>Then configure Synalinks to use it:</p> <pre><code>import synalinks\n\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"synalinks_traces\"\n)\n</code></pre>"},{"location":"Observability/MLflow/#using-docker-compose","title":"Using Docker Compose","text":"<p>For a more complete setup with persistent storage, create a <code>docker-compose.yml</code>:</p> <pre><code>services:\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:latest\n    container_name: mlflow\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./mlflow-data:/mlflow\n    command: &gt;\n      mlflow server\n      --host 0.0.0.0\n      --port 5000\n      --backend-store-uri sqlite:///mlflow/mlflow.db\n      --default-artifact-root mlflow-artifacts:/\n      --serve-artifacts\n      --artifacts-destination /mlflow/artifacts\n    restart: unless-stopped\n</code></pre> <p>Start the services:</p> <pre><code>docker compose up -d\n</code></pre> <p>Access the MLflow UI at http://localhost:5000.</p>"},{"location":"Observability/MLflow/#production-setup-with-postgresql","title":"Production Setup with PostgreSQL","text":"<p>For production deployments, use PostgreSQL as the backend store:</p> <pre><code>services:\n  postgres:\n    image: postgres:16-alpine\n    container_name: mlflow-postgres\n    environment:\n      POSTGRES_USER: mlflow\n      POSTGRES_PASSWORD: mlflow\n      POSTGRES_DB: mlflow\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U mlflow\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n    restart: unless-stopped\n\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:latest\n    container_name: mlflow\n    depends_on:\n      postgres:\n        condition: service_healthy\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - mlflow-artifacts:/mlflow/artifacts\n    environment:\n      MLFLOW_BACKEND_STORE_URI: postgresql://mlflow:mlflow@postgres:5432/mlflow\n    command: &gt;\n      mlflow server\n      --host 0.0.0.0\n      --port 5000\n      --backend-store-uri postgresql://mlflow:mlflow@postgres:5432/mlflow\n      --default-artifact-root mlflow-artifacts:/\n      --serve-artifacts\n      --artifacts-destination /mlflow/artifacts\n    restart: unless-stopped\n\nvolumes:\n  postgres-data:\n  mlflow-artifacts:\n</code></pre>"},{"location":"Observability/MLflow/#understanding-traces","title":"Understanding Traces","text":"<p>When you run a Synalinks program with observability enabled, MLflow captures detailed traces.</p>"},{"location":"Observability/MLflow/#span-types","title":"Span Types","text":"<p>Synalinks automatically categorizes spans based on module type for better visualization in MLflow:</p> Module Span Type <code>Generator</code>, <code>ChainOfThought</code>, <code>SelfCritique</code> <code>LLM</code> <code>FunctionCallingAgent</code> <code>AGENT</code> <code>EmbedKnowledge</code>, <code>RetrieveKnowledge</code>, <code>UpdateKnowledge</code> <code>RETRIEVER</code> <code>Tool</code> <code>TOOL</code> Other modules <code>CHAIN</code>"},{"location":"Observability/MLflow/#span-attributes","title":"Span Attributes","text":"<p>Each span includes these attributes:</p> Attribute Description <code>synalinks.call_id</code> Unique identifier for this call <code>synalinks.parent_call_id</code> ID of the parent call (for nested modules) <code>synalinks.module</code> Module class name (e.g., <code>Generator</code>) <code>synalinks.module_name</code> Custom name given to the module <code>synalinks.module_description</code> Module description <code>synalinks.is_symbolic</code> Whether the call was symbolic (graph building) <code>synalinks.duration</code> Call duration in seconds <code>synalinks.success</code> Whether the call succeeded <code>synalinks.cost</code> LLM API cost (when available)"},{"location":"Observability/MLflow/#exception-events","title":"Exception Events","text":"<p>When a module call fails, the span automatically records an exception event with: - <code>exception.type</code>: The exception class name - <code>exception.message</code>: The exception message</p>"},{"location":"Observability/MLflow/#viewing-traces","title":"Viewing Traces","text":"<ol> <li>Open MLflow UI at <code>http://localhost:5000</code></li> <li>Navigate to your experiment</li> <li>Click on a run to see detailed traces</li> <li>Use the trace view to explore the call hierarchy</li> </ol>"},{"location":"Observability/MLflow/#configuration-options","title":"Configuration Options","text":""},{"location":"Observability/MLflow/#environment-variables","title":"Environment Variables","text":"<p>You can also configure MLflow using environment variables:</p> <pre><code>export MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre> <p>Then in your code:</p> <pre><code>import synalinks\n\n# Will use MLFLOW_TRACKING_URI from environment\nsynalinks.enable_observability(experiment_name=\"my_experiment\")\n</code></pre>"},{"location":"Observability/MLflow/#direct-monitor-hook","title":"Direct Monitor Hook","text":"<p>For fine-grained control, you can create a Monitor hook directly:</p> <pre><code>import synalinks\n\nmonitor = synalinks.hooks.Monitor(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"custom_experiment\"\n)\n\n# Add to a specific module\ngenerator = synalinks.Generator(\n    data_model=Answer,\n    hooks=[monitor]\n)\n</code></pre>"},{"location":"Observability/MLflow/#training-metrics-and-artifacts","title":"Training Metrics and Artifacts","text":"<p>The <code>Monitor</code> callback logs training metrics and program artifacts to MLflow during <code>fit()</code>.</p>"},{"location":"Observability/MLflow/#basic-usage","title":"Basic Usage","text":"<pre><code>import synalinks\n\n# Create the monitor callback\nmonitor = synalinks.callbacks.Monitor(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"training_experiment\",\n    run_name=\"my_training_run\",\n    log_program_plot=True,  # Save program visualization as artifact\n)\n\n# Use during training\nprogram.fit(\n    x=train_inputs,\n    y=train_labels,\n    epochs=10,\n    callbacks=[monitor]\n)\n</code></pre>"},{"location":"Observability/MLflow/#program-plot-artifact","title":"Program Plot Artifact","text":"<p>When <code>log_program_plot=True</code> (the default), the Monitor callback automatically saves a visualization of your program architecture as an MLflow artifact at the start of training.</p> <p>The plot is saved under <code>program_plots/</code> in the artifacts folder and includes:</p> <ul> <li>Module names and types</li> <li>Input/output schemas</li> <li>Trainable status of each module</li> </ul> <p>You can view the program plot in the MLflow UI under the \"Artifacts\" tab of your run.</p>"},{"location":"Observability/MLflow/#program-model-artifact","title":"Program Model Artifact","text":"<p>When <code>log_program_model=True</code> (the default), the Monitor callback saves the program's trainable state at the end of training. This includes:</p> <ul> <li><code>model/state_tree.json</code>: Contains all trainable variables (few-shot examples, optimized prompts, etc.)</li> <li><code>model/model_info.json</code>: Metadata about the program (name, description, number of trainable variables)</li> </ul> <p>This is useful for:</p> <ul> <li>Checkpointing learned parameters during optimization</li> <li>Comparing different training runs</li> <li>Restoring program state for inference</li> </ul>"},{"location":"Observability/MLflow/#callback-parameters","title":"Callback Parameters","text":"Parameter Default Description <code>experiment_name</code> Program name MLflow experiment name <code>run_name</code> Auto-generated MLflow run name <code>tracking_uri</code> Local <code>./mlruns</code> MLflow tracking server URI <code>log_batch_metrics</code> <code>False</code> Log metrics at batch level <code>log_epoch_metrics</code> <code>True</code> Log metrics at epoch level <code>log_program_plot</code> <code>True</code> Save program visualization as artifact <code>log_program_model</code> <code>True</code> Save program trainable state as artifact <code>tags</code> <code>{}</code> Additional tags for the run"},{"location":"Observability/MLflow/#example-with-full-configuration","title":"Example with Full Configuration","text":"<pre><code>import synalinks\n\nmonitor = synalinks.callbacks.Monitor(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"gsm8k_optimization\",\n    run_name=\"chain_of_thought_v1\",\n    log_batch_metrics=True,\n    log_epoch_metrics=True,\n    log_program_plot=True,\n    tags={\n        \"model\": \"gpt-4o-mini\",\n        \"optimizer\": \"RandomFewShot\",\n        \"dataset\": \"gsm8k\"\n    }\n)\n\nprogram.fit(\n    x=train_questions,\n    y=train_answers,\n    epochs=5,\n    callbacks=[monitor]\n)\n</code></pre>"},{"location":"Observability/MLflow/#combining-tracing-with-training","title":"Combining Tracing with Training","text":"<p>When using both <code>enable_observability()</code> and the <code>Monitor</code> callback for training, traces are created in different experiments depending on the context:</p> <ol> <li> <p>During program building (symbolic calls): Traces go to the experiment specified    in <code>enable_observability()</code></p> </li> <li> <p>During training (<code>fit()</code>): Traces are associated with the training run and go to    the experiment specified in the <code>Monitor</code> callback</p> </li> </ol>"},{"location":"Observability/MLflow/#full-example","title":"Full Example","text":"<pre><code>import synalinks\n\n# Enable tracing for all module calls\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"synalinks_traces\"  # Traces during setup go here\n)\n\n# Create your program (symbolic traces created here)\ninputs = synalinks.Input(data_model=Question)\noutputs = await synalinks.Generator(\n    data_model=Answer,\n    language_model=language_model,\n)(inputs)\n\nprogram = synalinks.Program(inputs=inputs, outputs=outputs, name=\"my_program\")\n\n# Create Monitor callback for training\nmonitor = synalinks.callbacks.Monitor(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"training_runs\",  # Training metrics + traces go here\n    run_name=\"experiment_v1\",\n)\n\n# Train - traces during fit() are associated with the training run\nprogram.compile(reward=reward, optimizer=optimizer)\nawait program.fit(x=train_x, y=train_y, epochs=5, callbacks=[monitor])\n</code></pre> <p>After training, you'll have: - synalinks_traces experiment: Setup traces (symbolic module calls) - training_runs experiment: Training run with metrics, artifacts, and execution traces</p>"},{"location":"Observability/MLflow/#best-practices","title":"Best Practices","text":"<ol> <li>Enable observability early in your script, before creating any modules</li> <li>Use meaningful experiment names to organize your traces by project or feature</li> <li>Use persistent storage (PostgreSQL) for production deployments</li> <li>Set up retention policies to manage storage for long-running applications</li> </ol>"},{"location":"Observability/MLflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Observability/MLflow/#no-traces-being-created","title":"No traces being created","text":"<p>If you don't see any traces in MLflow:</p> <ol> <li> <p>Check call order: Ensure <code>enable_observability()</code> is called before creating any modules    </p><pre><code># Wrong - modules created before enabling observability\ninputs = synalinks.Input(data_model=Question)\nsynalinks.enable_observability()  # Too late!\n\n# Correct - enable first\nsynalinks.enable_observability()\ninputs = synalinks.Input(data_model=Question)  # Now traces will be created\n</code></pre><p></p> </li> <li> <p>Verify observability is enabled: Check with <code>synalinks.is_observability_enabled()</code></p> </li> <li> <p>Check the correct experiment: During training, traces go to the training experiment,    not the observability experiment</p> </li> </ol>"},{"location":"Observability/MLflow/#mlflow-not-receiving-traces","title":"MLflow not receiving traces","text":"<ol> <li>Verify the MLflow server is running: <code>curl http://localhost:5000/health</code></li> <li>Check the tracking URI is correct</li> <li>Ensure <code>mlflow</code> package is installed: <code>pip install mlflow</code></li> </ol>"},{"location":"Observability/MLflow/#artifacts-not-showing-in-mlflow-ui","title":"Artifacts not showing in MLflow UI","text":"<p>If artifacts are uploaded but don't appear in the MLflow UI:</p> <ol> <li>Check server configuration: Ensure the MLflow server is started with <code>--serve-artifacts</code> flag</li> <li>Verify artifact root: The server must use <code>--default-artifact-root mlflow-artifacts:/</code> for remote clients</li> <li>Check permissions: The server needs write access to <code>--artifacts-destination</code> path</li> </ol> <p>Correct server configuration: </p><pre><code>mlflow server \\\n    --serve-artifacts \\\n    --default-artifact-root mlflow-artifacts:/ \\\n    --artifacts-destination /mlflow/artifacts\n</code></pre><p></p> <p>Common mistake - Missing <code>--serve-artifacts</code> causes clients to try writing directly to the server's local filesystem, resulting in permission errors like: </p><pre><code>PermissionError: [Errno 13] Permission denied: '/mlflow'\n</code></pre><p></p>"},{"location":"Observability/MLflow/#missing-cost-information","title":"Missing cost information","text":"<p>Cost tracking requires the language model to return usage information. Ensure your LLM provider supports this feature.</p>"},{"location":"Synalinks%20API/Config/","title":"Config","text":""},{"location":"Synalinks%20API/Config/#synalinks.src.backend.common.global_state.clear_session","title":"<code>clear_session(free_memory=True)</code>","text":"<p>Resets all state generated by synalinks.</p> <p>synalinks manages a global state, which it uses to implement the Functional model-building API and to uniquify autogenerated modules names.</p> <p>If you are creating many models in a loop, this global state will consume an increasing amount of memory over time, and you may want to clear it. Calling <code>clear_session()</code> releases the global state: this helps avoid clutter from old programs and modules, especially when memory is limited.</p> <p>Parameters:</p> Name Type Description Default <code>free_memory</code> <code>bool</code> <p>Whether to call Python garbage collection. It's usually a good practice to call it to make sure memory used by deleted objects is immediately freed. However, it may take a few seconds to execute, so when using <code>clear_session()</code> in a short loop, you may want to skip it.</p> <code>True</code> Source code in <code>synalinks/src/backend/common/global_state.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.utils.clear_session\",\n        \"synalinks.backend.clear_session\",\n        \"synalinks.clear_session\",\n    ]\n)\ndef clear_session(free_memory=True):\n    \"\"\"Resets all state generated by synalinks.\n\n    synalinks manages a global state, which it uses to implement the Functional\n    model-building API and to uniquify autogenerated modules names.\n\n    If you are creating many models in a loop, this global state will consume\n    an increasing amount of memory over time, and you may want to clear it.\n    Calling `clear_session()` releases the global state: this helps avoid\n    clutter from old programs and modules, especially when memory is limited.\n\n    Args:\n        free_memory (bool): Whether to call Python garbage collection.\n            It's usually a good practice to call it to make sure\n            memory used by deleted objects is immediately freed.\n            However, it may take a few seconds to execute, so\n            when using `clear_session()` in a short loop,\n            you may want to skip it.\n    \"\"\"\n    global GLOBAL_STATE_TRACKER\n    global GLOBAL_SETTINGS_TRACKER\n\n    GLOBAL_STATE_TRACKER = threading.local()\n    GLOBAL_SETTINGS_TRACKER = threading.local()\n\n    if free_memory:\n        # Manually trigger garbage collection.\n        gc.collect()\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.SynalinksFileFormatter","title":"<code>SynalinksFileFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Formatter for file logging that removes ANSI escape codes.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>class SynalinksFileFormatter(logging.Formatter):\n    \"\"\"Formatter for file logging that removes ANSI escape codes.\"\"\"\n\n    ANSI_ESCAPE_PATTERN = re.compile(r\"\\x1b\\[[0-9;]*m\")\n\n    def format(self, record):\n        record.msg = self.ANSI_ESCAPE_PATTERN.sub(\"\", str(record.msg))\n        return super().format(record)\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.SynalinksLogFormatter","title":"<code>SynalinksLogFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Formatter for console logging with colors and prefix.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>class SynalinksLogFormatter(logging.Formatter):\n    \"\"\"Formatter for console logging with colors and prefix.\"\"\"\n\n    blue = \"\\x1b[34m\"\n    bold_red = \"\\x1b[31;1m\"\n    reset = \"\\x1b[0m\"\n    prefix = \"\ud83e\udde0\ud83d\udd17 Synalinks: \"\n\n    FORMATS = {\n        logging.DEBUG: f\"(DEBUG) {prefix}%(message)s\",\n        logging.INFO: f\"{prefix}%(message)s\",\n        logging.WARNING: f\"{prefix}%(message)s\",\n        logging.ERROR: f\"{bold_red}{prefix}%(message)s{reset}\",\n        logging.CRITICAL: f\"{bold_red}{prefix}%(message)s{reset}\",\n    }\n\n    def format(self, record):\n        log_fmt = self.FORMATS.get(record.levelno, self.FORMATS[logging.INFO])\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.api_base","title":"<code>api_base()</code>","text":"<p>Returns the Synalinks api base</p> <p>Returns:</p> Type Description <code>str</code> <p>The observability api base</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.api_base\",\n        \"synalinks.backend.api_base\",\n        \"synalinks.api_base\",\n    ]\n)\ndef api_base():\n    \"\"\"Returns the Synalinks api base\n\n    Returns:\n        (str): The observability api base\n    \"\"\"\n    return _SYNALINKS_API_BASE\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.api_key","title":"<code>api_key()</code>","text":"<p>Synalinks API key.</p> <p>Returns:</p> Type Description <code>str</code> <p>Synalinks API key.</p> <pre><code>&gt;&gt;&gt; synalinks.config.api_key()\n'my-secret-api-key'\n</code></pre> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.api_key\",\n        \"synalinks.backend.api_key\",\n    ]\n)\ndef api_key():\n    \"\"\"Synalinks API key.\n\n    Returns:\n        (str): Synalinks API key.\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.api_key()\n    'my-secret-api-key'\n    ```\n\n    \"\"\"\n    return _SYNALINKS_API_KEY\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.backend","title":"<code>backend()</code>","text":"<p>Publicly accessible method for determining the current backend.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the backend synalinks is currently using. like <code>\"pydantic\"</code>.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; synalinks.config.backend()\n'pydantic'\n</code></pre> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export([\"synalinks.config.backend\", \"synalinks.backend.backend\"])\ndef backend():\n    \"\"\"Publicly accessible method for determining the current backend.\n\n    Returns:\n        (str): The name of the backend synalinks is currently using. like\n            `\"pydantic\"`.\n\n    Example:\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.backend()\n    'pydantic'\n    ```\n    \"\"\"\n    return _BACKEND\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.enable_logging","title":"<code>enable_logging(log_level='debug', log_to_file=False)</code>","text":"<p>Configures and enables logging for the application.</p> <p>This function sets up the logging configuration for the application, allowing logs to be output either to a specified file or to the console. The logging level can be set to DEBUG for more verbose logging or INFO for standard logging.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>str</code> <p>The logging level.</p> <code>'debug'</code> <code>log_to_file</code> <code>bool</code> <p>If True save the logs into <code>synalinks.log</code></p> <code>False</code> <p>The log message format includes the timestamp, log level, and the log message itself.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.enable_logging\",\n        \"synalinks.backend.enable_logging\",\n        \"synalinks.enable_logging\",\n    ]\n)\ndef enable_logging(log_level=\"debug\", log_to_file=False):\n    \"\"\"\n    Configures and enables logging for the application.\n\n    This function sets up the logging configuration for the application,\n    allowing logs to be output either to a specified file or to the console.\n    The logging level can be set to DEBUG for more verbose logging or INFO\n    for standard logging.\n\n    Args:\n        log_level (str): The logging level.\n        log_to_file (bool): If True save the logs into `synalinks.log`\n\n    The log message format includes the timestamp, log level, and the log message itself.\n    \"\"\"\n    global logger\n\n    log_level = log_level.upper()\n    if log_level and hasattr(logging, log_level):\n        log_level = getattr(logging, log_level)\n\n    logger.setLevel(log_level)\n\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(log_level)\n    stream_handler.setFormatter(SynalinksLogFormatter())\n    logger.addHandler(stream_handler)\n\n    if log_to_file:\n        file_handler = logging.FileHandler(\"synalinks.log\", mode=\"w\")\n        file_handler.setLevel(log_level)\n        formatter = SynalinksLogFormatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.enable_observability","title":"<code>enable_observability(tracking_uri=None, experiment_name=None)</code>","text":"<p>Configures and enables observability for the application using MLflow.</p> <p>This function sets up the observability configuration for the application, enabling tracing of module calls via MLflow.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str</code> <p>Optional. The MLflow tracking server URI. If not provided, MLflow will use the default (local ./mlruns directory or MLFLOW_TRACKING_URI environment variable).</p> <code>None</code> <code>experiment_name</code> <code>str</code> <p>Optional. The MLflow experiment name. Defaults to \"synalinks_traces\".</p> <code>None</code> <p>Example:</p> <pre><code>import synalinks\n\n# Basic usage with defaults\nsynalinks.enable_observability()\n\n# With custom MLflow tracking server\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"my_experiment\"\n)\n</code></pre> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.enable_observability\",\n        \"synalinks.backend.enable_observability\",\n        \"synalinks.enable_observability\",\n    ]\n)\ndef enable_observability(tracking_uri=None, experiment_name=None):\n    \"\"\"\n    Configures and enables observability for the application using MLflow.\n\n    This function sets up the observability configuration for the application,\n    enabling tracing of module calls via MLflow.\n\n    Args:\n        tracking_uri (str): Optional. The MLflow tracking server URI.\n            If not provided, MLflow will use the default (local ./mlruns\n            directory or MLFLOW_TRACKING_URI environment variable).\n        experiment_name (str): Optional. The MLflow experiment name.\n            Defaults to \"synalinks_traces\".\n\n    Example:\n\n    ```python\n    import synalinks\n\n    # Basic usage with defaults\n    synalinks.enable_observability()\n\n    # With custom MLflow tracking server\n    synalinks.enable_observability(\n        tracking_uri=\"http://localhost:5000\",\n        experiment_name=\"my_experiment\"\n    )\n    ```\n    \"\"\"\n    global _MLFLOW_TRACKING_URI\n    global _MLFLOW_EXPERIMENT_NAME\n    global _ENABLE_OBSERVABILITY\n\n    if tracking_uri:\n        _MLFLOW_TRACKING_URI = tracking_uri\n    if experiment_name:\n        _MLFLOW_EXPERIMENT_NAME = experiment_name\n    _ENABLE_OBSERVABILITY = True\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.epsilon","title":"<code>epsilon()</code>","text":"<p>Return the value of the fuzz factor used in numeric expressions.</p> <p>Returns:</p> Type Description <code>float</code> <p>The epsilon value.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; synalinks.config.epsilon()\n1e-07\n</code></pre> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export([\"synalinks.config.epsilon\", \"synalinks.backend.epsilon\"])\ndef epsilon():\n    \"\"\"Return the value of the fuzz factor used in numeric expressions.\n\n    Returns:\n        (float): The epsilon value.\n\n    Example:\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.epsilon()\n    1e-07\n    ```\n\n    \"\"\"\n    return _EPSILON\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.floatx","title":"<code>floatx()</code>","text":"<p>Return the default float type, as a string.</p> <p>E.g. <code>'bfloat16'</code>, <code>'float16'</code>, <code>'float32'</code>, <code>'float64'</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>The current default float type.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; synalinks.config.floatx()\n'float32'\n</code></pre> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export([\"synalinks.config.floatx\", \"synalinks.backend.floatx\"])\ndef floatx():\n    \"\"\"Return the default float type, as a string.\n\n    E.g. `'bfloat16'`, `'float16'`, `'float32'`, `'float64'`.\n\n    Returns:\n        (str): The current default float type.\n\n    Example:\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.floatx()\n    'float32'\n    ```\n\n    \"\"\"\n    return _FLOATX\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.is_observability_enabled","title":"<code>is_observability_enabled()</code>","text":"<p>Check if the observability is enabled</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the observability is enabled.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.is_observability_enabled\",\n        \"synalinks.backend.is_observability_enabled\",\n        \"synalinks.is_observability_enabled\",\n    ]\n)\ndef is_observability_enabled():\n    \"\"\"Check if the observability is enabled\n\n    Returns:\n        (bool): True if the observability is enabled.\n    \"\"\"\n    return _ENABLE_OBSERVABILITY\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.mlflow_experiment_name","title":"<code>mlflow_experiment_name()</code>","text":"<p>Returns the MLflow experiment name for observability.</p> <p>Returns:</p> Type Description <code>str</code> <p>The MLflow experiment name.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.mlflow_experiment_name\",\n        \"synalinks.backend.mlflow_experiment_name\",\n    ]\n)\ndef mlflow_experiment_name():\n    \"\"\"Returns the MLflow experiment name for observability.\n\n    Returns:\n        (str): The MLflow experiment name.\n    \"\"\"\n    return _MLFLOW_EXPERIMENT_NAME\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.mlflow_tracking_uri","title":"<code>mlflow_tracking_uri()</code>","text":"<p>Returns the MLflow tracking URI for observability.</p> <p>Returns:</p> Type Description <code>str</code> <p>The MLflow tracking URI, or None if not set.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.mlflow_tracking_uri\",\n        \"synalinks.backend.mlflow_tracking_uri\",\n    ]\n)\ndef mlflow_tracking_uri():\n    \"\"\"Returns the MLflow tracking URI for observability.\n\n    Returns:\n        (str): The MLflow tracking URI, or None if not set.\n    \"\"\"\n    return _MLFLOW_TRACKING_URI\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.set_api_base","title":"<code>set_api_base(api_base)</code>","text":"<p>Set the observability api base</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The observability api base</p> required Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.set_api_base\",\n        \"synalinks.backend.set_api_base\",\n        \"synalinks.set_api_base\",\n    ]\n)\ndef set_api_base(api_base):\n    \"\"\"Set the observability api base\n\n    Args:\n        api_base (str): The observability api base\n    \"\"\"\n    global _SYNALINKS_API_BASE\n    _SYNALINKS_API_BASE = api_base\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.set_api_key","title":"<code>set_api_key(key)</code>","text":"<p>Set Synalinks API key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The API key value.</p> required <p>The API key is retrieved from the env variables at start.</p> <pre><code>&gt;&gt;&gt; os.environ[\"SYNALINKS_API_KEY\"] = 'my-secret-api-key'\n</code></pre> <p>Or you can setup it using the config</p> <pre><code>&gt;&gt;&gt; synalinks.config.set_api_key('my-secret-api-key')\n&gt;&gt;&gt; synalinks.config.api_key()\n'my-secret-api-key'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Synalinks API key.</p> required Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.set_api_key\",\n        \"synalinks.backend.set_api_key\",\n    ]\n)\ndef set_api_key(key):\n    \"\"\"Set Synalinks API key.\n\n    Args:\n        key (str): The API key value.\n\n    The API key is retrieved from the env variables at start.\n\n    ```python\n    &gt;&gt;&gt; os.environ[\"SYNALINKS_API_KEY\"] = 'my-secret-api-key'\n    ```\n\n    Or you can setup it using the config\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.set_api_key('my-secret-api-key')\n    &gt;&gt;&gt; synalinks.config.api_key()\n    'my-secret-api-key'\n    ```\n\n    Args:\n        key (str): Synalinks API key.\n    \"\"\"\n    global _SYNALINKS_API_KEY\n    _SYNALINKS_API_KEY = key\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.set_epsilon","title":"<code>set_epsilon(value)</code>","text":"<p>Set the value of the fuzz factor used in numeric expressions.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The new value of epsilon.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; synalinks.config.epsilon()\n1e-07\n</code></pre> <pre><code>&gt;&gt;&gt; synalinks.config.set_epsilon(1e-5)\n&gt;&gt;&gt; synalinks.config.epsilon()\n1e-05\n</code></pre> <pre><code>&gt;&gt;&gt; # Set it back to the default value.\n&gt;&gt;&gt; synalinks.config.set_epsilon(1e-7)\n</code></pre> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export([\"synalinks.config.set_epsilon\", \"synalinks.backend.set_epsilon\"])\ndef set_epsilon(value):\n    \"\"\"Set the value of the fuzz factor used in numeric expressions.\n\n    Args:\n        value (float): The new value of epsilon.\n\n    Examples:\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.epsilon()\n    1e-07\n    ```\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.set_epsilon(1e-5)\n    &gt;&gt;&gt; synalinks.config.epsilon()\n    1e-05\n    ```\n\n    ```python\n    &gt;&gt;&gt; # Set it back to the default value.\n    &gt;&gt;&gt; synalinks.config.set_epsilon(1e-7)\n    ```\n\n    \"\"\"\n    global _EPSILON\n    _EPSILON = value\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.set_floatx","title":"<code>set_floatx(value)</code>","text":"<p>Set the default float dtype.</p> <p>Note: It is not recommended to set this to <code>\"float16\"</code>, as this will likely cause numeric stability issues. Instead, use <code>float64</code> or <code>float32</code>.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The float type between <code>'bfloat16'</code>, <code>'float16'</code>, <code>'float32'</code>, or <code>'float64'</code>.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; synalinks.config.floatx()\n'float32'\n</code></pre> <pre><code>&gt;&gt;&gt; synalinks.config.set_floatx('float64')\n&gt;&gt;&gt; synalinks.config.floatx()\n'float64'\n</code></pre> <pre><code>&gt;&gt;&gt; # Set it back to float32\n&gt;&gt;&gt; synalinks.config.set_floatx('float32')\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>In case of invalid value.</p> Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export([\"synalinks.config.set_floatx\", \"synalinks.backend.set_floatx\"])\ndef set_floatx(value):\n    \"\"\"Set the default float dtype.\n\n    Note: It is not recommended to set this to `\"float16\"`,\n    as this will likely cause numeric stability issues.\n    Instead, use `float64` or `float32`.\n\n    Args:\n        value (str): The float type between `'bfloat16'`, `'float16'`, `'float32'`,\n            or `'float64'`.\n\n    Examples:\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.floatx()\n    'float32'\n    ```\n\n    ```python\n    &gt;&gt;&gt; synalinks.config.set_floatx('float64')\n    &gt;&gt;&gt; synalinks.config.floatx()\n    'float64'\n    ```\n\n    ```python\n    &gt;&gt;&gt; # Set it back to float32\n    &gt;&gt;&gt; synalinks.config.set_floatx('float32')\n    ```\n\n    Raises:\n        ValueError: In case of invalid value.\n    \"\"\"\n    global _FLOATX\n    accepted_dtypes = {\"bfloat16\", \"float16\", \"float32\", \"float64\"}\n    if value not in accepted_dtypes:\n        raise ValueError(\n            f\"Unknown `floatx` value: {value}. Expected one of {accepted_dtypes}\"\n        )\n    _FLOATX = str(value)\n</code></pre>"},{"location":"Synalinks%20API/Config/#synalinks.src.backend.config.set_seed","title":"<code>set_seed(value)</code>","text":"<p>Set the value of the random seed for reproductability.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The new value of epsilon.</p> required Source code in <code>synalinks/src/backend/config.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.config.set_seed\",\n        \"synalinks.backend.set_seed\",\n        \"synalinks.set_seed\",\n    ]\n)\ndef set_seed(value):\n    \"\"\"Set the value of the random seed for reproductability.\n\n    Args:\n        value (float): The new value of epsilon.\n    \"\"\"\n    global _RANDOM_SEED\n    _RANDOM_SEED = value\n    np.random.seed(_RANDOM_SEED)\n    random.seed(_RANDOM_SEED)\n</code></pre>"},{"location":"Synalinks%20API/Embedding%20Models%20API/","title":"Embedding Models API","text":""},{"location":"Synalinks%20API/Embedding%20Models%20API/#embedding-models-api","title":"Embedding Models API","text":""},{"location":"Synalinks%20API/Embedding%20Models%20API/#synalinks.src.embedding_models.embedding_model.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>               Bases: <code>SynalinksSaveable</code></p> <p>An embedding model API wrapper.</p> <p>Embedding models are a type of machine learning model used to convert high-dimensional data, such as text into lower-dimensional vector representations while preserving the semantic meaning and relationships within the data. These vector representations, known as embeddings, allow for more efficient and effective processing in various tasks.</p> <p>Many providers are available like OpenAI, Azure OpenAI, Vertex AI or Ollama.</p> <p>For the complete list of models, please refer to the providers documentation.</p> <p>Using OpenAI models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nembedding_model = synalinks.EmbeddingModel(\n    model=\"openai/text-embedding-ada-002\",\n)\n</code></pre> <p>Using Azure OpenAI models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"AZURE_API_KEY\"] = \"your-api-key\"\nos.environ[\"AZURE_API_BASE\"] = \"your-api-base\"\nos.environ[\"AZURE_API_VERSION\"] = \"your-api-version\"\n\nembedding_model = synalinks.EmbeddingModel(\n    model=\"azure/&lt;your_deployment_name&gt;\",\n)\n</code></pre> <p>Using VertexAI models</p> <pre><code>import synalinks\nimport os\n\nembedding_model = synalinks.EmbeddingModel(\n    model=\"vertex_ai/text-embedding-004\",\n    vertex_project = \"hardy-device-38811\", # Your Project ID\n    vertex_location = \"us-central1\",  # Project location\n)\n</code></pre> <p>Using Ollama models</p> <pre><code>import synalinks\n\nembedding_model = synalinks.EmbeddingModel(\n    model=\"ollama/mxbai-embed-large\",\n)\n</code></pre> <p>Note: Obviously, use an <code>.env</code> file and <code>.gitignore</code> to avoid putting your API keys in the code or a config file that can lead to leackage when pushing it into repositories.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use.</p> <code>None</code> <code>api_base</code> <code>str</code> <p>Optional. The endpoint to use.</p> <code>None</code> <code>retry</code> <code>int</code> <p>Optional. The number of retry.</p> <code>5</code> <code>fallback</code> <code>EmbeddingModel</code> <p>Optional. The embedding model to fallback if anything is wrong.</p> <code>None</code> <code>caching</code> <code>bool</code> <p>Enables caching (Default to True).</p> <code>True</code> Source code in <code>synalinks/src/embedding_models/embedding_model.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.EmbeddingModel\",\n        \"synalinks.embedding_models.EmbeddingModel\",\n    ]\n)\nclass EmbeddingModel(SynalinksSaveable):\n    \"\"\"An embedding model API wrapper.\n\n    Embedding models are a type of machine learning model used to convert\n    high-dimensional data, such as text into lower-dimensional vector\n    representations while preserving the semantic meaning and relationships\n    within the data. These vector representations, known as embeddings,\n    allow for more efficient and effective processing in various tasks.\n\n    Many providers are available like OpenAI, Azure OpenAI, Vertex AI or Ollama.\n\n    For the complete list of models, please refer to the providers documentation.\n\n    **Using OpenAI models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n    embedding_model = synalinks.EmbeddingModel(\n        model=\"openai/text-embedding-ada-002\",\n    )\n    ```\n\n    **Using Azure OpenAI models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"AZURE_API_KEY\"] = \"your-api-key\"\n    os.environ[\"AZURE_API_BASE\"] = \"your-api-base\"\n    os.environ[\"AZURE_API_VERSION\"] = \"your-api-version\"\n\n    embedding_model = synalinks.EmbeddingModel(\n        model=\"azure/&lt;your_deployment_name&gt;\",\n    )\n    ```\n\n    **Using VertexAI models**\n\n    ```python\n    import synalinks\n    import os\n\n    embedding_model = synalinks.EmbeddingModel(\n        model=\"vertex_ai/text-embedding-004\",\n        vertex_project = \"hardy-device-38811\", # Your Project ID\n        vertex_location = \"us-central1\",  # Project location\n    )\n    ```\n\n    **Using Ollama models**\n\n    ```python\n    import synalinks\n\n    embedding_model = synalinks.EmbeddingModel(\n        model=\"ollama/mxbai-embed-large\",\n    )\n    ```\n\n    **Note**: Obviously, use an `.env` file and `.gitignore` to avoid\n    putting your API keys in the code or a config file that can lead to\n    leackage when pushing it into repositories.\n\n    Args:\n        model (str): The model to use.\n        api_base (str): Optional. The endpoint to use.\n        retry (int): Optional. The number of retry.\n        fallback (EmbeddingModel): Optional. The embedding model to fallback\n            if anything is wrong.\n        caching (bool): Enables caching (Default to True).\n    \"\"\"\n\n    def __init__(\n        self,\n        model=None,\n        api_base=None,\n        retry=5,\n        fallback=None,\n        caching=True,\n    ):\n        if model is None:\n            raise ValueError(\n                \"You need to set the `model` argument for any EmbeddingModel\"\n            )\n        self.model = model\n        if self.model.startswith(\"ollama\") and not api_base:\n            self.api_base = \"http://localhost:11434\"\n        else:\n            self.api_base = api_base\n        self.retry = retry\n        self.fallback = fallback\n        self.caching = caching\n\n    async def __call__(self, texts, **kwargs):\n        \"\"\"\n        Call method to get dense embeddings vectors\n\n        Args:\n            texts (list): A list of texts to embed.\n\n        Returns:\n            (list): The list of corresponding vectors.\n        \"\"\"\n\n        for i in range(self.retry):\n            try:\n                if self.api_base:\n                    response = await litellm.aembedding(\n                        model=self.model,\n                        input=texts,\n                        api_base=self.api_base,\n                        caching=self.caching,\n                        **kwargs,\n                    )\n                else:\n                    response = await litellm.aembedding(\n                        model=self.model,\n                        input=texts,\n                        caching=self.caching,\n                        **kwargs,\n                    )\n                vectors = []\n                for data in response[\"data\"]:\n                    vectors.append(data[\"embedding\"])\n                return {\"embeddings\": vectors}\n            except Exception as e:\n                warnings.warn(f\"Error occured while trying to call {self}: \" + str(e))\n        if self.fallback:\n            return self.fallback(\n                texts,\n                **kwargs,\n            )\n        else:\n            return None\n\n    def _obj_type(self):\n        return \"EmbeddingModel\"\n\n    def get_config(self):\n        config = {\n            \"model\": self.model,\n            \"api_base\": self.api_base,\n            \"retry\": self.retry,\n        }\n        if self.fallback:\n            fallback_config = {\n                \"fallback\": serialization_lib.serialize_synalinks_object(\n                    self.fallback,\n                )\n            }\n            return {**fallback_config, **config}\n        else:\n            return config\n\n    @classmethod\n    def from_config(cls, config):\n        if \"fallback\" in config:\n            fallback = serialization_lib.deserialize_synalinks_object(\n                config.pop(\"fallback\")\n            )\n            return cls(fallback=fallback, **config)\n        else:\n            return cls(**config)\n\n    def __repr__(self):\n        api_base = f\" api_base={self.api_base}\" if self.api_base else \"\"\n        return f\"&lt;EmbeddingModel model={self.model}{api_base}&gt;\"\n</code></pre>"},{"location":"Synalinks%20API/Embedding%20Models%20API/#synalinks.src.embedding_models.embedding_model.EmbeddingModel.__call__","title":"<code>__call__(texts, **kwargs)</code>  <code>async</code>","text":"<p>Call method to get dense embeddings vectors</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list</code> <p>A list of texts to embed.</p> required <p>Returns:</p> Type Description <code>list</code> <p>The list of corresponding vectors.</p> Source code in <code>synalinks/src/embedding_models/embedding_model.py</code> <pre><code>async def __call__(self, texts, **kwargs):\n    \"\"\"\n    Call method to get dense embeddings vectors\n\n    Args:\n        texts (list): A list of texts to embed.\n\n    Returns:\n        (list): The list of corresponding vectors.\n    \"\"\"\n\n    for i in range(self.retry):\n        try:\n            if self.api_base:\n                response = await litellm.aembedding(\n                    model=self.model,\n                    input=texts,\n                    api_base=self.api_base,\n                    caching=self.caching,\n                    **kwargs,\n                )\n            else:\n                response = await litellm.aembedding(\n                    model=self.model,\n                    input=texts,\n                    caching=self.caching,\n                    **kwargs,\n                )\n            vectors = []\n            for data in response[\"data\"]:\n                vectors.append(data[\"embedding\"])\n            return {\"embeddings\": vectors}\n        except Exception as e:\n            warnings.warn(f\"Error occured while trying to call {self}: \" + str(e))\n    if self.fallback:\n        return self.fallback(\n            texts,\n            **kwargs,\n        )\n    else:\n        return None\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/","title":"Knowledge Bases API","text":""},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#knowledge-bases-api","title":"Knowledge Bases API","text":""},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase","title":"<code>KnowledgeBase</code>","text":"<p>               Bases: <code>SynalinksSaveable</code></p> <p>A knowledge base for storing and retrieving structured data.</p> <p>The KnowledgeBase provides a unified interface for storing structured data with support for full-text search and optional vector similarity search. It uses DuckDB as the underlying storage engine.</p>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase--basic-usage","title":"Basic Usage","text":"<pre><code>import synalinks\n\nclass Document(synalinks.DataModel):\n    id: str\n    title: str\n    content: str\n\n# Create a knowledge base without embeddings (full-text search only)\nknowledge_base = synalinks.KnowledgeBase(\n    uri=\"duckdb://my_database.db\",\n    data_models=[Document],\n)\n\n# Store a document\ndoc = Document(id=\"1\", title=\"Hello\", content=\"Hello World!\")\nawait knowledge_base.update(doc.to_json_data_model())\n\n# Retrieve by ID\nresult = await knowledge_base.get(\"1\", [Document.to_symbolic_data_model()])\n\n# Full-text search\nresults = await knowledge_base.fulltext_search(\"Hello\", k=10)\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase--with-vector-similarity-search","title":"With Vector Similarity Search","text":"<pre><code>embedding_model = synalinks.EmbeddingModel(\n    model=\"ollama/mxbai-embed-large\"\n)\n\nknowledge_base = synalinks.KnowledgeBase(\n    uri=\"duckdb://./my_database.db\",\n    data_models=[Document],\n    embedding_model=embedding_model,\n    metric=\"cosine\",\n)\n\n# Hybrid search (combines full-text and vector similarity)\nresults = await knowledge_base.hybrid_search(\"semantic query\", k=10)\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase--retrieving-table-definitions","title":"Retrieving Table Definitions","text":"<pre><code># Get all symbolic data models (table definitions) from the database\nsymbolic_models = knowledge_base.get_symbolic_data_models()\n\nfor model in symbolic_models:\n    print(model.get_schema())\n    # {'title': 'Document', 'type': 'object', 'properties': {...}, ...}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The database connection URI. Use \"duckdb://path/to/db.db\" for DuckDB. If not provided, uses an in-memory database.</p> <code>None</code> <code>data_models</code> <code>list</code> <p>Optional list of DataModel or SymbolicDataModel classes to create tables for.</p> <code>None</code> <code>embedding_model</code> <code>EmbeddingModel</code> <p>Optional embedding model for vector similarity search.</p> <code>None</code> <code>metric</code> <code>str</code> <p>The distance metric for vector search. Options: \"cosine\", \"l2seq\", \"ip\" (default: \"cosine\").</p> <code>'cosine'</code> <code>wipe_on_start</code> <code>bool</code> <p>Whether to clear the database on initialization (default: False).</p> <code>False</code> <code>name</code> <code>str</code> <p>Optional name for the knowledge base (used for serialization).</p> <code>None</code> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>@synalinks_export(\"synalinks.KnowledgeBase\")\nclass KnowledgeBase(SynalinksSaveable):\n    \"\"\"A knowledge base for storing and retrieving structured data.\n\n    The KnowledgeBase provides a unified interface for storing structured data\n    with support for full-text search and optional vector similarity search.\n    It uses DuckDB as the underlying storage engine.\n\n    ### Basic Usage\n\n    ```python\n    import synalinks\n\n    class Document(synalinks.DataModel):\n        id: str\n        title: str\n        content: str\n\n    # Create a knowledge base without embeddings (full-text search only)\n    knowledge_base = synalinks.KnowledgeBase(\n        uri=\"duckdb://my_database.db\",\n        data_models=[Document],\n    )\n\n    # Store a document\n    doc = Document(id=\"1\", title=\"Hello\", content=\"Hello World!\")\n    await knowledge_base.update(doc.to_json_data_model())\n\n    # Retrieve by ID\n    result = await knowledge_base.get(\"1\", [Document.to_symbolic_data_model()])\n\n    # Full-text search\n    results = await knowledge_base.fulltext_search(\"Hello\", k=10)\n    ```\n\n    ### With Vector Similarity Search\n\n    ```python\n    embedding_model = synalinks.EmbeddingModel(\n        model=\"ollama/mxbai-embed-large\"\n    )\n\n    knowledge_base = synalinks.KnowledgeBase(\n        uri=\"duckdb://./my_database.db\",\n        data_models=[Document],\n        embedding_model=embedding_model,\n        metric=\"cosine\",\n    )\n\n    # Hybrid search (combines full-text and vector similarity)\n    results = await knowledge_base.hybrid_search(\"semantic query\", k=10)\n    ```\n\n    ### Retrieving Table Definitions\n\n    ```python\n    # Get all symbolic data models (table definitions) from the database\n    symbolic_models = knowledge_base.get_symbolic_data_models()\n\n    for model in symbolic_models:\n        print(model.get_schema())\n        # {'title': 'Document', 'type': 'object', 'properties': {...}, ...}\n    ```\n\n    Args:\n        uri (str): The database connection URI. Use \"duckdb://path/to/db.db\"\n            for DuckDB. If not provided, uses an in-memory database.\n        data_models (list): Optional list of DataModel or SymbolicDataModel\n            classes to create tables for.\n        embedding_model (EmbeddingModel): Optional embedding model for\n            vector similarity search.\n        metric (str): The distance metric for vector search.\n            Options: \"cosine\", \"l2seq\", \"ip\" (default: \"cosine\").\n        wipe_on_start (bool): Whether to clear the database on initialization\n            (default: False).\n        name (str): Optional name for the knowledge base (used for serialization).\n    \"\"\"\n\n    def __init__(\n        self,\n        uri=None,\n        data_models=None,\n        embedding_model=None,\n        metric=\"cosine\",\n        wipe_on_start=False,\n        name=None,\n    ):\n        self.adapter = database_adapters.get(uri)(\n            uri=uri,\n            data_models=data_models,\n            embedding_model=embedding_model,\n            metric=metric,\n            wipe_on_start=wipe_on_start,\n            name=name,\n        )\n        self.uri = uri\n        self.data_models = data_models or []\n        self.embedding_model = embedding_model\n        self.metric = metric\n        self.wipe_on_start = wipe_on_start\n        if not name:\n            self.name = auto_name(\"knowledge_base\")\n        else:\n            self.name = name\n\n    async def update(\n        self,\n        data_model_or_data_models: Union[Any, List[Any]],\n    ) -&gt; Union[Any, List[Any]]:\n        \"\"\"Insert or update records in the knowledge base.\n\n        Args:\n            data_model_or_data_models (JsonDataModel | List[JsonDataModel]):\n                A single JsonDataModel or a list of JsonDataModels to insert\n                or update. Uses the first field as the primary key for upserts.\n\n        Returns:\n            The primary key value(s) of the inserted/updated records.\n        \"\"\"\n        return await self.adapter.update(data_model_or_data_models)\n\n    async def get(\n        self,\n        id_or_ids: Any,\n        data_models: Optional[List[Any]] = None,\n    ) -&gt; Optional[Any]:\n        \"\"\"Retrieve a record by its primary key.\n\n        Args:\n            id_or_ids: The primary key value to look up.\n            data_models: Optional list of SymbolicDataModels to search in.\n                If not provided, searches all tables.\n\n        Returns:\n            JsonDataModel if found, None otherwise.\n        \"\"\"\n        return await self.adapter.get(id_or_ids, data_models=data_models)\n\n    async def getall(\n        self,\n        data_model: Any,\n        limit: int = 50,\n        offset: int = 0,\n    ) -&gt; List[Any]:\n        \"\"\"Retrieve all records from a table with pagination.\n\n        Args:\n            data_model: The SymbolicDataModel representing the table to query.\n            limit: Maximum number of records to return (default: 50).\n            offset: Number of records to skip (default: 0).\n\n        Returns:\n            List of JsonDataModels.\n        \"\"\"\n        return await self.adapter.getall(data_model, limit=limit, offset=offset)\n\n    async def query(\n        self,\n        query: str,\n        params: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Execute a raw SQL query against the knowledge base.\n\n        Args:\n            query (str): The SQL query to execute.\n            params (dict): Optional list of parameters for parameterized queries.\n            **kwargs (Any): Additional options (e.g., read_only=True/False).\n\n        Returns:\n            List of result dictionaries.\n        \"\"\"\n        return await self.adapter.query(query, params=params, **kwargs)\n\n    async def similarity_search(\n        self,\n        text_or_texts: Union[str, List[str]],\n        data_models: Optional[List[Any]] = None,\n        k: int = 10,\n        threshold: Optional[float] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Perform vector similarity search using embeddings.\n\n        Requires an embedding_model to be configured.\n\n        Args:\n            text_or_texts: Query text or list of query texts.\n            data_models: Optional list of SymbolicDataModels to search in.\n            k: Maximum number of results to return (default: 10).\n            threshold: Optional maximum distance threshold for filtering.\n\n        Returns:\n            List of matching records with similarity scores.\n        \"\"\"\n        return await self.adapter.similarity_search(\n            text_or_texts,\n            data_models=data_models,\n            k=k,\n            threshold=threshold,\n        )\n\n    async def fulltext_search(\n        self,\n        text_or_texts: Union[str, List[str]],\n        data_models: Optional[List[Any]] = None,\n        k: int = 10,\n        threshold: Optional[float] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Perform full-text search using BM25 ranking.\n\n        Searches text fields (description, text, content, message, name,\n        query, question) for matching documents.\n\n        Args:\n            text_or_texts: Query text or list of query texts.\n            data_models: Optional list of SymbolicDataModels to search in.\n            k: Maximum number of results to return (default: 10).\n            threshold: Optional minimum BM25 score threshold.\n\n        Returns:\n            List of matching records with relevance scores.\n        \"\"\"\n        return await self.adapter.fulltext_search(\n            text_or_texts,\n            data_models=data_models,\n            k=k,\n            threshold=threshold,\n        )\n\n    async def hybrid_search(\n        self,\n        text_or_texts: Union[str, List[str]],\n        data_models: Optional[List[Any]] = None,\n        k: int = 10,\n        k_rank: int = 60,\n        similarity_threshold: Optional[float] = None,\n        fulltext_threshold: Optional[float] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Perform hybrid search combining vector similarity and full-text.\n\n        Uses Reciprocal Rank Fusion (RRF) to combine results from both\n        similarity search and full-text search. Falls back to full-text\n        search only if no embedding model is configured.\n\n        Args:\n            text_or_texts: Query text or list of query texts.\n            data_models: Optional list of SymbolicDataModels to search in.\n            k: Maximum number of results to return (default: 10).\n            k_rank: RRF smoothing constant. Lower values emphasize top ranks\n                more strongly (default: 60).\n            similarity_threshold: Optional threshold for vector similarity.\n            fulltext_threshold: Optional threshold for full-text relevance.\n\n        Returns:\n            List of matching records with combined scores.\n        \"\"\"\n        return await self.adapter.hybrid_search(\n            text_or_texts,\n            data_models=data_models,\n            k=k,\n            k_rank=k_rank,\n            similarity_threshold=similarity_threshold,\n            fulltext_threshold=fulltext_threshold,\n        )\n\n    def get_symbolic_data_models(self) -&gt; List[Any]:\n        \"\"\"Retrieve all symbolic data models (table definitions) from the database.\n\n        Returns a list of SymbolicDataModel objects representing each table\n        in the database. This is useful for introspecting the database schema\n        or for passing to search methods to limit the search scope.\n\n        Returns:\n            list: List of symbolic data models representing the database tables.\n\n        Example:\n            ```python\n            symbolic_models = knowledge_base.get_symbolic_data_models()\n            for model in symbolic_models:\n                schema = model.get_schema()\n                print(f\"Table: {schema['title']}\")\n                print(f\"Fields: {list(schema['properties'].keys())}\")\n            ```\n        \"\"\"\n        return self.adapter.get_symbolic_data_models()\n\n    def get_config(self):\n        config = {\n            \"uri\": self.uri,\n            \"name\": self.name,\n            \"metric\": self.metric,\n            \"wipe_on_start\": self.wipe_on_start,\n        }\n        data_models_config = {\n            \"data_models\": [\n                (\n                    serialization_lib.serialize_synalinks_object(\n                        data_model.to_symbolic_data_model(\n                            name=\"data_model\" + (f\"_{i}_\" if i &gt; 0 else \"_\") + self.name\n                        )\n                    )\n                    if not is_symbolic_data_model(data_model)\n                    else serialization_lib.serialize_synalinks_object(data_model)\n                )\n                for i, data_model in enumerate(self.data_models)\n            ]\n        }\n        embedding_model_config = {}\n        if self.embedding_model:\n            embedding_model_config = {\n                \"embedding_model\": serialization_lib.serialize_synalinks_object(\n                    self.embedding_model,\n                )\n            }\n        return {\n            **data_models_config,\n            **embedding_model_config,\n            **config,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        data_models_config = config.pop(\"data_models\", [])\n        data_models = [\n            serialization_lib.deserialize_synalinks_object(data_model)\n            for data_model in data_models_config\n        ]\n        embedding_model = None\n        if \"embedding_model\" in config:\n            embedding_model = serialization_lib.deserialize_synalinks_object(\n                config.pop(\"embedding_model\"),\n            )\n        return cls(\n            data_models=data_models,\n            embedding_model=embedding_model,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.fulltext_search","title":"<code>fulltext_search(text_or_texts, data_models=None, k=10, threshold=None)</code>  <code>async</code>","text":"<p>Perform full-text search using BM25 ranking.</p> <p>Searches text fields (description, text, content, message, name, query, question) for matching documents.</p> <p>Parameters:</p> Name Type Description Default <code>text_or_texts</code> <code>Union[str, List[str]]</code> <p>Query text or list of query texts.</p> required <code>data_models</code> <code>Optional[List[Any]]</code> <p>Optional list of SymbolicDataModels to search in.</p> <code>None</code> <code>k</code> <code>int</code> <p>Maximum number of results to return (default: 10).</p> <code>10</code> <code>threshold</code> <code>Optional[float]</code> <p>Optional minimum BM25 score threshold.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of matching records with relevance scores.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def fulltext_search(\n    self,\n    text_or_texts: Union[str, List[str]],\n    data_models: Optional[List[Any]] = None,\n    k: int = 10,\n    threshold: Optional[float] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Perform full-text search using BM25 ranking.\n\n    Searches text fields (description, text, content, message, name,\n    query, question) for matching documents.\n\n    Args:\n        text_or_texts: Query text or list of query texts.\n        data_models: Optional list of SymbolicDataModels to search in.\n        k: Maximum number of results to return (default: 10).\n        threshold: Optional minimum BM25 score threshold.\n\n    Returns:\n        List of matching records with relevance scores.\n    \"\"\"\n    return await self.adapter.fulltext_search(\n        text_or_texts,\n        data_models=data_models,\n        k=k,\n        threshold=threshold,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.get","title":"<code>get(id_or_ids, data_models=None)</code>  <code>async</code>","text":"<p>Retrieve a record by its primary key.</p> <p>Parameters:</p> Name Type Description Default <code>id_or_ids</code> <code>Any</code> <p>The primary key value to look up.</p> required <code>data_models</code> <code>Optional[List[Any]]</code> <p>Optional list of SymbolicDataModels to search in. If not provided, searches all tables.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>JsonDataModel if found, None otherwise.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def get(\n    self,\n    id_or_ids: Any,\n    data_models: Optional[List[Any]] = None,\n) -&gt; Optional[Any]:\n    \"\"\"Retrieve a record by its primary key.\n\n    Args:\n        id_or_ids: The primary key value to look up.\n        data_models: Optional list of SymbolicDataModels to search in.\n            If not provided, searches all tables.\n\n    Returns:\n        JsonDataModel if found, None otherwise.\n    \"\"\"\n    return await self.adapter.get(id_or_ids, data_models=data_models)\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.get_symbolic_data_models","title":"<code>get_symbolic_data_models()</code>","text":"<p>Retrieve all symbolic data models (table definitions) from the database.</p> <p>Returns a list of SymbolicDataModel objects representing each table in the database. This is useful for introspecting the database schema or for passing to search methods to limit the search scope.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[Any]</code> <p>List of symbolic data models representing the database tables.</p> Example <pre><code>symbolic_models = knowledge_base.get_symbolic_data_models()\nfor model in symbolic_models:\n    schema = model.get_schema()\n    print(f\"Table: {schema['title']}\")\n    print(f\"Fields: {list(schema['properties'].keys())}\")\n</code></pre> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>def get_symbolic_data_models(self) -&gt; List[Any]:\n    \"\"\"Retrieve all symbolic data models (table definitions) from the database.\n\n    Returns a list of SymbolicDataModel objects representing each table\n    in the database. This is useful for introspecting the database schema\n    or for passing to search methods to limit the search scope.\n\n    Returns:\n        list: List of symbolic data models representing the database tables.\n\n    Example:\n        ```python\n        symbolic_models = knowledge_base.get_symbolic_data_models()\n        for model in symbolic_models:\n            schema = model.get_schema()\n            print(f\"Table: {schema['title']}\")\n            print(f\"Fields: {list(schema['properties'].keys())}\")\n        ```\n    \"\"\"\n    return self.adapter.get_symbolic_data_models()\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.getall","title":"<code>getall(data_model, limit=50, offset=0)</code>  <code>async</code>","text":"<p>Retrieve all records from a table with pagination.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>Any</code> <p>The SymbolicDataModel representing the table to query.</p> required <code>limit</code> <code>int</code> <p>Maximum number of records to return (default: 50).</p> <code>50</code> <code>offset</code> <code>int</code> <p>Number of records to skip (default: 0).</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of JsonDataModels.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def getall(\n    self,\n    data_model: Any,\n    limit: int = 50,\n    offset: int = 0,\n) -&gt; List[Any]:\n    \"\"\"Retrieve all records from a table with pagination.\n\n    Args:\n        data_model: The SymbolicDataModel representing the table to query.\n        limit: Maximum number of records to return (default: 50).\n        offset: Number of records to skip (default: 0).\n\n    Returns:\n        List of JsonDataModels.\n    \"\"\"\n    return await self.adapter.getall(data_model, limit=limit, offset=offset)\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.hybrid_search","title":"<code>hybrid_search(text_or_texts, data_models=None, k=10, k_rank=60, similarity_threshold=None, fulltext_threshold=None)</code>  <code>async</code>","text":"<p>Perform hybrid search combining vector similarity and full-text.</p> <p>Uses Reciprocal Rank Fusion (RRF) to combine results from both similarity search and full-text search. Falls back to full-text search only if no embedding model is configured.</p> <p>Parameters:</p> Name Type Description Default <code>text_or_texts</code> <code>Union[str, List[str]]</code> <p>Query text or list of query texts.</p> required <code>data_models</code> <code>Optional[List[Any]]</code> <p>Optional list of SymbolicDataModels to search in.</p> <code>None</code> <code>k</code> <code>int</code> <p>Maximum number of results to return (default: 10).</p> <code>10</code> <code>k_rank</code> <code>int</code> <p>RRF smoothing constant. Lower values emphasize top ranks more strongly (default: 60).</p> <code>60</code> <code>similarity_threshold</code> <code>Optional[float]</code> <p>Optional threshold for vector similarity.</p> <code>None</code> <code>fulltext_threshold</code> <code>Optional[float]</code> <p>Optional threshold for full-text relevance.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of matching records with combined scores.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def hybrid_search(\n    self,\n    text_or_texts: Union[str, List[str]],\n    data_models: Optional[List[Any]] = None,\n    k: int = 10,\n    k_rank: int = 60,\n    similarity_threshold: Optional[float] = None,\n    fulltext_threshold: Optional[float] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Perform hybrid search combining vector similarity and full-text.\n\n    Uses Reciprocal Rank Fusion (RRF) to combine results from both\n    similarity search and full-text search. Falls back to full-text\n    search only if no embedding model is configured.\n\n    Args:\n        text_or_texts: Query text or list of query texts.\n        data_models: Optional list of SymbolicDataModels to search in.\n        k: Maximum number of results to return (default: 10).\n        k_rank: RRF smoothing constant. Lower values emphasize top ranks\n            more strongly (default: 60).\n        similarity_threshold: Optional threshold for vector similarity.\n        fulltext_threshold: Optional threshold for full-text relevance.\n\n    Returns:\n        List of matching records with combined scores.\n    \"\"\"\n    return await self.adapter.hybrid_search(\n        text_or_texts,\n        data_models=data_models,\n        k=k,\n        k_rank=k_rank,\n        similarity_threshold=similarity_threshold,\n        fulltext_threshold=fulltext_threshold,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.query","title":"<code>query(query, params=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a raw SQL query against the knowledge base.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>params</code> <code>dict</code> <p>Optional list of parameters for parameterized queries.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional options (e.g., read_only=True/False).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of result dictionaries.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def query(\n    self,\n    query: str,\n    params: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Execute a raw SQL query against the knowledge base.\n\n    Args:\n        query (str): The SQL query to execute.\n        params (dict): Optional list of parameters for parameterized queries.\n        **kwargs (Any): Additional options (e.g., read_only=True/False).\n\n    Returns:\n        List of result dictionaries.\n    \"\"\"\n    return await self.adapter.query(query, params=params, **kwargs)\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.similarity_search","title":"<code>similarity_search(text_or_texts, data_models=None, k=10, threshold=None)</code>  <code>async</code>","text":"<p>Perform vector similarity search using embeddings.</p> <p>Requires an embedding_model to be configured.</p> <p>Parameters:</p> Name Type Description Default <code>text_or_texts</code> <code>Union[str, List[str]]</code> <p>Query text or list of query texts.</p> required <code>data_models</code> <code>Optional[List[Any]]</code> <p>Optional list of SymbolicDataModels to search in.</p> <code>None</code> <code>k</code> <code>int</code> <p>Maximum number of results to return (default: 10).</p> <code>10</code> <code>threshold</code> <code>Optional[float]</code> <p>Optional maximum distance threshold for filtering.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of matching records with similarity scores.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def similarity_search(\n    self,\n    text_or_texts: Union[str, List[str]],\n    data_models: Optional[List[Any]] = None,\n    k: int = 10,\n    threshold: Optional[float] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Perform vector similarity search using embeddings.\n\n    Requires an embedding_model to be configured.\n\n    Args:\n        text_or_texts: Query text or list of query texts.\n        data_models: Optional list of SymbolicDataModels to search in.\n        k: Maximum number of results to return (default: 10).\n        threshold: Optional maximum distance threshold for filtering.\n\n    Returns:\n        List of matching records with similarity scores.\n    \"\"\"\n    return await self.adapter.similarity_search(\n        text_or_texts,\n        data_models=data_models,\n        k=k,\n        threshold=threshold,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Knowledge%20Bases%20API/#synalinks.src.knowledge_bases.knowledge_base.KnowledgeBase.update","title":"<code>update(data_model_or_data_models)</code>  <code>async</code>","text":"<p>Insert or update records in the knowledge base.</p> <p>Parameters:</p> Name Type Description Default <code>data_model_or_data_models</code> <code>JsonDataModel | List[JsonDataModel]</code> <p>A single JsonDataModel or a list of JsonDataModels to insert or update. Uses the first field as the primary key for upserts.</p> required <p>Returns:</p> Type Description <code>Union[Any, List[Any]]</code> <p>The primary key value(s) of the inserted/updated records.</p> Source code in <code>synalinks/src/knowledge_bases/knowledge_base.py</code> <pre><code>async def update(\n    self,\n    data_model_or_data_models: Union[Any, List[Any]],\n) -&gt; Union[Any, List[Any]]:\n    \"\"\"Insert or update records in the knowledge base.\n\n    Args:\n        data_model_or_data_models (JsonDataModel | List[JsonDataModel]):\n            A single JsonDataModel or a list of JsonDataModels to insert\n            or update. Uses the first field as the primary key for upserts.\n\n    Returns:\n        The primary key value(s) of the inserted/updated records.\n    \"\"\"\n    return await self.adapter.update(data_model_or_data_models)\n</code></pre>"},{"location":"Synalinks%20API/Language%20Models%20API/","title":"Language Models API","text":""},{"location":"Synalinks%20API/Language%20Models%20API/#language-models-api","title":"Language Models API","text":""},{"location":"Synalinks%20API/Language%20Models%20API/#synalinks.src.language_models.language_model.LanguageModel","title":"<code>LanguageModel</code>","text":"<p>               Bases: <code>SynalinksSaveable</code></p> <p>A language model API wrapper.</p> <p>A language model is a type of AI model designed to generate, and interpret human language. It is trained on large amounts of text data to learn patterns and structures in language. Language models can perform various tasks such as text generation, translation, summarization, and answering questions.</p> <p>We support providers that implement constrained structured output like OpenAI, Azure, Ollama or Mistral. In addition we support providers that otherwise allow to constrain the use of a specific tool like Groq or Anthropic.</p> <p>For the complete list of models, please refer to the providers documentation.</p> <p>Using OpenAI models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"openai/gpt-4o-mini\",\n)\n</code></pre> <p>Using Groq models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"groq/llama3-8b-8192\",\n)\n</code></pre> <p>Using Anthropic models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"anthropic/claude-3-sonnet-20240229\",\n)\n</code></pre> <p>Using Mistral models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"mistral/codestral-latest\",\n)\n</code></pre> <p>Using Ollama models</p> <pre><code>import synalinks\nimport os\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"ollama/mistral\",\n)\n</code></pre> <p>Using Azure OpenAI models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"AZURE_API_KEY\"] = \"your-api-key\"\nos.environ[\"AZURE_API_BASE\"] = \"your-api-key\"\nos.environ[\"AZURE_API_VERSION\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"azure/&lt;your_deployment_name&gt;\",\n)\n</code></pre> <p>Using Google Gemini models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"gemini/gemini-2.5-pro\",\n)\n</code></pre> <p>Using XAI models</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"XAI_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"xai/grok-code-fast-1\",\n)\n</code></pre> <p>To cascade models in case there is anything wrong with the model provider (hence making your pipelines more robust). Use the <code>fallback</code> argument like in this example:</p> <pre><code>import synalinks\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"anthropic/claude-3-sonnet-20240229\",\n    fallback=synalinks.LanguageModel(\n        model=\"openai/gpt-4o-mini\",\n    )\n)\n</code></pre> <p>Note: Obviously, use an <code>.env</code> file and <code>.gitignore</code> to avoid putting your API keys in the code or a config file that can lead to leackage when pushing it into repositories.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use.</p> <code>None</code> <code>api_base</code> <code>str</code> <p>Optional. The endpoint to use.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Optional. The timeout value in seconds (Default to 600).</p> <code>600</code> <code>retry</code> <code>int</code> <p>Optional. The number of retry (default to 2).</p> <code>2</code> <code>fallback</code> <code>LanguageModel</code> <p>Optional. The language model to fallback if anything is wrong.</p> <code>None</code> <code>caching</code> <code>bool</code> <p>Optional. Enable caching of LM calls (Default to False).</p> <code>False</code> Source code in <code>synalinks/src/language_models/language_model.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.LanguageModel\",\n        \"synalinks.language_models.LanguageModel\",\n    ]\n)\nclass LanguageModel(SynalinksSaveable):\n    \"\"\"A language model API wrapper.\n\n    A language model is a type of AI model designed to generate, and interpret human\n    language. It is trained on large amounts of text data to learn patterns and\n    structures in language. Language models can perform various tasks such as text\n    generation, translation, summarization, and answering questions.\n\n    We support providers that implement *constrained structured output*\n    like OpenAI, Azure, Ollama or Mistral. In addition we support providers that otherwise\n    allow to constrain the use of a specific tool like Groq or Anthropic.\n\n    For the complete list of models, please refer to the providers documentation.\n\n    **Using OpenAI models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"openai/gpt-4o-mini\",\n    )\n    ```\n\n    **Using Groq models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"GROQ_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"groq/llama3-8b-8192\",\n    )\n    ```\n\n    **Using Anthropic models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"anthropic/claude-3-sonnet-20240229\",\n    )\n    ```\n\n    **Using Mistral models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"MISTRAL_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"mistral/codestral-latest\",\n    )\n    ```\n\n    **Using Ollama models**\n\n    ```python\n    import synalinks\n    import os\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n    ```\n\n    **Using Azure OpenAI models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"AZURE_API_KEY\"] = \"your-api-key\"\n    os.environ[\"AZURE_API_BASE\"] = \"your-api-key\"\n    os.environ[\"AZURE_API_VERSION\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"azure/&lt;your_deployment_name&gt;\",\n    )\n    ```\n\n    **Using Google Gemini models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"gemini/gemini-2.5-pro\",\n    )\n    ```\n\n    **Using XAI models**\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"XAI_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"xai/grok-code-fast-1\",\n    )\n    ```\n\n    To cascade models in case there is anything wrong with\n    the model provider (hence making your pipelines more robust).\n    Use the `fallback` argument like in this example:\n\n    ```python\n    import synalinks\n    import os\n\n    os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n    os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\n    language_model = synalinks.LanguageModel(\n        model=\"anthropic/claude-3-sonnet-20240229\",\n        fallback=synalinks.LanguageModel(\n            model=\"openai/gpt-4o-mini\",\n        )\n    )\n    ```\n\n    **Note**: Obviously, use an `.env` file and `.gitignore` to avoid\n    putting your API keys in the code or a config file that can lead to\n    leackage when pushing it into repositories.\n\n    Args:\n        model (str): The model to use.\n        api_base (str): Optional. The endpoint to use.\n        timeout (int): Optional. The timeout value in seconds (Default to 600).\n        retry (int): Optional. The number of retry (default to 2).\n        fallback (LanguageModel): Optional. The language model to fallback\n            if anything is wrong.\n        caching (bool): Optional. Enable caching of LM calls (Default to False).\n    \"\"\"\n\n    def __init__(\n        self,\n        model=None,\n        api_base=None,\n        timeout=600,\n        retry=2,\n        fallback=None,\n        caching=False,\n    ):\n        if model is None:\n            raise ValueError(\"You need to set the `model` argument for any LanguageModel\")\n        model_provider = model.split(\"/\")[0]\n        if model_provider == \"ollama\":\n            # Switch from `ollama` to `ollama_chat`\n            # because it have better performance due to the chat prompts\n            model = model.replace(\"ollama\", \"ollama_chat\")\n        if model_provider == \"vllm\":\n            model = model.replace(\"vllm\", \"hosted_vllm\")\n        self.model = model\n        self.fallback = fallback\n        if self.model.startswith(\"ollama\") and not api_base:\n            self.api_base = \"http://localhost:11434\"\n        else:\n            self.api_base = api_base\n        if self.model.startswith(\"hosted_vllm\") and not api_base:\n            self.api_base = os.environ.get(\n                \"HOSTED_VLLM_API_BASE\", \"http://localhost:8000\"\n            )\n        self.timeout = timeout\n        self.retry = retry\n        self.caching = caching\n        self.cumulated_cost = 0.0\n        self.last_call_cost = 0.0\n\n    async def __call__(self, messages, schema=None, streaming=False, **kwargs):\n        \"\"\"\n        Call method to generate a response using the language model.\n\n        Args:\n            messages (dict): A formatted dict of chat messages.\n            schema (dict): The target JSON schema for structed output (optional).\n                If None, output a ChatMessage-like answer.\n            streaming (bool): Enable streaming (optional). Default to False.\n                Can be enabled only if schema is None.\n            **kwargs (keyword arguments): The additional keywords arguments\n                forwarded to the LM call.\n        Returns:\n            (dict): The generated structured response.\n        \"\"\"\n        formatted_messages = messages.get_json().get(\"messages\", [])\n        json_instance = {}\n        input_kwargs = copy.deepcopy(kwargs)\n        schema = copy.deepcopy(schema)\n\n        # Handle reasoning_effort parameter - just forward to litellm if supported\n        reasoning_effort = kwargs.pop(\"reasoning_effort\", \"none\")\n        if reasoning_effort not in (\"none\", \"disable\"):\n            if litellm.supports_reasoning(model=self.model):\n                kwargs[\"reasoning_effort\"] = reasoning_effort\n\n        if schema:\n            if self.model.startswith(\"groq\"):\n                # Use a tool created on the fly for groq\n                kwargs.update(\n                    {\n                        \"tools\": [\n                            {\n                                \"function\": {\n                                    \"name\": \"structured_output\",\n                                    \"description\": \"Generate a valid JSON output\",\n                                    \"parameters\": schema.get(\"properties\"),\n                                },\n                                \"type\": \"function\",\n                            }\n                        ],\n                        \"tool_choice\": {\n                            \"type\": \"function\",\n                            \"function\": {\"name\": \"structured_output\"},\n                        },\n                    }\n                )\n            elif self.model.startswith(\"anthropic\"):\n                # Use response_format for Anthropic - LiteLLM handles this correctly:\n                # - For newer models (sonnet-4.5, opus-4.1): uses native output_format\n                # - For older models: uses tool call with proper tool_choice handling\n                #   (auto when thinking is enabled, forced otherwise)\n                kwargs.update(\n                    {\n                        \"response_format\": {\n                            \"type\": \"json_schema\",\n                            \"json_schema\": {\n                                \"schema\": schema,\n                            },\n                        },\n                    }\n                )\n            elif self.model.startswith(\"ollama\") or self.model.startswith(\"mistral\"):\n                # Use constrained structured output for ollama/mistral\n                kwargs.update(\n                    {\n                        \"response_format\": {\n                            \"type\": \"json_schema\",\n                            \"json_schema\": {\"schema\": schema},\n                            \"strict\": True,\n                        },\n                    }\n                )\n            elif self.model.startswith(\"openai\") or self.model.startswith(\"azure\"):\n                # Use constrained structured output for openai\n                # OpenAI require the field  \"additionalProperties\"\n                # Also OpenAI disallow the field \"description\" in $ref\n                if \"properties\" in schema:\n                    for prop_key, prop_value in schema[\"properties\"].items():\n                        if \"$ref\" in prop_value and \"description\" in prop_value:\n                            del prop_value[\"description\"]\n                kwargs.update(\n                    {\n                        \"response_format\": {\n                            \"type\": \"json_schema\",\n                            \"json_schema\": {\n                                \"name\": \"structured_output\",\n                                \"strict\": True,\n                                \"schema\": schema,\n                            },\n                        }\n                    }\n                )\n            elif self.model.startswith(\"gemini\"):\n                kwargs.update(\n                    {\n                        \"response_format\": {\n                            \"type\": \"json_schema\",\n                            \"json_schema\": {\n                                \"schema\": schema,\n                            },\n                            \"strict\": True,\n                        }\n                    }\n                )\n            elif self.model.startswith(\"xai\"):\n                kwargs.update(\n                    {\n                        \"response_format\": {\n                            \"type\": \"json_schema\",\n                            \"json_schema\": {\n                                \"schema\": schema,\n                            },\n                            \"strict\": True,\n                        }\n                    }\n                )\n            elif self.model.startswith(\"hosted_vllm\"):\n                kwargs.update(\n                    {\n                        \"response_format\": {\n                            \"type\": \"json_schema\",\n                            \"json_schema\": {\n                                \"schema\": schema,\n                            },\n                            \"strict\": True,\n                        }\n                    }\n                )\n            else:\n                provider = self.model.split(\"/\")[0]\n                raise ValueError(\n                    f\"LM provider '{provider}' not supported yet, please ensure that\"\n                    \" they support constrained structured output and fill an issue.\"\n                )\n\n        if self.api_base:\n            kwargs.update(\n                {\n                    \"api_base\": self.api_base,\n                }\n            )\n        if streaming and schema:\n            streaming = False\n        if streaming:\n            kwargs.update({\"stream\": True})\n        # Enable prompt caching for the system instructions (that only change during training not inference)\n        system_message_with_cache_control = {**formatted_messages[0], \"cache_control\": {\"type\": \"ephemeral\"}}\n        formatted_messages[0] = system_message_with_cache_control\n        for i in range(self.retry):\n            try:\n                response_str = \"\"\n                response = await litellm.acompletion(\n                    model=self.model,\n                    messages=formatted_messages,\n                    timeout=self.timeout,\n                    caching=self.caching,\n                    **kwargs,\n                )\n                if hasattr(response, \"_hidden_params\"):\n                    if \"response_cost\" in response._hidden_params:\n                        self.last_call_cost = response._hidden_params[\"response_cost\"]\n                        self.cumulated_cost += self.last_call_cost\n                if streaming:\n                    return StreamingIterator(response)\n                if self.model.startswith(\"groq\") and schema:\n                    # Groq uses tool_calls for structured output\n                    response_str = response[\"choices\"][0][\"message\"][\"tool_calls\"][0][\n                        \"function\"\n                    ][\"arguments\"]\n                else:\n                    # Anthropic and other providers use response_format,\n                    # which returns content in message[\"content\"]\n                    response_str = response[\"choices\"][0][\"message\"][\"content\"].strip()\n                if schema:\n                    json_instance = orjson.loads(response_str)\n                else:\n                    json_instance = {\n                        \"role\": ChatRole.ASSISTANT,\n                        \"content\": response_str,\n                        \"tool_call_id\": None,\n                        \"tool_calls\": [],\n                        \"created_at\": None,\n                    }\n                return json_instance\n            except Exception as e:\n                warnings.warn(\n                    f\"Error occured while trying to call {self}: \"\n                    + str(e)\n                    + f\"\\nReceived response={shorten_text(response_str)}\"\n                )\n            await asyncio.sleep(1)\n        if self.fallback:\n            return await self.fallback(\n                messages,\n                schema=schema,\n                streaming=streaming,\n                **input_kwargs,\n            )\n        else:\n            return None\n\n    def _obj_type(self):\n        return \"LanguageModel\"\n\n    def get_config(self):\n        config = {\n            \"model\": self.model,\n            \"api_base\": self.api_base,\n            \"timeout\": self.timeout,\n            \"retry\": self.retry,\n            \"caching\": self.caching,\n        }\n        if self.fallback:\n            fallback_config = {\n                \"fallback\": serialization_lib.serialize_synalinks_object(\n                    self.fallback,\n                )\n            }\n            return {**fallback_config, **config}\n        else:\n            return config\n\n    @classmethod\n    def from_config(cls, config):\n        if \"fallback\" in config:\n            fallback = serialization_lib.deserialize_synalinks_object(\n                config.pop(\"fallback\")\n            )\n            return cls(fallback=fallback, **config)\n        else:\n            return cls(**config)\n\n    def __repr__(self):\n        api_base = f\" api_base={self.api_base}\" if self.api_base else \"\"\n        return f\"&lt;LanguageModel model={self.model}{api_base}&gt;\"\n</code></pre>"},{"location":"Synalinks%20API/Language%20Models%20API/#synalinks.src.language_models.language_model.LanguageModel.__call__","title":"<code>__call__(messages, schema=None, streaming=False, **kwargs)</code>  <code>async</code>","text":"<p>Call method to generate a response using the language model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>dict</code> <p>A formatted dict of chat messages.</p> required <code>schema</code> <code>dict</code> <p>The target JSON schema for structed output (optional). If None, output a ChatMessage-like answer.</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Enable streaming (optional). Default to False. Can be enabled only if schema is None.</p> <code>False</code> <code>**kwargs</code> <code>keyword arguments</code> <p>The additional keywords arguments forwarded to the LM call.</p> <code>{}</code> <p>Returns:     (dict): The generated structured response.</p> Source code in <code>synalinks/src/language_models/language_model.py</code> <pre><code>async def __call__(self, messages, schema=None, streaming=False, **kwargs):\n    \"\"\"\n    Call method to generate a response using the language model.\n\n    Args:\n        messages (dict): A formatted dict of chat messages.\n        schema (dict): The target JSON schema for structed output (optional).\n            If None, output a ChatMessage-like answer.\n        streaming (bool): Enable streaming (optional). Default to False.\n            Can be enabled only if schema is None.\n        **kwargs (keyword arguments): The additional keywords arguments\n            forwarded to the LM call.\n    Returns:\n        (dict): The generated structured response.\n    \"\"\"\n    formatted_messages = messages.get_json().get(\"messages\", [])\n    json_instance = {}\n    input_kwargs = copy.deepcopy(kwargs)\n    schema = copy.deepcopy(schema)\n\n    # Handle reasoning_effort parameter - just forward to litellm if supported\n    reasoning_effort = kwargs.pop(\"reasoning_effort\", \"none\")\n    if reasoning_effort not in (\"none\", \"disable\"):\n        if litellm.supports_reasoning(model=self.model):\n            kwargs[\"reasoning_effort\"] = reasoning_effort\n\n    if schema:\n        if self.model.startswith(\"groq\"):\n            # Use a tool created on the fly for groq\n            kwargs.update(\n                {\n                    \"tools\": [\n                        {\n                            \"function\": {\n                                \"name\": \"structured_output\",\n                                \"description\": \"Generate a valid JSON output\",\n                                \"parameters\": schema.get(\"properties\"),\n                            },\n                            \"type\": \"function\",\n                        }\n                    ],\n                    \"tool_choice\": {\n                        \"type\": \"function\",\n                        \"function\": {\"name\": \"structured_output\"},\n                    },\n                }\n            )\n        elif self.model.startswith(\"anthropic\"):\n            # Use response_format for Anthropic - LiteLLM handles this correctly:\n            # - For newer models (sonnet-4.5, opus-4.1): uses native output_format\n            # - For older models: uses tool call with proper tool_choice handling\n            #   (auto when thinking is enabled, forced otherwise)\n            kwargs.update(\n                {\n                    \"response_format\": {\n                        \"type\": \"json_schema\",\n                        \"json_schema\": {\n                            \"schema\": schema,\n                        },\n                    },\n                }\n            )\n        elif self.model.startswith(\"ollama\") or self.model.startswith(\"mistral\"):\n            # Use constrained structured output for ollama/mistral\n            kwargs.update(\n                {\n                    \"response_format\": {\n                        \"type\": \"json_schema\",\n                        \"json_schema\": {\"schema\": schema},\n                        \"strict\": True,\n                    },\n                }\n            )\n        elif self.model.startswith(\"openai\") or self.model.startswith(\"azure\"):\n            # Use constrained structured output for openai\n            # OpenAI require the field  \"additionalProperties\"\n            # Also OpenAI disallow the field \"description\" in $ref\n            if \"properties\" in schema:\n                for prop_key, prop_value in schema[\"properties\"].items():\n                    if \"$ref\" in prop_value and \"description\" in prop_value:\n                        del prop_value[\"description\"]\n            kwargs.update(\n                {\n                    \"response_format\": {\n                        \"type\": \"json_schema\",\n                        \"json_schema\": {\n                            \"name\": \"structured_output\",\n                            \"strict\": True,\n                            \"schema\": schema,\n                        },\n                    }\n                }\n            )\n        elif self.model.startswith(\"gemini\"):\n            kwargs.update(\n                {\n                    \"response_format\": {\n                        \"type\": \"json_schema\",\n                        \"json_schema\": {\n                            \"schema\": schema,\n                        },\n                        \"strict\": True,\n                    }\n                }\n            )\n        elif self.model.startswith(\"xai\"):\n            kwargs.update(\n                {\n                    \"response_format\": {\n                        \"type\": \"json_schema\",\n                        \"json_schema\": {\n                            \"schema\": schema,\n                        },\n                        \"strict\": True,\n                    }\n                }\n            )\n        elif self.model.startswith(\"hosted_vllm\"):\n            kwargs.update(\n                {\n                    \"response_format\": {\n                        \"type\": \"json_schema\",\n                        \"json_schema\": {\n                            \"schema\": schema,\n                        },\n                        \"strict\": True,\n                    }\n                }\n            )\n        else:\n            provider = self.model.split(\"/\")[0]\n            raise ValueError(\n                f\"LM provider '{provider}' not supported yet, please ensure that\"\n                \" they support constrained structured output and fill an issue.\"\n            )\n\n    if self.api_base:\n        kwargs.update(\n            {\n                \"api_base\": self.api_base,\n            }\n        )\n    if streaming and schema:\n        streaming = False\n    if streaming:\n        kwargs.update({\"stream\": True})\n    # Enable prompt caching for the system instructions (that only change during training not inference)\n    system_message_with_cache_control = {**formatted_messages[0], \"cache_control\": {\"type\": \"ephemeral\"}}\n    formatted_messages[0] = system_message_with_cache_control\n    for i in range(self.retry):\n        try:\n            response_str = \"\"\n            response = await litellm.acompletion(\n                model=self.model,\n                messages=formatted_messages,\n                timeout=self.timeout,\n                caching=self.caching,\n                **kwargs,\n            )\n            if hasattr(response, \"_hidden_params\"):\n                if \"response_cost\" in response._hidden_params:\n                    self.last_call_cost = response._hidden_params[\"response_cost\"]\n                    self.cumulated_cost += self.last_call_cost\n            if streaming:\n                return StreamingIterator(response)\n            if self.model.startswith(\"groq\") and schema:\n                # Groq uses tool_calls for structured output\n                response_str = response[\"choices\"][0][\"message\"][\"tool_calls\"][0][\n                    \"function\"\n                ][\"arguments\"]\n            else:\n                # Anthropic and other providers use response_format,\n                # which returns content in message[\"content\"]\n                response_str = response[\"choices\"][0][\"message\"][\"content\"].strip()\n            if schema:\n                json_instance = orjson.loads(response_str)\n            else:\n                json_instance = {\n                    \"role\": ChatRole.ASSISTANT,\n                    \"content\": response_str,\n                    \"tool_call_id\": None,\n                    \"tool_calls\": [],\n                    \"created_at\": None,\n                }\n            return json_instance\n        except Exception as e:\n            warnings.warn(\n                f\"Error occured while trying to call {self}: \"\n                + str(e)\n                + f\"\\nReceived response={shorten_text(response_str)}\"\n            )\n        await asyncio.sleep(1)\n    if self.fallback:\n        return await self.fallback(\n            messages,\n            schema=schema,\n            streaming=streaming,\n            **input_kwargs,\n        )\n    else:\n        return None\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/","title":"Built-in Datasets","text":""},{"location":"Synalinks%20API/Built-in%20Datasets/#built-in-datasets","title":"Built-in Datasets","text":"<p>The <code>synalinks.datasets</code> module provide a few datasets that can be used for debugging, evaluation or to create code examples.</p> <p>These datasets are leaked in nowadays LMs training data, which is a big concern in todays ML community, so they won't give you much information about the reasoning abilities of the underlying models. But they are still useful as baseline to compare neuro-symbolic methods or when using small language models.</p> <ul> <li> <p>GSM8K dataset: A dataset of 8.5K high quality linguistically diverse grade school math word problems. Useful to evaluate reasoning capabilities.</p> </li> <li> <p>HotpotQA: A dataset of 113k wikipedia-based question/answer pairs that need multiple documents to answer. This dataset is useful to evaluate Agentic RAGs with multi-hop retrieval.</p> </li> <li> <p>ARC-AGI: A challenging dataset containing tasks based on core knowledge principles to evaluate reasoning and program synthesis systems. </p> </li> </ul>"},{"location":"Synalinks%20API/Built-in%20Datasets/ARC-AGI/","title":"ARC AGI","text":""},{"location":"Synalinks%20API/Built-in%20Datasets/ARC-AGI/#synalinks.src.datasets.arcagi.get_input_data_model","title":"<code>get_input_data_model()</code>","text":"<p>Returns ARC-AGI input data model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The ARC-AGI input data model</p> Source code in <code>synalinks/src/datasets/arcagi.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.arcagi.get_input_data_model\")\ndef get_input_data_model():\n    \"\"\"\n    Returns ARC-AGI input data model for pipeline configurations.\n\n    Returns:\n        (DataModel): The ARC-AGI input data model\n    \"\"\"\n    return ARCAGIInput\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/ARC-AGI/#synalinks.src.datasets.arcagi.get_output_data_model","title":"<code>get_output_data_model()</code>","text":"<p>Returns ARC-AGI output data model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The ARC-AGI output data model</p> Source code in <code>synalinks/src/datasets/arcagi.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.arcagi.get_output_data_model\")\ndef get_output_data_model():\n    \"\"\"\n    Returns ARC-AGI output data model for pipeline configurations.\n\n    Returns:\n        (DataModel): The ARC-AGI output data model\n    \"\"\"\n    return ARCAGIOutput\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/ARC-AGI/#synalinks.src.datasets.arcagi.load_data","title":"<code>load_data(task_name, filepath=None, arc_version=1, one_leave_out=True, permutation=False, repeat=1, curriculum_learning=True)</code>","text":"<p>Load task data by name.</p> <p>Example:</p> <pre><code>(x_train, y_train), (x_test, y_test) = synalinks.datasets.arcagi1.load_data(\n    task_name=\"62c24649\",\n    arc_version=1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task</p> required <code>filepath</code> <code>str</code> <p>The task filepath</p> <code>None</code> <code>arc_version</code> <code>int</code> <p>ARC-AGI version between 1 or 2</p> <code>1</code> <code>one_leave_out</code> <code>bool</code> <p>If True create a traning set using the one-leave-out technique.</p> <code>True</code> <code>permutation</code> <code>bool</code> <p>If True augment the training data using permutation of examples.</p> <code>False</code> <code>repeat</code> <code>int</code> <p>The number of times to repeat the training data.</p> <code>1</code> <code>curriculum_learning</code> <code>bool</code> <p>Wether or not to sort the training set by difficulty. In this case, the difficulty refer to the grid-size of the expected output. (Default to True)</p> <code>True</code> Source code in <code>synalinks/src/datasets/arcagi.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.arcagi.load_data\")\ndef load_data(\n    task_name,\n    filepath=None,\n    arc_version=1,\n    one_leave_out=True,\n    permutation=False,\n    repeat=1,\n    curriculum_learning=True,\n):\n    \"\"\"\n    Load task data by name.\n\n    Example:\n\n    ```python\n    (x_train, y_train), (x_test, y_test) = synalinks.datasets.arcagi1.load_data(\n        task_name=\"62c24649\",\n        arc_version=1,\n    )\n    ```\n\n    Args:\n        task_name (str): The name of the task\n        filepath (str): The task filepath\n        arc_version (int): ARC-AGI version between 1 or 2\n        one_leave_out (bool): If True create a traning set using the\n            one-leave-out technique.\n        permutation (bool): If True augment the training data using\n            permutation of examples.\n        repeat (int): The number of times to repeat the training data.\n        curriculum_learning (bool): Wether or not to sort the training set by difficulty.\n            In this case, the difficulty refer to the grid-size of the expected output.\n            (Default to True)\n    \"\"\"\n    if filepath:\n        try:\n            with open(filepath, \"rb\") as f:\n                json_data = orjson.loads(f.read())\n        except Exception:\n            raise ValueError(\n                f\"Could not find task data at '{filepath}', \"\n                \"make sure the path is correct.\",\n            )\n    else:\n        if arc_version == 1:\n            if task_name in get_arcagi1_training_task_names():\n                url = f\"{ARCAGI1_BASE_URL}/training/{task_name}.json\"\n            elif task_name in get_arcagi1_evaluation_task_names():\n                url = f\"{ARCAGI1_BASE_URL}/evaluation/{task_name}.json\"\n            else:\n                raise ValueError(\n                    f\"Task '{task_name}' not recognized, make sure that\"\n                    \" the task name is valid.\"\n                )\n        elif arc_version == 2:\n            if task_name in get_arcagi2_training_task_names():\n                url = f\"{ARCAGI2_BASE_URL}/training/{task_name}.json\"\n            elif task_name in get_arcagi2_evaluation_task_names():\n                url = f\"{ARCAGI2_BASE_URL}/evaluation/{task_name}.json\"\n            else:\n                raise ValueError(\n                    f\"Task '{task_name}' not recognized, make sure that\"\n                    \" the task name is valid.\"\n                )\n        else:\n            raise ValueError(\"Invalid `arc_version` provided, should be 1 or 2\")\n        file_path = file_utils.get_file(origin=url, progbar=False)\n        with open(file_path, \"rb\") as f:\n            json_data = orjson.loads(f.read())\n\n    x_train = []\n    y_train = []\n    x_test = []\n    y_test = []\n\n    trainset = json_data.get(\"train\")\n    testset = json_data.get(\"test\")\n\n    for i in range(len(trainset)):\n        if one_leave_out:\n            other_examples = [j for j in range(len(trainset)) if j != i]\n            if permutation:\n                permutations = list(itertools.permutations(other_examples))\n                for k, perm in enumerate(permutations):\n                    examples = []\n                    for j in perm:\n                        input_grid_example = trainset[j].get(\"input\")\n                        output_grid_example = trainset[j].get(\"output\")\n                        task = ARCAGITask(\n                            input_grid=input_grid_example,\n                            output_grid=output_grid_example,\n                        )\n                        examples.append(task)\n                    input_grid = trainset[i].get(\"input\")\n                    output_grid = trainset[i].get(\"output\")\n                    inputs = ARCAGIInput(\n                        examples=examples,\n                        input_grid=input_grid,\n                    )\n                    outputs = ARCAGIOutput(\n                        output_grid=output_grid,\n                    )\n                    for i in range(repeat):\n                        x_train.append(inputs)\n                        y_train.append(outputs)\n            else:\n                examples = []\n                for j in other_examples:\n                    input_grid_example = trainset[j].get(\"input\")\n                    output_grid_example = trainset[j].get(\"output\")\n                    task = ARCAGITask(\n                        input_grid=input_grid_example,\n                        output_grid=output_grid_example,\n                    )\n                    examples.append(task)\n                input_grid = trainset[i].get(\"input\")\n                output_grid = trainset[i].get(\"output\")\n                inputs = ARCAGIInput(\n                    examples=examples,\n                    input_grid=input_grid,\n                )\n                outputs = ARCAGIOutput(\n                    output_grid=output_grid,\n                )\n                for i in range(repeat):\n                    x_train.append(inputs)\n                    y_train.append(outputs)\n        else:\n            for i in range(len(trainset)):\n                input_grid = trainset[i].get(\"input\")\n                output_grid = trainset[i].get(\"output\")\n                inputs = ARCAGIInput(\n                    examples=[],\n                    input_grid=input_grid,\n                )\n                outputs = ARCAGIOutput(\n                    output_grid=output_grid,\n                )\n                for i in range(repeat):\n                    x_train.append(inputs)\n                    y_train.append(outputs)\n\n    for i in range(len(testset)):\n        if one_leave_out:\n            examples = []\n            for j in range(len(trainset)):\n                input_grid_example = trainset[j].get(\"input\")\n                output_grid_example = trainset[j].get(\"output\")\n\n                task = ARCAGITask(\n                    input_grid=input_grid_example,\n                    output_grid=output_grid_example,\n                )\n                examples.append(task)\n            input_grid = testset[i].get(\"input\")\n            output_grid = testset[i].get(\"output\")\n            inputs = ARCAGIInput(\n                examples=examples,\n                input_grid=input_grid,\n            )\n            outputs = ARCAGIOutput(\n                output_grid=output_grid,\n            )\n            x_test.append(inputs)\n            y_test.append(outputs)\n        else:\n            input_grid = testset[i].get(\"input\")\n            output_grid = testset[i].get(\"output\")\n            inputs = ARCAGIInput(\n                examples=[],\n                input_grid=input_grid,\n            )\n            outputs = ARCAGIOutput(\n                output_grid=output_grid,\n            )\n            x_test.append(inputs)\n            y_test.append(outputs)\n\n    if curriculum_learning:\n\n        def get_output_grid_size(y_example):\n            output_grid = y_example.output_grid\n            if output_grid:\n                return (\n                    len(output_grid) * len(output_grid[0]) if len(output_grid) &gt; 0 else 0\n                )\n            return 0\n\n        training_pairs = list(zip(x_train, y_train))\n        training_pairs.sort(key=lambda pair: get_output_grid_size(pair[1]))\n        x_train, y_train = zip(*training_pairs)\n        x_train = list(x_train)\n        y_train = list(y_train)\n\n    x_train = np.array(x_train, dtype=\"object\")\n    y_train = np.array(y_train, dtype=\"object\")\n    x_test = np.array(x_test, dtype=\"object\")\n    y_test = np.array(y_test, dtype=\"object\")\n    return (x_train, y_train), (x_test, y_test)\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/GSM8K/","title":"GSM8K","text":""},{"location":"Synalinks%20API/Built-in%20Datasets/GSM8K/#synalinks.src.datasets.gsm8k.get_input_data_model","title":"<code>get_input_data_model()</code>","text":"<p>Returns GSM8K input data_model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The GSM8K input data_model</p> Source code in <code>synalinks/src/datasets/gsm8k.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.gsm8k.get_input_data_model\")\ndef get_input_data_model():\n    \"\"\"\n    Returns GSM8K input data_model for pipeline configurations.\n\n    Returns:\n        (DataModel): The GSM8K input data_model\n    \"\"\"\n    return MathQuestion\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/GSM8K/#synalinks.src.datasets.gsm8k.get_output_data_model","title":"<code>get_output_data_model()</code>","text":"<p>Returns GSM8K output data_model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The GSM8K output data_model</p> Source code in <code>synalinks/src/datasets/gsm8k.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.gsm8k.get_output_data_model\")\ndef get_output_data_model():\n    \"\"\"\n    Returns GSM8K output data_model for pipeline configurations.\n\n    Returns:\n        (DataModel): The GSM8K output data_model\n    \"\"\"\n    return NumericalAnswerWithThinking\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/GSM8K/#synalinks.src.datasets.gsm8k.load_data","title":"<code>load_data()</code>","text":"<p>Load and format data from HuggingFace</p> <p>Example:</p> <pre><code>(x_train, y_train), (x_test, y_test) = synalinks.datasets.gsm8k.load_data()\n</code></pre> <p>Returns:</p> Type Description <code>tuple</code> <p>The train and test data ready for training</p> Source code in <code>synalinks/src/datasets/gsm8k.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.gsm8k.load_data\")\ndef load_data():\n    \"\"\"\n    Load and format data from HuggingFace\n\n    Example:\n\n    ```python\n    (x_train, y_train), (x_test, y_test) = synalinks.datasets.gsm8k.load_data()\n    ```\n\n    Returns:\n        (tuple): The train and test data ready for training\n    \"\"\"\n    dataset = load_dataset(\"gsm8k\", \"main\")\n\n    x_train = []\n    y_train = []\n    x_test = []\n    y_test = []\n\n    for data_point in dataset[\"train\"]:\n        question = data_point[\"question\"]\n        thinking = data_point[\"answer\"].split(\"####\")[0].strip()\n        answer = data_point[\"answer\"].split(\"####\")[-1].strip()\n        x_train.append(MathQuestion(question=question))\n        y_train.append(\n            NumericalAnswerWithThinking(\n                thinking=thinking,\n                answer=float(answer.replace(\",\", \"\")),\n            )\n        )\n\n    for data_point in dataset[\"test\"]:\n        question = data_point[\"question\"]\n        thinking = data_point[\"answer\"].split(\"####\")[0].strip()\n        answer = data_point[\"answer\"].split(\"####\")[-1].strip()\n        x_test.append(MathQuestion(question=question))\n        y_test.append(\n            NumericalAnswerWithThinking(\n                thinking=thinking,\n                answer=float(answer.replace(\",\", \"\")),\n            )\n        )\n\n    x_train = np.array(x_train, dtype=\"object\")\n    y_train = np.array(y_train, dtype=\"object\")\n\n    x_test = np.array(x_test, dtype=\"object\")\n    y_test = np.array(y_test, dtype=\"object\")\n\n    return (x_train, y_train), (x_test, y_test)\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/HotpotQA/","title":"HotpotQA","text":""},{"location":"Synalinks%20API/Built-in%20Datasets/HotpotQA/#synalinks.src.datasets.hotpotqa.get_input_data_model","title":"<code>get_input_data_model()</code>","text":"<p>Returns HotpotQA input data model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The HotpotQA input data model</p> Source code in <code>synalinks/src/datasets/hotpotqa.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.hotpotqa.get_input_data_model\")\ndef get_input_data_model():\n    \"\"\"\n    Returns HotpotQA input data model for pipeline configurations.\n\n    Returns:\n        (DataModel): The HotpotQA input data model\n    \"\"\"\n    return Question\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/HotpotQA/#synalinks.src.datasets.hotpotqa.get_knowledge_data_model","title":"<code>get_knowledge_data_model()</code>","text":"<p>Returns HotpotQA knowledge data model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The HotpotQA knowledge data model</p> Source code in <code>synalinks/src/datasets/hotpotqa.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.hotpotqa.get_knowledge_data_model\")\ndef get_knowledge_data_model():\n    \"\"\"\n    Returns HotpotQA knowledge data model for pipeline configurations.\n\n    Returns:\n        (DataModel): The HotpotQA knowledge data model\n    \"\"\"\n    return Document\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/HotpotQA/#synalinks.src.datasets.hotpotqa.get_output_data_model","title":"<code>get_output_data_model()</code>","text":"<p>Returns HotpotQA output data model for pipeline configurations.</p> <p>Returns:</p> Type Description <code>DataModel</code> <p>The HotpotQA output data model</p> Source code in <code>synalinks/src/datasets/hotpotqa.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.hotpotqa.get_output_data_model\")\ndef get_output_data_model():\n    \"\"\"\n    Returns HotpotQA output data model for pipeline configurations.\n\n    Returns:\n        (DataModel): The HotpotQA output data model\n    \"\"\"\n    return Answer\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/HotpotQA/#synalinks.src.datasets.hotpotqa.load_data","title":"<code>load_data()</code>","text":"<p>Load and format data from HuggingFace</p> <p>Example:</p> <pre><code>(x_train, y_train), (x_test, y_test) = synalinks.datasets.hotpotqa.load_data()\n</code></pre> <p>Returns:</p> Type Description <code>tuple</code> <p>The train and test data ready for training</p> Source code in <code>synalinks/src/datasets/hotpotqa.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.hotpotqa.load_data\")\ndef load_data():\n    \"\"\"\n    Load and format data from HuggingFace\n\n    Example:\n\n    ```python\n    (x_train, y_train), (x_test, y_test) = synalinks.datasets.hotpotqa.load_data()\n    ```\n\n    Returns:\n        (tuple): The train and test data ready for training\n    \"\"\"\n    x_train = []\n    y_train = []\n    x_test = []\n    y_test = []\n\n    train_examples = load_dataset(\n        \"hotpot_qa\", \"fullwiki\", split=\"train\", trust_remote_code=True\n    )\n    eval_examples = load_dataset(\n        \"hotpot_qa\", \"fullwiki\", split=\"validation\", trust_remote_code=True\n    )\n\n    for raw_example in train_examples:\n        x_train.append(Question(question=raw_example[\"question\"]))\n        y_train.append(Answer(answer=raw_example[\"answer\"]))\n\n    for raw_example in eval_examples:\n        if raw_example[\"level\"] == \"hard\":\n            x_test.append(Question(question=raw_example[\"question\"]))\n            y_test.append(Answer(answer=raw_example[\"answer\"]))\n\n    x_train = np.array(x_train, dtype=\"object\")\n    y_train = np.array(y_train, dtype=\"object\")\n\n    x_test = np.array(x_test, dtype=\"object\")\n    y_test = np.array(y_test, dtype=\"object\")\n\n    return (x_train, y_train), (x_test, y_test)\n</code></pre>"},{"location":"Synalinks%20API/Built-in%20Datasets/HotpotQA/#synalinks.src.datasets.hotpotqa.load_knowledge","title":"<code>load_knowledge()</code>","text":"<p>Load and format data from HuggingFace</p> <p>Example:</p> <pre><code>knowledge = synalinks.datasets.hotpotqa.load_knowledge()\n</code></pre> <p>Returns:</p> Type Description <code>list</code> <p>The  data ready for knowledge injestion</p> Source code in <code>synalinks/src/datasets/hotpotqa.py</code> <pre><code>@synalinks_export(\"synalinks.datasets.hotpotqa.load_knowledge\")\ndef load_knowledge():\n    \"\"\"\n    Load and format data from HuggingFace\n\n    Example:\n\n    ```python\n    knowledge = synalinks.datasets.hotpotqa.load_knowledge()\n    ```\n\n    Returns:\n        (list): The  data ready for knowledge injestion\n    \"\"\"\n    documents = []\n    train_examples = load_dataset(\n        \"hotpot_qa\", \"fullwiki\", split=\"train\", trust_remote_code=True\n    )\n    for raw_example in train_examples:\n        context = raw_example.get(\"context\", None)\n        if context:\n            for i in range(len(context[\"title\"])):\n                documents.append(\n                    Document(\n                        title=context[\"title\"][i],\n                        text=\"\\n\".join(context[\"sentences\"][i]),\n                    )\n                )\n    documents = np.array(documents, dtype=\"object\")\n    return documents\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/","title":"Callbacks API","text":""},{"location":"Synalinks%20API/Callbacks%20API/#callbacks-api","title":"Callbacks API","text":"<p>A callback is an object that can perform various actions at multiple stages of the program's training. For example, at the start or end of an epoch, before or after a single batch, etc.</p>"},{"location":"Synalinks%20API/Callbacks%20API/#how-to-use-callbacks","title":"How to use Callbacks","text":"<p>You can pass a list of callbacks to the <code>.fit()</code> method of a program.</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    # ... you program declaration here\n\n    callbacks = [\n        synalinks.callbacks.CSVLogger(filepath=\"training_log.csv\"),\n        synalinks.callbacks.ProgramCheckpoint(\n            filepath=\"program.{epoch:02d}-{val_loss:.2f}.json\"\n        ),\n    ]\n\n    history = await program.fit(\n        x=x_train,\n        y=y_train,\n        epochs=10,\n        callbacks=callbacks,\n    )\n\nif __main__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/#callbacks-overview","title":"Callbacks Overview","text":"<ul> <li>Base Callback class</li> <li>CSVLogger callback</li> <li>ProgramCheckPoint callback</li> <li>Monitor callback</li> </ul>"},{"location":"Synalinks%20API/Callbacks%20API/BackUpAndRestore/","title":"BackUpAndRestore","text":""},{"location":"Synalinks%20API/Callbacks%20API/BackUpAndRestore/#synalinks.src.callbacks.backup_and_restore.BackupAndRestore","title":"<code>BackupAndRestore</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to back up and restore the training state.</p> <p><code>BackupAndRestore</code> callback is intended to recover training from an interruption that has happened in the middle of a <code>Program.fit</code> execution, by backing up the training states in a temporary checkpoint file, at the end of each epoch. Each backup overwrites the previously written checkpoint file, so at any given time there is at most one such checkpoint file for backup/restoring purpose.</p> <p>If training restarts before completion, the training state (which includes the <code>Program</code> weights and epoch number) is restored to the most recently saved state at the beginning of a new <code>Program.fit</code> run. At the completion of a <code>Program.fit</code> run, the temporary checkpoint file is deleted.</p> <p>Note that the user is responsible to bring jobs back after the interruption. This callback is important for the backup and restore mechanism for fault tolerance purpose, and the program to be restored from a previous checkpoint is expected to be the same as the one used to back up. If user changes arguments passed to compile or fit, the checkpoint saved for fault tolerance can become invalid.</p> <p>Example:</p> <pre><code>class InterruptingCallback(synalinks.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        if epoch == 4:\n            raise RuntimeError('Interrupting!')\n\ncallback = synalinks.callbacks.BackupAndRestore(backup_dir=\"/tmp/backup\")\nprogram = synalinks.programs.Sequential(\n    [synalinks.Generator(data_model=Answer)]\n)\nprogram.compile(\n    synalinks.optimizers.RandomFewShot(),\n    reward=synalinks.reward.ExactMatch(),\n)\nprogram.build(Query)\n\ntry:\n    program.fit(..., callbacks=[callback, InterruptingCallback()], verbose=0)\nexcept:\n    pass\n\nhistory = program.fit(\n    ..., epochs=10, batch_size=1, callbacks=[callback], verbose=0\n)\n# Only 6 more epochs are run, since first training got interrupted at\n# zero-indexed epoch 4, second training will continue from 4 to 9.\nlen(history.history['reward'])  # Returns 6\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>backup_dir</code> <code>str</code> <p>Path of directory where to store the data needed to restore the program. The directory cannot be reused elsewhere to store other files, e.g. by the <code>BackupAndRestore</code> callback of another training run, or by another callback (e.g. <code>ProgramCheckpoint</code>) of the same training run.</p> required <code>save_freq</code> <code>str | int</code> <p><code>\"epoch\"</code>, integer, or <code>False</code>. When set to <code>\"epoch\"</code> the callback saves the checkpoint at the end of each epoch. When set to an integer, the callback saves the checkpoint every <code>save_freq</code> batches. Set <code>save_freq=False</code> only if using preemption checkpointing (i.e. with <code>save_before_preemption=True</code>).</p> <code>'epoch'</code> <code>double_checkpoint</code> <code>bool</code> <p>If enabled, <code>BackupAndRestore</code> callback will save 2 last training states (current and previous). After interruption if current state can't be loaded due to IO error (e.g. file corrupted) it will try to restore previous one. Such behaviour will consume twice more space on disk, but increase fault tolerance. Defaults to <code>False</code>.</p> <code>False</code> <code>delete_checkpoint</code> <code>bool</code> <p>This <code>BackupAndRestore</code> callback works by saving a checkpoint to back up the training state. If <code>delete_checkpoint=True</code>, the checkpoint will be deleted after training is finished. Use <code>False</code> if you'd like to keep the checkpoint for future usage. Defaults to <code>True</code>.</p> <code>True</code> Source code in <code>synalinks/src/callbacks/backup_and_restore.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.BackupAndRestore\")\nclass BackupAndRestore(Callback):\n    \"\"\"Callback to back up and restore the training state.\n\n    `BackupAndRestore` callback is intended to recover training from an\n    interruption that has happened in the middle of a `Program.fit` execution, by\n    backing up the training states in a temporary checkpoint file, at the end of\n    each epoch. Each backup overwrites the previously written checkpoint file,\n    so at any given time there is at most one such checkpoint file for\n    backup/restoring purpose.\n\n    If training restarts before completion, the training state (which includes\n    the `Program` weights and epoch number) is restored to the most recently saved\n    state at the beginning of a new `Program.fit` run. At the completion of a\n    `Program.fit` run, the temporary checkpoint file is deleted.\n\n    Note that the user is responsible to bring jobs back after the interruption.\n    This callback is important for the backup and restore mechanism for fault\n    tolerance purpose, and the program to be restored from a previous checkpoint\n    is expected to be the same as the one used to back up. If user changes\n    arguments passed to compile or fit, the checkpoint saved for fault tolerance\n    can become invalid.\n\n    Example:\n\n    ```python\n    class InterruptingCallback(synalinks.callbacks.Callback):\n        def on_epoch_begin(self, epoch, logs=None):\n            if epoch == 4:\n                raise RuntimeError('Interrupting!')\n\n    callback = synalinks.callbacks.BackupAndRestore(backup_dir=\"/tmp/backup\")\n    program = synalinks.programs.Sequential(\n        [synalinks.Generator(data_model=Answer)]\n    )\n    program.compile(\n        synalinks.optimizers.RandomFewShot(),\n        reward=synalinks.reward.ExactMatch(),\n    )\n    program.build(Query)\n\n    try:\n        program.fit(..., callbacks=[callback, InterruptingCallback()], verbose=0)\n    except:\n        pass\n\n    history = program.fit(\n        ..., epochs=10, batch_size=1, callbacks=[callback], verbose=0\n    )\n    # Only 6 more epochs are run, since first training got interrupted at\n    # zero-indexed epoch 4, second training will continue from 4 to 9.\n    len(history.history['reward'])  # Returns 6\n    ```\n\n    Args:\n        backup_dir (str): Path of directory where to store the data\n            needed to restore the program. The directory\n            cannot be reused elsewhere to store other files, e.g. by the\n            `BackupAndRestore` callback of another training run,\n            or by another callback (e.g. `ProgramCheckpoint`)\n            of the same training run.\n        save_freq (str | int): `\"epoch\"`, integer, or `False`. When set to\n            `\"epoch\"` the callback saves the checkpoint at the end of each epoch.\n            When set to an integer, the callback saves the checkpoint every\n            `save_freq` batches. Set `save_freq=False` only if using\n            preemption checkpointing (i.e. with `save_before_preemption=True`).\n        double_checkpoint (bool): If enabled, `BackupAndRestore` callback\n            will save 2 last training states (current and previous). After\n            interruption if current state can't be loaded due to IO error\n            (e.g. file corrupted) it will try to restore previous one. Such\n            behaviour will consume twice more space on disk, but increase fault\n            tolerance. Defaults to `False`.\n        delete_checkpoint (bool): This `BackupAndRestore`\n            callback works by saving a checkpoint to back up the training state.\n            If `delete_checkpoint=True`, the checkpoint will be deleted after\n            training is finished. Use `False` if you'd like to keep the checkpoint\n            for future usage. Defaults to `True`.\n    \"\"\"\n\n    def __init__(\n        self,\n        backup_dir,\n        save_freq=\"epoch\",\n        double_checkpoint=False,\n        delete_checkpoint=True,\n    ):\n        super().__init__()\n        self.save_freq = save_freq\n        self.double_checkpoint = double_checkpoint\n        self.delete_checkpoint = delete_checkpoint\n        self._batches_seen_since_last_saving = 0\n        self._last_batch_seen = 0\n        self._current_epoch = 0\n\n        if not backup_dir:\n            raise ValueError(\"Empty `backup_dir` argument passed\")\n        self.backup_dir = backup_dir\n        self._variables_path = file_utils.join(backup_dir, \"latest.variables.json\")\n        self._training_metadata_path = file_utils.join(\n            backup_dir, \"training_metadata.json\"\n        )\n        self._prev_variables_path = self._variables_path + \".bkp\"\n        self._prev_training_metadata_path = self._training_metadata_path + \".bkp\"\n        if save_freq != \"epoch\" and not isinstance(save_freq, int):\n            raise ValueError(\n                \"Invalid value for argument `save_freq`. \"\n                f\"Received: save_freq={save_freq}. \"\n                \"Expected either 'epoch' or an integer value.\"\n            )\n\n    def on_train_begin(self, logs=None):\n        try:\n            self._load_program()\n        except OSError as e:\n            # Weights may be corrupted. Trying to load previous one.\n            if not file_utils.exists(self._prev_variables_path):\n                raise e\n            file_utils.copy(self._prev_variables_path, self._variables_path)\n            if file_utils.exists(self._prev_training_metadata_path):\n                file_utils.copy(\n                    self._prev_training_metadata_path,\n                    self._training_metadata_path,\n                )\n            elif file_utils.exists(self._training_metadata_path):\n                file_utils.remove(self._training_metadata_path)\n            self._load_program()\n\n    def _load_program(self):\n        \"\"\"Get training state from temporary file and restore it.\"\"\"\n        if not self.program.built:\n            raise ValueError(\n                \"To use the BackupAndRestore callback, \"\n                \"you program must be built before you call `fit()`. \"\n                f\"Program {self.program} is unbuilt. You can build it \"\n                \"beforehand by calling it on a batch of data.\"\n            )\n        if file_utils.exists(self._variables_path):\n            if self.program.optimizer is not None and not self.program.optimizer.built:\n                # Make sure optimizer variables exist before loading.\n                run_maybe_nested(\n                    self.program.optimizer.build(self.program.trainable_variables)\n                )\n            self.program.load_variables(self._variables_path)\n\n        if file_utils.exists(self._training_metadata_path):\n            with file_utils.File(self._training_metadata_path, \"rb\") as f:\n                training_metadata = orjson.loads(f.read())\n            epoch = training_metadata[\"epoch\"]\n            self.program._initial_epoch = epoch\n\n    def on_epoch_end(self, epoch, logs=None):\n        self._current_epoch = epoch + 1\n        self._last_batch_seen = 0\n        if self.save_freq == \"epoch\":\n            self._save_program()\n\n    def on_train_batch_end(self, batch, logs=None):\n        if self._should_save_on_batch(batch):\n            self._save_program()\n\n    def _save_program(self):\n        \"\"\"Saves the program.\n\n        Args:\n            epoch: the epoch this iteration is in.\n            batch: the batch this iteration is in. `None` if the `save_freq`\n                is set to `\"epoch\"`.\n            logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n        \"\"\"\n        # Create host directory if it doesn't exist.\n        if not file_utils.exists(self.backup_dir):\n            file_utils.makedirs(self.backup_dir)\n        if self.double_checkpoint and file_utils.exists(self._variables_path):\n            file_utils.copy(self._variables_path, self._prev_variables_path)\n        if self.double_checkpoint and file_utils.exists(self._training_metadata_path):\n            file_utils.copy(\n                self._training_metadata_path, self._prev_training_metadata_path\n            )\n        self.program.save_variables(filepath=self._variables_path, overwrite=True)\n        with file_utils.File(self._training_metadata_path, \"wb\") as f:\n            training_metadata = {\n                \"epoch\": self._current_epoch,\n                \"batch\": self._last_batch_seen,\n            }\n            f.write(orjson.dumps(training_metadata))\n\n    def _should_save_on_batch(self, batch):\n        \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n        if self.save_freq == \"epoch\":\n            return False\n        if batch &lt;= self._last_batch_seen:  # New epoch.\n            add_batches = batch + 1  # batches are zero-indexed.\n        else:\n            add_batches = batch - self._last_batch_seen\n        self._batches_seen_since_last_saving += add_batches\n        self._last_batch_seen = batch\n\n        if self._batches_seen_since_last_saving &gt;= self.save_freq:\n            self._batches_seen_since_last_saving = 0\n            return True\n        return False\n\n    def on_train_end(self, logs=None):\n        if self.delete_checkpoint and file_utils.exists(self.backup_dir):\n            file_utils.rmtree(self.backup_dir)\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/","title":"Base Callback class","text":""},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback","title":"<code>Callback</code>","text":"<p>Base class used to build new callbacks.</p> <p>Callbacks can be passed to synalinks methods such as <code>fit()</code>, <code>evaluate()</code>, and <code>predict()</code> in order to hook into the various stages of the program training, evaluation, and inference lifecycle.</p> <p>To create a custom callback, subclass <code>synalinks.callbacks.Callback</code> and override the method associated with the stage of interest.</p> <p>If you want to use <code>Callback</code> objects in a custom training loop:</p> <ol> <li>You should pack all your callbacks into a single <code>callbacks.CallbackList</code>    so they can all be called together.</li> <li>You will need to manually call all the <code>on_*</code> methods at the appropriate    locations in your loop.</li> </ol> <p>The <code>logs</code> dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch (see method-specific docstrings).</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>dict</code> <p>Training parameters (eg. verbosity, batch size, number of epochs...).</p> <code>program</code> <code>Program</code> <p>Instance of <code>Program</code>. Reference of the program being trained.</p> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.Callback\")\nclass Callback:\n    \"\"\"Base class used to build new callbacks.\n\n    Callbacks can be passed to synalinks methods such as `fit()`, `evaluate()`, and\n    `predict()` in order to hook into the various stages of the program training,\n    evaluation, and inference lifecycle.\n\n    To create a custom callback, subclass `synalinks.callbacks.Callback` and\n    override the method associated with the stage of interest.\n\n    If you want to use `Callback` objects in a custom training loop:\n\n    1. You should pack all your callbacks into a single `callbacks.CallbackList`\n       so they can all be called together.\n    2. You will need to manually call all the `on_*` methods at the appropriate\n       locations in your loop.\n\n    The `logs` dictionary that callback methods\n    take as argument will contain keys for quantities relevant to\n    the current batch or epoch (see method-specific docstrings).\n\n    Attributes:\n        params (dict): Training parameters\n            (eg. verbosity, batch size, number of epochs...).\n        program (Program): Instance of `Program`.\n            Reference of the program being trained.\n    \"\"\"\n\n    def __init__(self):\n        self.params = None\n        self._program = None\n\n    def set_params(self, params):\n        self.params = params\n\n    def set_program(self, program):\n        self._program = program\n\n    @property\n    def program(self):\n        return self._program\n\n    @utils.default\n    def on_batch_begin(self, batch, logs=None):\n        \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n\n    @utils.default\n    def on_batch_end(self, batch, logs=None):\n        \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n\n    @utils.default\n    def on_epoch_begin(self, epoch, logs=None):\n        \"\"\"Called at the start of an epoch.\n\n        Subclasses should override for any actions to run. This function should\n        only be called during TRAIN mode.\n\n        Args:\n            epoch (int): Index of epoch.\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of an epoch.\n\n        Subclasses should override for any actions to run. This function should\n        only be called during TRAIN mode.\n\n        Args:\n            epoch (int): Index of epoch.\n            logs (dict): Metric results for this training epoch, and for the\n                validation epoch if validation is performed. Validation result\n                keys are prefixed with `val_`. For training epoch, the values of\n                the `Program`'s metrics are returned. Example:\n                `{'reward': 0.2, 'accuracy': 0.7}`.\n        \"\"\"\n\n    @utils.default\n    def on_train_batch_begin(self, batch, logs=None):\n        \"\"\"Called at the beginning of a training batch in `fit` methods.\n\n        Subclasses should override for any actions to run.\n\n        Note that if the `steps_per_execution` argument to `compile` in\n        `Program` is set to `N`, this method will only be called every\n        `N` batches.\n\n        Args:\n            batch (int): Index of batch within the current epoch.\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n        # For backwards compatibility.\n        self.on_batch_begin(batch, logs=logs)\n\n    @utils.default\n    def on_train_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a training batch in `fit` methods.\n\n        Subclasses should override for any actions to run.\n\n        Note that if the `steps_per_execution` argument to `compile` in\n        `Program` is set to `N`, this method will only be called every\n        `N` batches.\n\n        Args:\n            batch (int): Index of batch within the current epoch.\n            logs (dict): Aggregated metric results up until this batch.\n        \"\"\"\n        # For backwards compatibility.\n        self.on_batch_end(batch, logs=logs)\n\n    @utils.default\n    def on_test_batch_begin(self, batch, logs=None):\n        \"\"\"Called at the beginning of a batch in `evaluate` methods.\n\n        Also called at the beginning of a validation batch in the `fit`\n        methods, if validation data is provided.\n\n        Subclasses should override for any actions to run.\n\n        Note that if the `steps_per_execution` argument to `compile` in\n        `Program` is set to `N`, this method will only be called every\n        `N` batches.\n\n        Args:\n            batch (int): Index of batch within the current epoch.\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_test_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a batch in `evaluate` methods.\n\n        Also called at the end of a validation batch in the `fit`\n        methods, if validation data is provided.\n\n        Subclasses should override for any actions to run.\n\n        Note that if the `steps_per_execution` argument to `compile` in\n        `Program` is set to `N`, this method will only be called every\n        `N` batches.\n\n        Args:\n            batch (int): Index of batch within the current epoch.\n            logs (dict): Aggregated metric results up until this batch.\n        \"\"\"\n\n    @utils.default\n    def on_predict_batch_begin(self, batch, logs=None):\n        \"\"\"Called at the beginning of a batch in `predict` methods.\n\n        Subclasses should override for any actions to run.\n\n        Note that if the `steps_per_execution` argument to `compile` in\n        `Program` is set to `N`, this method will only be called every\n        `N` batches.\n\n        Args:\n            batch (int): Index of batch within the current epoch.\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_predict_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a batch in `predict` methods.\n\n        Subclasses should override for any actions to run.\n\n        Note that if the `steps_per_execution` argument to `compile` in\n        `Program` is set to `N`, this method will only be called every\n        `N` batches.\n\n        Args:\n            batch (int): Index of batch within the current epoch.\n            logs (dict): Aggregated metric results up until this batch.\n        \"\"\"\n\n    @utils.default\n    def on_train_begin(self, logs=None):\n        \"\"\"Called at the beginning of training.\n\n        Subclasses should override for any actions to run.\n\n        Args:\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_train_end(self, logs=None):\n        \"\"\"Called at the end of training.\n\n        Subclasses should override for any actions to run.\n\n        Args:\n            logs (dict): Currently the output of the last call to\n                `on_epoch_end()` is passed to this argument for this method but\n                that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_test_begin(self, logs=None):\n        \"\"\"Called at the beginning of evaluation or validation.\n\n        Subclasses should override for any actions to run.\n\n        Args:\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_test_end(self, logs=None):\n        \"\"\"Called at the end of evaluation or validation.\n\n        Subclasses should override for any actions to run.\n\n        Args:\n            logs (dict): Currently the output of the last call to\n                `on_test_batch_end()` is passed to this argument for this method\n                but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_predict_begin(self, logs=None):\n        \"\"\"Called at the beginning of prediction.\n\n        Subclasses should override for any actions to run.\n\n        Args:\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n\n    @utils.default\n    def on_predict_end(self, logs=None):\n        \"\"\"Called at the end of prediction.\n\n        Subclasses should override for any actions to run.\n\n        Args:\n            logs (dict): Currently no data is passed to this argument for this\n                method but that may change in the future.\n        \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_batch_begin","title":"<code>on_batch_begin(batch, logs=None)</code>","text":"<p>A backwards compatibility alias for <code>on_train_batch_begin</code>.</p> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_batch_begin(self, batch, logs=None):\n    \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_batch_end","title":"<code>on_batch_end(batch, logs=None)</code>","text":"<p>A backwards compatibility alias for <code>on_train_batch_end</code>.</p> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_batch_end(self, batch, logs=None):\n    \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_epoch_begin","title":"<code>on_epoch_begin(epoch, logs=None)</code>","text":"<p>Called at the start of an epoch.</p> <p>Subclasses should override for any actions to run. This function should only be called during TRAIN mode.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Index of epoch.</p> required <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_epoch_begin(self, epoch, logs=None):\n    \"\"\"Called at the start of an epoch.\n\n    Subclasses should override for any actions to run. This function should\n    only be called during TRAIN mode.\n\n    Args:\n        epoch (int): Index of epoch.\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_epoch_end","title":"<code>on_epoch_end(epoch, logs=None)</code>","text":"<p>Called at the end of an epoch.</p> <p>Subclasses should override for any actions to run. This function should only be called during TRAIN mode.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Index of epoch.</p> required <code>logs</code> <code>dict</code> <p>Metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with <code>val_</code>. For training epoch, the values of the <code>Program</code>'s metrics are returned. Example: <code>{'reward': 0.2, 'accuracy': 0.7}</code>.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_epoch_end(self, epoch, logs=None):\n    \"\"\"Called at the end of an epoch.\n\n    Subclasses should override for any actions to run. This function should\n    only be called during TRAIN mode.\n\n    Args:\n        epoch (int): Index of epoch.\n        logs (dict): Metric results for this training epoch, and for the\n            validation epoch if validation is performed. Validation result\n            keys are prefixed with `val_`. For training epoch, the values of\n            the `Program`'s metrics are returned. Example:\n            `{'reward': 0.2, 'accuracy': 0.7}`.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_predict_batch_begin","title":"<code>on_predict_batch_begin(batch, logs=None)</code>","text":"<p>Called at the beginning of a batch in <code>predict</code> methods.</p> <p>Subclasses should override for any actions to run.</p> <p>Note that if the <code>steps_per_execution</code> argument to <code>compile</code> in <code>Program</code> is set to <code>N</code>, this method will only be called every <code>N</code> batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>Index of batch within the current epoch.</p> required <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_predict_batch_begin(self, batch, logs=None):\n    \"\"\"Called at the beginning of a batch in `predict` methods.\n\n    Subclasses should override for any actions to run.\n\n    Note that if the `steps_per_execution` argument to `compile` in\n    `Program` is set to `N`, this method will only be called every\n    `N` batches.\n\n    Args:\n        batch (int): Index of batch within the current epoch.\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_predict_batch_end","title":"<code>on_predict_batch_end(batch, logs=None)</code>","text":"<p>Called at the end of a batch in <code>predict</code> methods.</p> <p>Subclasses should override for any actions to run.</p> <p>Note that if the <code>steps_per_execution</code> argument to <code>compile</code> in <code>Program</code> is set to <code>N</code>, this method will only be called every <code>N</code> batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>Index of batch within the current epoch.</p> required <code>logs</code> <code>dict</code> <p>Aggregated metric results up until this batch.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_predict_batch_end(self, batch, logs=None):\n    \"\"\"Called at the end of a batch in `predict` methods.\n\n    Subclasses should override for any actions to run.\n\n    Note that if the `steps_per_execution` argument to `compile` in\n    `Program` is set to `N`, this method will only be called every\n    `N` batches.\n\n    Args:\n        batch (int): Index of batch within the current epoch.\n        logs (dict): Aggregated metric results up until this batch.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_predict_begin","title":"<code>on_predict_begin(logs=None)</code>","text":"<p>Called at the beginning of prediction.</p> <p>Subclasses should override for any actions to run.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_predict_begin(self, logs=None):\n    \"\"\"Called at the beginning of prediction.\n\n    Subclasses should override for any actions to run.\n\n    Args:\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_predict_end","title":"<code>on_predict_end(logs=None)</code>","text":"<p>Called at the end of prediction.</p> <p>Subclasses should override for any actions to run.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_predict_end(self, logs=None):\n    \"\"\"Called at the end of prediction.\n\n    Subclasses should override for any actions to run.\n\n    Args:\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_test_batch_begin","title":"<code>on_test_batch_begin(batch, logs=None)</code>","text":"<p>Called at the beginning of a batch in <code>evaluate</code> methods.</p> <p>Also called at the beginning of a validation batch in the <code>fit</code> methods, if validation data is provided.</p> <p>Subclasses should override for any actions to run.</p> <p>Note that if the <code>steps_per_execution</code> argument to <code>compile</code> in <code>Program</code> is set to <code>N</code>, this method will only be called every <code>N</code> batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>Index of batch within the current epoch.</p> required <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_test_batch_begin(self, batch, logs=None):\n    \"\"\"Called at the beginning of a batch in `evaluate` methods.\n\n    Also called at the beginning of a validation batch in the `fit`\n    methods, if validation data is provided.\n\n    Subclasses should override for any actions to run.\n\n    Note that if the `steps_per_execution` argument to `compile` in\n    `Program` is set to `N`, this method will only be called every\n    `N` batches.\n\n    Args:\n        batch (int): Index of batch within the current epoch.\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_test_batch_end","title":"<code>on_test_batch_end(batch, logs=None)</code>","text":"<p>Called at the end of a batch in <code>evaluate</code> methods.</p> <p>Also called at the end of a validation batch in the <code>fit</code> methods, if validation data is provided.</p> <p>Subclasses should override for any actions to run.</p> <p>Note that if the <code>steps_per_execution</code> argument to <code>compile</code> in <code>Program</code> is set to <code>N</code>, this method will only be called every <code>N</code> batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>Index of batch within the current epoch.</p> required <code>logs</code> <code>dict</code> <p>Aggregated metric results up until this batch.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_test_batch_end(self, batch, logs=None):\n    \"\"\"Called at the end of a batch in `evaluate` methods.\n\n    Also called at the end of a validation batch in the `fit`\n    methods, if validation data is provided.\n\n    Subclasses should override for any actions to run.\n\n    Note that if the `steps_per_execution` argument to `compile` in\n    `Program` is set to `N`, this method will only be called every\n    `N` batches.\n\n    Args:\n        batch (int): Index of batch within the current epoch.\n        logs (dict): Aggregated metric results up until this batch.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_test_begin","title":"<code>on_test_begin(logs=None)</code>","text":"<p>Called at the beginning of evaluation or validation.</p> <p>Subclasses should override for any actions to run.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_test_begin(self, logs=None):\n    \"\"\"Called at the beginning of evaluation or validation.\n\n    Subclasses should override for any actions to run.\n\n    Args:\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_test_end","title":"<code>on_test_end(logs=None)</code>","text":"<p>Called at the end of evaluation or validation.</p> <p>Subclasses should override for any actions to run.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>Currently the output of the last call to <code>on_test_batch_end()</code> is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_test_end(self, logs=None):\n    \"\"\"Called at the end of evaluation or validation.\n\n    Subclasses should override for any actions to run.\n\n    Args:\n        logs (dict): Currently the output of the last call to\n            `on_test_batch_end()` is passed to this argument for this method\n            but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_train_batch_begin","title":"<code>on_train_batch_begin(batch, logs=None)</code>","text":"<p>Called at the beginning of a training batch in <code>fit</code> methods.</p> <p>Subclasses should override for any actions to run.</p> <p>Note that if the <code>steps_per_execution</code> argument to <code>compile</code> in <code>Program</code> is set to <code>N</code>, this method will only be called every <code>N</code> batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>Index of batch within the current epoch.</p> required <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_train_batch_begin(self, batch, logs=None):\n    \"\"\"Called at the beginning of a training batch in `fit` methods.\n\n    Subclasses should override for any actions to run.\n\n    Note that if the `steps_per_execution` argument to `compile` in\n    `Program` is set to `N`, this method will only be called every\n    `N` batches.\n\n    Args:\n        batch (int): Index of batch within the current epoch.\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n    # For backwards compatibility.\n    self.on_batch_begin(batch, logs=logs)\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_train_batch_end","title":"<code>on_train_batch_end(batch, logs=None)</code>","text":"<p>Called at the end of a training batch in <code>fit</code> methods.</p> <p>Subclasses should override for any actions to run.</p> <p>Note that if the <code>steps_per_execution</code> argument to <code>compile</code> in <code>Program</code> is set to <code>N</code>, this method will only be called every <code>N</code> batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>Index of batch within the current epoch.</p> required <code>logs</code> <code>dict</code> <p>Aggregated metric results up until this batch.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_train_batch_end(self, batch, logs=None):\n    \"\"\"Called at the end of a training batch in `fit` methods.\n\n    Subclasses should override for any actions to run.\n\n    Note that if the `steps_per_execution` argument to `compile` in\n    `Program` is set to `N`, this method will only be called every\n    `N` batches.\n\n    Args:\n        batch (int): Index of batch within the current epoch.\n        logs (dict): Aggregated metric results up until this batch.\n    \"\"\"\n    # For backwards compatibility.\n    self.on_batch_end(batch, logs=logs)\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_train_begin","title":"<code>on_train_begin(logs=None)</code>","text":"<p>Called at the beginning of training.</p> <p>Subclasses should override for any actions to run.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>Currently no data is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_train_begin(self, logs=None):\n    \"\"\"Called at the beginning of training.\n\n    Subclasses should override for any actions to run.\n\n    Args:\n        logs (dict): Currently no data is passed to this argument for this\n            method but that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Base%20Callback%20class/#synalinks.src.callbacks.callback.Callback.on_train_end","title":"<code>on_train_end(logs=None)</code>","text":"<p>Called at the end of training.</p> <p>Subclasses should override for any actions to run.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>Currently the output of the last call to <code>on_epoch_end()</code> is passed to this argument for this method but that may change in the future.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/callback.py</code> <pre><code>@utils.default\ndef on_train_end(self, logs=None):\n    \"\"\"Called at the end of training.\n\n    Subclasses should override for any actions to run.\n\n    Args:\n        logs (dict): Currently the output of the last call to\n            `on_epoch_end()` is passed to this argument for this method but\n            that may change in the future.\n    \"\"\"\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/CSVLogger/","title":"CSVLogger","text":""},{"location":"Synalinks%20API/Callbacks%20API/CSVLogger/#synalinks.src.callbacks.csv_logger.CSVLogger","title":"<code>CSVLogger</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback that streams epoch results to a CSV file.</p> <p>Supports all values that can be represented as a string, including 1D iterables such as <code>np.ndarray</code>.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | PathLike</code> <p>Filepath of the CSV file, e.g. <code>'run/log.csv'</code>.</p> required <code>separator</code> <code>str</code> <p>String used to separate elements in the CSV file.</p> <code>','</code> <code>append</code> <code>bool</code> <p>True: append if file exists (useful for continuing training). False: overwrite existing file.</p> <code>False</code> <p>Example:</p> <pre><code>csv_logger = CSVLogger(filepath='training.log')\nprogram.fit(x_train, y_train, callbacks=[csv_logger])\n</code></pre> Source code in <code>synalinks/src/callbacks/csv_logger.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.CSVLogger\")\nclass CSVLogger(Callback):\n    \"\"\"Callback that streams epoch results to a CSV file.\n\n    Supports all values that can be represented as a string,\n    including 1D iterables such as `np.ndarray`.\n\n    Args:\n        filepath (str | os.PathLike): Filepath of the CSV file, e.g. `'run/log.csv'`.\n        separator (str): String used to separate elements in the CSV file.\n        append (bool): True: append if file exists (useful for continuing\n            training). False: overwrite existing file.\n\n    Example:\n\n    ```python\n    csv_logger = CSVLogger(filepath='training.log')\n    program.fit(x_train, y_train, callbacks=[csv_logger])\n    ```\n    \"\"\"\n\n    def __init__(self, filepath, separator=\",\", append=False):\n        super().__init__()\n        self.sep = separator\n        self.filepath = file_utils.path_to_string(filepath)\n        self.append = append\n        self.writer = None\n        self.keys = None\n        self.append_header = True\n\n    def on_train_begin(self, logs=None):\n        if self.append:\n            if file_utils.exists(self.filepath):\n                with file_utils.File(self.filepath, \"r\") as f:\n                    self.append_header = not bool(len(f.readline()))\n            mode = \"a\"\n        else:\n            mode = \"w\"\n        self.csv_file = file_utils.File(self.filepath, mode)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        def handle_value(k):\n            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n            if isinstance(k, str):\n                return k\n            elif isinstance(k, collections.abc.Iterable) and not is_zero_dim_ndarray:\n                return f'\"[{\", \".join(map(str, k))}]\"'\n            else:\n                return k\n\n        if self.keys is None:\n            self.keys = sorted(logs.keys())\n            # When validation_freq &gt; 1, `val_` keys are not in first epoch logs\n            # Add the `val_` keys so that its part of the fieldnames of writer.\n            val_keys_found = False\n            for key in self.keys:\n                if key.startswith(\"val_\"):\n                    val_keys_found = True\n                    break\n            if not val_keys_found:\n                self.keys.extend([\"val_\" + k for k in self.keys])\n\n        if not self.writer:\n\n            class CustomDialect(csv.excel):\n                delimiter = self.sep\n\n            fieldnames = [\"epoch\"] + self.keys\n\n            self.writer = csv.DictWriter(\n                self.csv_file, fieldnames=fieldnames, dialect=CustomDialect\n            )\n            if self.append_header:\n                self.writer.writeheader()\n\n        row_dict = collections.OrderedDict({\"epoch\": epoch})\n        row_dict.update((key, handle_value(logs.get(key, \"NA\"))) for key in self.keys)\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()\n\n    def on_train_end(self, logs=None):\n        self.csv_file.close()\n        self.writer = None\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/EarlyStopping/","title":"EarlyStopping","text":""},{"location":"Synalinks%20API/Callbacks%20API/EarlyStopping/#synalinks.src.callbacks.early_stopping.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Stop training when a monitored metric has stopped improving.</p> <p>Assuming the goal of a training is to maximize the reward. With this, the metric to be monitored would be <code>reward</code>, and mode would be <code>max</code>. A <code>program.fit()</code> training loop will check at end of every epoch whether the reward is no longer augmenting, considering the <code>min_delta</code> and <code>patience</code> if applicable. Once it's found no longer increasing, <code>program.stop_training</code> is marked True and the training terminates.</p> <p>The quantity to be monitored needs to be available in <code>logs</code> dict To make it so, pass the reward or metrics at <code>program.compile</code>.</p> <p>Example:</p> <p>callback = synalinks.callbacks.EarlyStopping(monitor='reward', patience=3)</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>str</code> <p>Quantity to be monitored. Defaults to <code>val_reward</code>.</p> <code>'val_reward'</code> <code>min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Defaults to <code>0</code>.</p> <code>0</code> <code>stop_at</code> <code>float</code> <p>The value at which we should stop training.</p> <code>1.0</code> <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped. Defaults to <code>0</code>.</p> <code>0</code> <code>verbose</code> <code>int</code> <p>Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays messages when the callback takes an action. Defaults to <code>0</code>.</p> <code>0</code> <code>mode</code> <code>str</code> <p>One of <code>{\"auto\", \"min\", \"max\"}</code>. In <code>min</code> mode, training will stop when the quantity monitored has stopped decreasing; in <code>\"max\"</code> mode it will stop when the quantity monitored has stopped increasing; in <code>\"auto\"</code> mode, the direction is automatically inferred from the name of the monitored quantity. Defaults to <code>\"auto\"</code>.</p> <code>'auto'</code> <code>baseline</code> <code>float</code> <p>Baseline value for the monitored quantity. If not <code>None</code>, training will stop if the program doesn't show improvement over the baseline. Defaults to <code>None</code>.</p> <code>None</code> <code>restore_best_variables</code> <code>bool</code> <p>Whether to restore program variables from the epoch with the best value of the monitored quantity. If <code>False</code>, the program variables obtained at the last step of training are used. An epoch will be restored regardless of the performance relative to the <code>baseline</code>. If no epoch improves on <code>baseline</code>, training will run for <code>patience</code> epochs and restore variables from the best epoch in that set. Defaults to <code>False</code>.</p> <code>False</code> <code>start_from_epoch</code> <code>int</code> <p>Number of epochs to wait before starting to monitor improvement. This allows for a warm-up period in which no improvement is expected and thus training will not be stopped. Defaults to <code>0</code>.</p> <code>0</code> Source code in <code>synalinks/src/callbacks/early_stopping.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.EarlyStopping\")\nclass EarlyStopping(Callback):\n    \"\"\"Stop training when a monitored metric has stopped improving.\n\n    Assuming the goal of a training is to maximize the reward.\n    With this, the metric to be monitored would be `reward`,\n    and mode would be `max`. A `program.fit()` training loop\n    will check at end of every epoch whether the reward is no\n    longer augmenting, considering the `min_delta` and `patience`\n    if applicable. Once it's found no longer increasing,\n    `program.stop_training` is marked True and the training terminates.\n\n    The quantity to be monitored needs to be available in `logs` dict\n    To make it so, pass the reward or metrics at `program.compile`.\n\n    Example:\n\n    &gt;&gt;&gt; callback = synalinks.callbacks.EarlyStopping(monitor='reward', patience=3)\n    &gt;&gt;&gt; # This callback will stop the training when there is no improvement in\n    &gt;&gt;&gt; # the loss for three consecutive epochs.\n    &gt;&gt;&gt; program = synalinks.programs.Sequential(\n    ...     [synalinks.modules.Generator(data_model=Answer)])\n    &gt;&gt;&gt; program.compile(\n    ...     synalinks.optimizers.RandomFewShot(),\n    ...     reward=synalinks.rewards.ExactMatch())\n    &gt;&gt;&gt; history = program.fit(\n    ...     ..., epochs=10, batch_size=1, callbacks=[callback], verbose=0)\n    &gt;&gt;&gt; len(history.history['reward'])  # Only 4 epochs are run.\n    4\n\n    Args:\n        monitor (str): Quantity to be monitored. Defaults to `val_reward`.\n        min_delta (float): Minimum change in the monitored quantity to qualify\n            as an improvement, i.e. an absolute change of less than min_delta,\n            will count as no improvement. Defaults to `0`.\n        stop_at (float): The value at which we should stop training.\n        patience (int): Number of epochs with no improvement after which\n            training will be stopped. Defaults to `0`.\n        verbose (int): Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n            displays messages when the callback takes an action. Defaults to `0`.\n        mode (str): One of `{\"auto\", \"min\", \"max\"}`. In `min` mode, training\n            will stop when the quantity monitored has stopped decreasing; in\n            `\"max\"` mode it will stop when the quantity monitored has stopped\n            increasing; in `\"auto\"` mode, the direction is automatically\n            inferred from the name of the monitored quantity. Defaults to `\"auto\"`.\n        baseline (float): Baseline value for the monitored quantity. If not\n            `None`, training will stop if the program doesn't show improvement\n            over the baseline. Defaults to `None`.\n        restore_best_variables (bool): Whether to restore program variables from\n            the epoch with the best value of the monitored quantity. If `False`,\n            the program variables obtained at the last step of training are used.\n            An epoch will be restored regardless of the performance relative to\n            the `baseline`. If no epoch improves on `baseline`, training will run\n            for `patience` epochs and restore variables from the best epoch in\n            that set. Defaults to `False`.\n        start_from_epoch (int): Number of epochs to wait before starting to\n            monitor improvement. This allows for a warm-up period in which no\n            improvement is expected and thus training will not be stopped.\n            Defaults to `0`.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        monitor=\"val_reward\",\n        min_delta=0,\n        stop_at=1.0,\n        patience=0,\n        verbose=0,\n        mode=\"auto\",\n        baseline=None,\n        restore_best_variables=False,\n        start_from_epoch=0,\n    ):\n        super().__init__()\n\n        self.monitor = monitor\n        self.patience = patience\n        self.verbose = verbose\n        self.baseline = baseline\n        self.min_delta = abs(min_delta)\n        self.stop_at = stop_at\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.restore_best_variables = restore_best_variables\n        self.best_variables = None\n        self.start_from_epoch = start_from_epoch\n\n        if mode not in [\"auto\", \"min\", \"max\"]:\n            warnings.warn(\n                f\"EarlyStopping mode {mode} is unknown, fallback to auto mode.\",\n                stacklevel=2,\n            )\n            mode = \"auto\"\n        self.mode = mode\n        self.monitor_op = None\n\n    def _set_monitor_op(self):\n        if self.mode == \"min\":\n            self.monitor_op = np.less\n        elif self.mode == \"max\":\n            self.monitor_op = np.greater\n        else:\n            metric_name = self.monitor.removeprefix(\"val_\")\n            if metric_name == \"reward\":\n                self.monitor_op = np.greater\n            if hasattr(self.program, \"metrics\"):\n                all_metrics = []\n                for m in self.program.metrics:\n                    if isinstance(\n                        m,\n                        (\n                            compile_utils.CompileMetrics,\n                            compile_utils.MetricsList,\n                        ),\n                    ):\n                        all_metrics.extend(m.metrics)\n                for m in all_metrics:\n                    if m.name == metric_name:\n                        if hasattr(m, \"_direction\"):\n                            if m._direction == \"up\":\n                                self.monitor_op = np.greater\n                            else:\n                                self.monitor_op = np.less\n        if self.monitor_op is None:\n            raise ValueError(\n                f\"EarlyStopping callback received monitor={self.monitor} \"\n                \"but Synalinks isn't able to automatically determine whether \"\n                \"that metric should be maximized or minimized. \"\n                \"Pass `mode='max'` in order to do early stopping based \"\n                \"on the highest metric value, or pass `mode='min'` \"\n                \"in order to use the lowest value.\"\n            )\n        if self.monitor_op == np.less:\n            self.min_delta *= -1\n\n    def on_train_begin(self, logs=None):\n        # Allow instances to be re-used\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.best = None\n        self.best_variables = None\n        self.best_epoch = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.monitor_op is None:\n            # Delay setup until the model's metrics are all built\n            self._set_monitor_op()\n\n        current = self.get_monitor_value(logs)\n        if current &gt;= self.stop_at:\n            self.program.stop_training = True\n            return\n        if current is None or epoch &lt; self.start_from_epoch:\n            # If no monitor value exists or still in initial warm-up stage.\n            return\n        if self.restore_best_variables and self.best_variables is None:\n            # If best variables were never set,\n            # then the current variables are the best.\n            self.best_variables = self.program.get_variables()\n            self.best_epoch = epoch\n\n        self.wait += 1\n        if self._is_improvement(current, self.best):\n            self.best = current\n            self.best_epoch = epoch\n            if self.restore_best_variables:\n                self.best_variables = self.program.get_variables()\n            # Only restart wait if we beat both the baseline and our previous\n            # best.\n            if self.baseline is None or self._is_improvement(current, self.baseline):\n                self.wait = 0\n            return\n\n        if self.wait &gt;= self.patience and epoch &gt; 0:\n            # Patience has been exceeded: stop training\n            self.stopped_epoch = epoch\n            self.program.stop_training = True\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch &gt; 0 and self.verbose &gt; 0:\n            io_utils.print_msg(f\"Epoch {self.stopped_epoch + 1}: early stopping\")\n        if self.restore_best_variables and self.best_variables is not None:\n            if self.verbose &gt; 0:\n                io_utils.print_msg(\n                    \"Restoring model variables from \"\n                    \"the end of the best epoch: \"\n                    f\"{self.best_epoch + 1}.\"\n                )\n            self.program.set_variables(self.best_variables)\n\n    def get_monitor_value(self, logs):\n        logs = logs or {}\n        monitor_value = logs.get(self.monitor)\n        if monitor_value is None:\n            warnings.warn(\n                (\n                    f\"Early stopping conditioned on metric `{self.monitor}` \"\n                    \"which is not available. \"\n                    f\"Available metrics are: {','.join(list(logs.keys()))}\"\n                ),\n                stacklevel=2,\n            )\n        return monitor_value\n\n    def _is_improvement(self, monitor_value, reference_value):\n        if reference_value is None:\n            return True\n        return self.monitor_op(monitor_value - self.min_delta, reference_value)\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/EarlyStopping/#synalinks.src.callbacks.early_stopping.EarlyStopping--this-callback-will-stop-the-training-when-there-is-no-improvement-in","title":"This callback will stop the training when there is no improvement in","text":""},{"location":"Synalinks%20API/Callbacks%20API/EarlyStopping/#synalinks.src.callbacks.early_stopping.EarlyStopping--the-loss-for-three-consecutive-epochs","title":"the loss for three consecutive epochs.","text":"<p>program = synalinks.programs.Sequential( ...     [synalinks.modules.Generator(data_model=Answer)]) program.compile( ...     synalinks.optimizers.RandomFewShot(), ...     reward=synalinks.rewards.ExactMatch()) history = program.fit( ...     ..., epochs=10, batch_size=1, callbacks=[callback], verbose=0) len(history.history['reward'])  # Only 4 epochs are run. 4</p>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/","title":"Monitor","text":""},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor","title":"<code>Monitor</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Monitor callback for logging training metrics to MLflow.</p> <p>This callback logs training progress and evaluation metrics to MLflow for experiment tracking and visualization.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the MLflow experiment. If None, uses the program name.</p> <code>None</code> <code>run_name</code> <code>str</code> <p>Name of the MLflow run. If None, auto-generated.</p> <code>None</code> <code>tracking_uri</code> <code>str</code> <p>MLflow tracking server URI. If None, uses the default (local ./mlruns directory or MLFLOW_TRACKING_URI env var).</p> <code>None</code> <code>log_batch_metrics</code> <code>bool</code> <p>Whether to log metrics at batch level (default: False).</p> <code>False</code> <code>log_epoch_metrics</code> <code>bool</code> <p>Whether to log metrics at epoch level (default: True).</p> <code>True</code> <code>log_program_plot</code> <code>bool</code> <p>Whether to log the program plot as an artifact at the beginning of training (default: True).</p> <code>True</code> <code>log_program_model</code> <code>bool</code> <p>Whether to log the program as an MLflow model at the end of training (default: True).</p> <code>True</code> <code>tags</code> <code>dict</code> <p>Optional tags to add to the MLflow run.</p> <code>None</code> <p>Example:</p> <pre><code>import synalinks\n\n# Basic usage - uses local MLflow storage\nmonitor = synalinks.callbacks.Monitor(experiment_name=\"my_experiment\")\n\n# With custom MLflow tracking server\nmonitor = synalinks.callbacks.Monitor(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"my_experiment\",\n    run_name=\"training_run_1\",\n    log_program_plot=True,\n    log_program_model=True,\n    tags={\"model_type\": \"chain_of_thought\"}\n)\n\n# Use in training\nprogram.fit(\n    x=train_data,\n    y=train_labels,\n    epochs=10,\n    callbacks=[monitor]\n)\n</code></pre> Note <p>For tracing module calls along with training metrics, use <code>synalinks.enable_observability()</code> at the beggining of your script which configures the Monitor hook &amp; callback:</p> <pre><code>synalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"my_traces\"\n)\n</code></pre> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.Monitor\")\nclass Monitor(Callback):\n    \"\"\"Monitor callback for logging training metrics to MLflow.\n\n    This callback logs training progress and evaluation metrics to MLflow\n    for experiment tracking and visualization.\n\n    Args:\n        experiment_name (str): Name of the MLflow experiment. If None, uses\n            the program name.\n        run_name (str): Name of the MLflow run. If None, auto-generated.\n        tracking_uri (str): MLflow tracking server URI. If None, uses the\n            default (local ./mlruns directory or MLFLOW_TRACKING_URI env var).\n        log_batch_metrics (bool): Whether to log metrics at batch level\n            (default: False).\n        log_epoch_metrics (bool): Whether to log metrics at epoch level\n            (default: True).\n        log_program_plot (bool): Whether to log the program plot as an artifact\n            at the beginning of training (default: True).\n        log_program_model (bool): Whether to log the program as an MLflow model\n            at the end of training (default: True).\n        tags (dict): Optional tags to add to the MLflow run.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    # Basic usage - uses local MLflow storage\n    monitor = synalinks.callbacks.Monitor(experiment_name=\"my_experiment\")\n\n    # With custom MLflow tracking server\n    monitor = synalinks.callbacks.Monitor(\n        tracking_uri=\"http://localhost:5000\",\n        experiment_name=\"my_experiment\",\n        run_name=\"training_run_1\",\n        log_program_plot=True,\n        log_program_model=True,\n        tags={\"model_type\": \"chain_of_thought\"}\n    )\n\n    # Use in training\n    program.fit(\n        x=train_data,\n        y=train_labels,\n        epochs=10,\n        callbacks=[monitor]\n    )\n    ```\n\n    Note:\n        For tracing module calls along with training metrics, use\n        `synalinks.enable_observability()` at the beggining of your script\n        which configures the Monitor hook &amp; callback:\n\n        ```python\n        synalinks.enable_observability(\n            tracking_uri=\"http://localhost:5000\",\n            experiment_name=\"my_traces\"\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        experiment_name=None,\n        run_name=None,\n        tracking_uri=None,\n        log_batch_metrics=False,\n        log_epoch_metrics=True,\n        log_program_plot=True,\n        log_program_model=True,\n        tags=None,\n    ):\n        super().__init__()\n        if not MLFLOW_AVAILABLE:\n            raise ImportError(\n                \"mlflow is required for the Monitor callback. \"\n                \"Install it with: pip install mlflow\"\n            )\n\n        self.experiment_name = experiment_name\n        self.run_name = run_name\n        self.tracking_uri = tracking_uri\n        self.log_batch_metrics = log_batch_metrics\n        self.log_epoch_metrics = log_epoch_metrics\n        self.log_program_plot = log_program_plot\n        self.log_program_model = log_program_model\n        self.tags = tags or {}\n        self.logger = logging.getLogger(__name__)\n\n        self._run = None\n        self._step = 0\n        self._epoch = 0\n        # Track if we're inside fit() to avoid ending run during validation\n        self._in_training = False\n\n    def _setup_mlflow(self):\n        \"\"\"Configure MLflow tracking.\"\"\"\n        if self.tracking_uri:\n            mlflow.set_tracking_uri(self.tracking_uri)\n\n        experiment_name = self.experiment_name\n        if experiment_name is None and self.program is not None:\n            experiment_name = self.program.name or \"synalinks_experiment\"\n\n        mlflow.set_experiment(experiment_name)\n\n    def _start_run(self, run_name_suffix=\"\"):\n        \"\"\"Start a new MLflow run.\"\"\"\n        run_name = self.run_name\n        if run_name and run_name_suffix:\n            run_name = f\"{run_name}_{run_name_suffix}\"\n        elif run_name_suffix:\n            run_name = run_name_suffix\n\n        self._run = mlflow.start_run(run_name=run_name)\n\n        tags = dict(self.tags)\n        if self.program is not None:\n            if self.program.name:\n                tags[\"program_name\"] = self.program.name\n            if self.program.description:\n                tags[\"program_description\"] = self.program.description\n\n        if tags:\n            mlflow.set_tags(tags)\n\n        self._step = 0\n        self._epoch = 0\n\n    def _end_run(self):\n        \"\"\"End the current MLflow run.\"\"\"\n        if self._run is not None:\n            mlflow.end_run()\n            self._run = None\n\n    async def _log_metrics(self, logs, step=None):\n        \"\"\"Log metrics to MLflow asynchronously.\"\"\"\n        if logs is None or self._run is None:\n            return\n\n        metrics = {}\n        for key, value in logs.items():\n            if isinstance(value, (int, float)):\n                metrics[key] = value\n\n        if metrics:\n            await asyncio.to_thread(mlflow.log_metrics, metrics, step=step)\n\n    async def _upload_artifact_via_http(self, local_path, artifact_path, run_id):\n        \"\"\"Upload artifact via HTTP to MLflow server asynchronously.\n\n        This method uses the MLflow REST API to upload artifacts directly,\n        bypassing local filesystem artifact repo issues. Requires the MLflow\n        server to be started with --serve-artifacts flag.\n        \"\"\"\n        import requests\n\n        if not self.tracking_uri:\n            raise ValueError(\"tracking_uri is required for HTTP artifact upload\")\n\n        # Get the run's artifact URI to determine the correct upload path\n        client = mlflow.MlflowClient(tracking_uri=self.tracking_uri)\n        run = await asyncio.to_thread(client.get_run, run_id)\n        artifact_uri = run.info.artifact_uri\n\n        filename = os.path.basename(local_path)\n        if artifact_path:\n            full_artifact_path = f\"{artifact_path}/{filename}\"\n        else:\n            full_artifact_path = filename\n\n        # Parse the artifact URI to construct the correct upload URL\n        # artifact_uri can be:\n        #   - mlflow-artifacts:/&lt;experiment_id&gt;/&lt;run_id&gt;/artifacts\n        #   - mlflow-artifacts://host:port/&lt;experiment_id&gt;/&lt;run_id&gt;/artifacts\n        #   - /mlflow/artifacts/&lt;experiment_id&gt;/&lt;run_id&gt;/artifacts (server local path)\n        if artifact_uri.startswith(\"mlflow-artifacts:\"):\n            # Extract the path part after the scheme\n            uri_path = artifact_uri.replace(\"mlflow-artifacts://\", \"\").replace(\n                \"mlflow-artifacts:/\", \"\"\n            )\n            # Remove host:port if present (will use tracking_uri instead)\n            if \"/\" in uri_path and not uri_path.startswith(\"/\"):\n                parts = uri_path.split(\"/\", 1)\n                if \":\" in parts[0] or \".\" in parts[0]:\n                    # First part looks like host:port, skip it\n                    uri_path = parts[1] if len(parts) &gt; 1 else \"\"\n        elif artifact_uri.startswith(\"/\"):\n            # Server-side local path like /mlflow/artifacts/&lt;exp_id&gt;/&lt;run_id&gt;/artifacts\n            # Extract the relative path: &lt;exp_id&gt;/&lt;run_id&gt;/artifacts\n            # Find the pattern after the base artifacts directory\n            parts = artifact_uri.split(\"/\")\n            # Look for 'artifacts' in the path and take everything after the first one\n            try:\n                artifacts_idx = parts.index(\"artifacts\")\n                uri_path = \"/\".join(parts[artifacts_idx + 1 :])\n            except ValueError:\n                # Fallback: use experiment_id/run_id/artifacts pattern\n                uri_path = f\"0/{run_id}/artifacts\"\n        else:\n            # Fallback for other URI schemes\n            uri_path = f\"0/{run_id}/artifacts\"\n\n        # Construct the full upload URL\n        base = f\"{self.tracking_uri}/api/2.0/mlflow-artifacts/artifacts\"\n        url = f\"{base}/{uri_path}/{full_artifact_path}\"\n\n        with open(local_path, \"rb\") as f:\n            content = f.read()\n\n        # Determine content type based on file extension\n        content_type = \"application/octet-stream\"\n        if local_path.endswith(\".png\"):\n            content_type = \"image/png\"\n        elif local_path.endswith(\".json\"):\n            content_type = \"application/json\"\n\n        headers = {\"Content-Type\": content_type}\n        response = await asyncio.to_thread(\n            requests.put, url, data=content, headers=headers\n        )\n\n        if response.status_code not in (200, 201, 204):\n            raise Exception(\n                f\"Failed to upload artifact: {response.status_code} {response.text}\"\n            )\n\n    async def _log_program_plot_artifact(self):\n        \"\"\"Log the program plot as an MLflow artifact asynchronously.\"\"\"\n        if self._run is None:\n            self.logger.warning(\"No MLflow run active, skipping plot logging\")\n            return\n\n        if self.program is None:\n            self.logger.warning(\"No program set, skipping plot logging\")\n            return\n\n        if not self.program.built:\n            self.logger.warning(\"Program not built, skipping plot logging\")\n            return\n\n        try:\n            from synalinks.src.utils.program_visualization import check_graphviz\n            from synalinks.src.utils.program_visualization import check_pydot\n            from synalinks.src.utils.program_visualization import plot_program\n\n            if not check_pydot() or not check_graphviz():\n                self.logger.warning(\n                    \"pydot or graphviz not available, skipping program plot\"\n                )\n                return\n\n            run_id = self._run.info.run_id\n\n            with tempfile.TemporaryDirectory() as tmpdir:\n                plot_filename = f\"{self.program.name or 'program'}.png\"\n                plot_path = os.path.join(tmpdir, plot_filename)\n\n                # Run plot generation in thread pool\n                await asyncio.to_thread(\n                    plot_program,\n                    self.program,\n                    to_file=plot_filename,\n                    to_folder=tmpdir,\n                    show_schemas=True,\n                    show_module_names=True,\n                    show_trainable=True,\n                    dpi=96,  # Lower DPI for smaller file size\n                )\n\n                if os.path.exists(plot_path):\n                    # Use HTTP upload if tracking_uri is set (remote server),\n                    # otherwise fall back to direct artifact logging (local)\n                    if self.tracking_uri:\n                        await self._upload_artifact_via_http(\n                            plot_path, artifact_path=\"program_plots\", run_id=run_id\n                        )\n                    else:\n                        await asyncio.to_thread(\n                            mlflow.log_artifact,\n                            plot_path,\n                            artifact_path=\"program_plots\",\n                            run_id=run_id,\n                        )\n                    self.logger.info(f\"Logged program plot: {plot_filename}\")\n                else:\n                    self.logger.warning(f\"Plot file not created: {plot_path}\")\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to log program plot: {e}\")\n\n    async def _log_params(self):\n        \"\"\"Log training hyperparameters to MLflow asynchronously.\"\"\"\n        if self._run is None or self.params is None:\n            return\n\n        try:\n            params_to_log = {}\n            for key, value in self.params.items():\n                if isinstance(value, (str, int, float, bool)):\n                    params_to_log[key] = value\n\n            if params_to_log:\n                await asyncio.to_thread(mlflow.log_params, params_to_log)\n                self.logger.debug(f\"Logged params: {params_to_log}\")\n        except Exception as e:\n            self.logger.warning(f\"Failed to log params: {e}\")\n\n    async def _log_program_model(self):\n        \"\"\"Log the program trainable state as an MLflow artifact asynchronously.\n\n        This saves only the trainable variables (state), not the full\n        program architecture. This is useful for checkpointing the learned\n        parameters like few-shot examples, optimized prompts, etc.\n        \"\"\"\n        if self._run is None or self.program is None:\n            self.logger.warning(\"No run or program, skipping model logging\")\n            return\n\n        try:\n            import orjson\n\n            # Get the state tree (trainable, non-trainable, optimizer variables)\n            state_tree = self.program.get_state_tree()\n\n            # Create model info\n            model_info = {\n                \"program_name\": self.program.name or \"program\",\n                \"program_description\": self.program.description or \"\",\n                \"framework\": \"synalinks\",\n                \"num_trainable_variables\": len(self.program.trainable_variables),\n            }\n\n            run_id = self._run.info.run_id\n\n            # Write to temp files and log as artifacts\n            with tempfile.TemporaryDirectory() as tmpdir:\n                # Save state tree\n                state_path = os.path.join(tmpdir, \"state_tree.json\")\n                with open(state_path, \"wb\") as f:\n                    f.write(orjson.dumps(state_tree, option=orjson.OPT_INDENT_2))\n\n                # Save model info\n                info_path = os.path.join(tmpdir, \"model_info.json\")\n                with open(info_path, \"wb\") as f:\n                    f.write(orjson.dumps(model_info, option=orjson.OPT_INDENT_2))\n\n                # Upload artifacts\n                if self.tracking_uri:\n                    await self._upload_artifact_via_http(\n                        state_path, artifact_path=\"model\", run_id=run_id\n                    )\n                    await self._upload_artifact_via_http(\n                        info_path, artifact_path=\"model\", run_id=run_id\n                    )\n                else:\n                    await asyncio.to_thread(\n                        mlflow.log_artifact,\n                        state_path,\n                        artifact_path=\"model\",\n                        run_id=run_id,\n                    )\n                    await asyncio.to_thread(\n                        mlflow.log_artifact,\n                        info_path,\n                        artifact_path=\"model\",\n                        run_id=run_id,\n                    )\n\n            self.logger.info(\n                f\"Logged program state: {self.program.name} \"\n                f\"({len(self.program.trainable_variables)} trainable variables)\"\n            )\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to log program model: {e}\")\n\n    def on_train_begin(self, logs=None):\n        \"\"\"Called at the beginning of training.\"\"\"\n        self._in_training = True\n        self._setup_mlflow()\n        self._start_run(run_name_suffix=\"train\")\n        self.logger.debug(\"MLflow run started for training\")\n\n        # Log hyperparameters\n        run_maybe_nested(self._log_params())\n\n        # Log program plot\n        if self.log_program_plot:\n            run_maybe_nested(self._log_program_plot_artifact())\n\n    def on_train_end(self, logs=None):\n        \"\"\"Called at the end of training.\"\"\"\n        run_maybe_nested(self._log_metrics(logs, step=self._step))\n\n        # Log program as model at end of training\n        if self.log_program_model:\n            run_maybe_nested(self._log_program_model())\n\n        self._end_run()\n        self._in_training = False\n        self.logger.debug(\"MLflow run ended for training\")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        \"\"\"Called at the start of an epoch.\"\"\"\n        self._epoch = epoch\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of an epoch.\"\"\"\n        if not self.log_epoch_metrics:\n            return\n\n        self._epoch = epoch\n        run_maybe_nested(self._log_metrics(logs, step=epoch))\n        self.logger.debug(f\"Logged metrics for epoch {epoch}\")\n\n    def on_train_batch_begin(self, batch, logs=None):\n        \"\"\"Called at the beginning of a training batch.\"\"\"\n        pass\n\n    def on_train_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a training batch.\"\"\"\n        if not self.log_batch_metrics:\n            return\n\n        self._step += 1\n        run_maybe_nested(self._log_metrics(logs, step=self._step))\n\n    def on_test_begin(self, logs=None):\n        \"\"\"Called at the beginning of evaluation or validation.\"\"\"\n        # Only start a new run if we're not already in a training run\n        if self._run is None and not self._in_training:\n            self._setup_mlflow()\n            self._start_run(run_name_suffix=\"test\")\n            self.logger.debug(\"MLflow run started for testing\")\n\n    def on_test_end(self, logs=None):\n        \"\"\"Called at the end of evaluation or validation.\"\"\"\n        run_maybe_nested(self._log_metrics(logs, step=self._step))\n        # Only end the run if we're not in training (standalone evaluate() call)\n        if self._run is not None and not self._in_training:\n            self._end_run()\n            self.logger.debug(\"MLflow run ended for testing\")\n\n    def on_test_batch_begin(self, batch, logs=None):\n        \"\"\"Called at the beginning of a test batch.\"\"\"\n        pass\n\n    def on_test_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a test batch.\"\"\"\n        if not self.log_batch_metrics:\n            return\n\n        self._step += 1\n        run_maybe_nested(self._log_metrics(logs, step=self._step))\n\n    def on_predict_begin(self, logs=None):\n        \"\"\"Called at the beginning of prediction.\"\"\"\n        pass\n\n    def on_predict_end(self, logs=None):\n        \"\"\"Called at the end of prediction.\"\"\"\n        pass\n\n    def on_predict_batch_begin(self, batch, logs=None):\n        \"\"\"Called at the beginning of a prediction batch.\"\"\"\n        pass\n\n    def on_predict_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a prediction batch.\"\"\"\n        pass\n\n    def __del__(self):\n        \"\"\"Cleanup any open MLflow run.\"\"\"\n        if hasattr(self, \"_run\") and self._run is not None:\n            try:\n                mlflow.end_run()\n            except Exception:\n                pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup any open MLflow run.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup any open MLflow run.\"\"\"\n    if hasattr(self, \"_run\") and self._run is not None:\n        try:\n            mlflow.end_run()\n        except Exception:\n            pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_epoch_begin","title":"<code>on_epoch_begin(epoch, logs=None)</code>","text":"<p>Called at the start of an epoch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_epoch_begin(self, epoch, logs=None):\n    \"\"\"Called at the start of an epoch.\"\"\"\n    self._epoch = epoch\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_epoch_end","title":"<code>on_epoch_end(epoch, logs=None)</code>","text":"<p>Called at the end of an epoch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_epoch_end(self, epoch, logs=None):\n    \"\"\"Called at the end of an epoch.\"\"\"\n    if not self.log_epoch_metrics:\n        return\n\n    self._epoch = epoch\n    run_maybe_nested(self._log_metrics(logs, step=epoch))\n    self.logger.debug(f\"Logged metrics for epoch {epoch}\")\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_predict_batch_begin","title":"<code>on_predict_batch_begin(batch, logs=None)</code>","text":"<p>Called at the beginning of a prediction batch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_predict_batch_begin(self, batch, logs=None):\n    \"\"\"Called at the beginning of a prediction batch.\"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_predict_batch_end","title":"<code>on_predict_batch_end(batch, logs=None)</code>","text":"<p>Called at the end of a prediction batch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_predict_batch_end(self, batch, logs=None):\n    \"\"\"Called at the end of a prediction batch.\"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_predict_begin","title":"<code>on_predict_begin(logs=None)</code>","text":"<p>Called at the beginning of prediction.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_predict_begin(self, logs=None):\n    \"\"\"Called at the beginning of prediction.\"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_predict_end","title":"<code>on_predict_end(logs=None)</code>","text":"<p>Called at the end of prediction.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_predict_end(self, logs=None):\n    \"\"\"Called at the end of prediction.\"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_test_batch_begin","title":"<code>on_test_batch_begin(batch, logs=None)</code>","text":"<p>Called at the beginning of a test batch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_test_batch_begin(self, batch, logs=None):\n    \"\"\"Called at the beginning of a test batch.\"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_test_batch_end","title":"<code>on_test_batch_end(batch, logs=None)</code>","text":"<p>Called at the end of a test batch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_test_batch_end(self, batch, logs=None):\n    \"\"\"Called at the end of a test batch.\"\"\"\n    if not self.log_batch_metrics:\n        return\n\n    self._step += 1\n    run_maybe_nested(self._log_metrics(logs, step=self._step))\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_test_begin","title":"<code>on_test_begin(logs=None)</code>","text":"<p>Called at the beginning of evaluation or validation.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_test_begin(self, logs=None):\n    \"\"\"Called at the beginning of evaluation or validation.\"\"\"\n    # Only start a new run if we're not already in a training run\n    if self._run is None and not self._in_training:\n        self._setup_mlflow()\n        self._start_run(run_name_suffix=\"test\")\n        self.logger.debug(\"MLflow run started for testing\")\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_test_end","title":"<code>on_test_end(logs=None)</code>","text":"<p>Called at the end of evaluation or validation.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_test_end(self, logs=None):\n    \"\"\"Called at the end of evaluation or validation.\"\"\"\n    run_maybe_nested(self._log_metrics(logs, step=self._step))\n    # Only end the run if we're not in training (standalone evaluate() call)\n    if self._run is not None and not self._in_training:\n        self._end_run()\n        self.logger.debug(\"MLflow run ended for testing\")\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_train_batch_begin","title":"<code>on_train_batch_begin(batch, logs=None)</code>","text":"<p>Called at the beginning of a training batch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_train_batch_begin(self, batch, logs=None):\n    \"\"\"Called at the beginning of a training batch.\"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_train_batch_end","title":"<code>on_train_batch_end(batch, logs=None)</code>","text":"<p>Called at the end of a training batch.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_train_batch_end(self, batch, logs=None):\n    \"\"\"Called at the end of a training batch.\"\"\"\n    if not self.log_batch_metrics:\n        return\n\n    self._step += 1\n    run_maybe_nested(self._log_metrics(logs, step=self._step))\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_train_begin","title":"<code>on_train_begin(logs=None)</code>","text":"<p>Called at the beginning of training.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_train_begin(self, logs=None):\n    \"\"\"Called at the beginning of training.\"\"\"\n    self._in_training = True\n    self._setup_mlflow()\n    self._start_run(run_name_suffix=\"train\")\n    self.logger.debug(\"MLflow run started for training\")\n\n    # Log hyperparameters\n    run_maybe_nested(self._log_params())\n\n    # Log program plot\n    if self.log_program_plot:\n        run_maybe_nested(self._log_program_plot_artifact())\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/Monitor/#synalinks.src.callbacks.monitor.Monitor.on_train_end","title":"<code>on_train_end(logs=None)</code>","text":"<p>Called at the end of training.</p> Source code in <code>synalinks/src/callbacks/monitor.py</code> <pre><code>def on_train_end(self, logs=None):\n    \"\"\"Called at the end of training.\"\"\"\n    run_maybe_nested(self._log_metrics(logs, step=self._step))\n\n    # Log program as model at end of training\n    if self.log_program_model:\n        run_maybe_nested(self._log_program_model())\n\n    self._end_run()\n    self._in_training = False\n    self.logger.debug(\"MLflow run ended for training\")\n</code></pre>"},{"location":"Synalinks%20API/Callbacks%20API/ProgramCheckpoint/","title":"ProgramCheckpoint","text":""},{"location":"Synalinks%20API/Callbacks%20API/ProgramCheckpoint/#synalinks.src.callbacks.program_checkpoint.ProgramCheckpoint","title":"<code>ProgramCheckpoint</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to save the Synalinks program or program variables at some frequency.</p> <p><code>ProgramCheckpoint</code> callback is used in conjunction with training using <code>program.fit()</code> to save a program or variables (in a checkpoint file) at some interval, so the program or variables can be loaded later to continue the training from the state saved.</p> <p>A few options this callback provides include:</p> <ul> <li>Whether to only keep the program that has achieved the \"best performance\" so   far, or whether to save the program at the end of every epoch regardless of   performance.</li> <li>Definition of \"best\"; which quantity to monitor and whether it should be   maximized or minimized.</li> <li>The frequency it should save at. Currently, the callback supports saving   at the end of every epoch, or after a fixed number of training batches.</li> <li>Whether only variables are saved, or the whole program is saved.</li> </ul> <p>Example:</p> <pre><code>program.compile(\n    reward=...,\n    optimizer=...,\n    metrics=[\n        ...\n    ],\n)\n\nEPOCHS = 10\ncheckpoint_filepath = '/tmp/synalinks/checkpoint.program.json'\n\nprogram_checkpoint_callback = synalinks.callbacks.ProgramCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_reward',\n    mode='max',\n    save_best_only=True,\n)\n\n# Program is saved at the end of every epoch, if it's the best seen so far.\nprogram.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_test, y_test),\n    epochs=EPOCHS,\n    callbacks=[program_checkpoint_callback]\n)\n\n# The program (that are considered the best) can be loaded as -\nsynalinks.programs.load_program(checkpoint_filepath)\n\n# Alternatively, one could checkpoint just the program variables as -\ncheckpoint_filepath = '/tmp/synalinks/checkpoint.variables.json'\nprogram_checkpoint_callback = synalinks.callbacks.ProgramCheckpoint(\n    filepath=checkpoint_filepath,\n    save_variables_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n)\n\n# Program variables are saved at the end of every epoch, if it's the best seen\n# so far.\nprogram.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_test, y_test),\n    epochs=EPOCHS,\n    callbacks=[program_checkpoint_callback]\n)\n\n# The program variables (that are considered the best) can be loaded as -\nprogram.load_variables(checkpoint_filepath)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | PathLike</code> <p>string or <code>PathLike</code>, path to save the program file. <code>filepath</code> can contain named formatting options, which will be filled the value of <code>epoch</code> and keys in <code>logs</code> (passed in <code>on_epoch_end</code>). The <code>filepath</code> name needs to end with <code>\".variables.json\"</code> when <code>save_variables_only=True</code> or should end with <code>\".json\"</code> when checkpoint saving the whole program (default). For example, if <code>filepath</code> is <code>\"{epoch:02d}-{val_loss:.2f}.json\"</code> or \"{epoch:02d}-{val_loss:.2f}.variables.json\"`, then the program checkpoints will be saved with the epoch number and the validation loss in the filename. The directory of the filepath should not be reused by any other callbacks to avoid conflicts.</p> required <code>monitor</code> <code>str</code> <p>The metric name to monitor. Typically the metrics are set by the <code>Program.compile</code> method. Note: * Prefix the name with <code>\"val_\"</code> to monitor validation metrics. * Use <code>\"reward\"</code> or <code>\"val_reward\"</code> to monitor the program's total reward. * If you specify metrics as strings, like <code>\"accuracy\"</code>, pass the     same string (with or without the <code>\"val_\"</code> prefix). * If you pass <code>metrics.Metric</code> objects, <code>monitor</code> should be set to     <code>metric.name</code> * If you're not sure about the metric names you can check the     contents of the <code>history.history</code> dictionary returned by     <code>history = program.fit()</code> * Multi-output programs set additional prefixes on the metric names.</p> <code>'val_reward'</code> <code>verbose</code> <code>str | int</code> <p>Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays messages when the callback takes an action.</p> <code>0</code> <code>save_best_only</code> <code>bool</code> <p>if <code>save_best_only=True</code>, it only saves when the program is considered the \"best\" and the latest best program according to the quantity monitored will not be overwritten. If <code>filepath</code> doesn't contain formatting options like <code>{epoch}</code> then <code>filepath</code> will be overwritten by each new better program.</p> <code>False</code> <code>mode</code> <code>str</code> <p>one of {<code>\"auto\"</code>, <code>\"min\"</code>, <code>\"max\"</code>}. If <code>save_best_only=True</code>, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. In <code>\"auto\"</code> mode, the mode is set to <code>\"max\"</code>.</p> <code>'auto'</code> <code>save_variables_only</code> <code>bool</code> <p>if <code>True</code>, then only the program's variables will be saved (<code>program.save_variables(filepath)</code>), else the full program is saved (<code>program.save(filepath)</code>).</p> <code>False</code> <code>save_freq</code> <code>str | int</code> <p><code>\"epoch\"</code> or integer. When using <code>\"epoch\"</code>, the callback saves the program after each epoch. When using integer, the callback saves the program at end of this many batches. If the <code>Program</code> is compiled with <code>steps_per_execution=N</code>, then the saving criteria will be checked every Nth batch. Note that if the saving isn't aligned to epochs, the monitored metric may potentially be less reliable (it could reflect as little as 1 batch, since the metrics get reset every epoch). Defaults to <code>\"epoch\"</code>.</p> <code>'epoch'</code> <code>initial_value_threshold</code> <code>float</code> <p>Floating point initial \"best\" value of the metric to be monitored. Only applies if <code>save_best_value=True</code>. Only overwrites the program variables already saved if the performance of current program is better than this value.</p> <code>None</code> Source code in <code>synalinks/src/callbacks/program_checkpoint.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.ProgramCheckpoint\")\nclass ProgramCheckpoint(Callback):\n    \"\"\"Callback to save the Synalinks program or program variables at some frequency.\n\n    `ProgramCheckpoint` callback is used in conjunction with training using\n    `program.fit()` to save a program or variables (in a checkpoint file) at some\n    interval, so the program or variables can be loaded later to continue the\n    training from the state saved.\n\n    A few options this callback provides include:\n\n    - Whether to only keep the program that has achieved the \"best performance\" so\n      far, or whether to save the program at the end of every epoch regardless of\n      performance.\n    - Definition of \"best\"; which quantity to monitor and whether it should be\n      maximized or minimized.\n    - The frequency it should save at. Currently, the callback supports saving\n      at the end of every epoch, or after a fixed number of training batches.\n    - Whether only variables are saved, or the whole program is saved.\n\n    Example:\n\n    ```python\n    program.compile(\n        reward=...,\n        optimizer=...,\n        metrics=[\n            ...\n        ],\n    )\n\n    EPOCHS = 10\n    checkpoint_filepath = '/tmp/synalinks/checkpoint.program.json'\n\n    program_checkpoint_callback = synalinks.callbacks.ProgramCheckpoint(\n        filepath=checkpoint_filepath,\n        monitor='val_reward',\n        mode='max',\n        save_best_only=True,\n    )\n\n    # Program is saved at the end of every epoch, if it's the best seen so far.\n    program.fit(\n        x=x_train,\n        y=y_train,\n        validation_data=(x_test, y_test),\n        epochs=EPOCHS,\n        callbacks=[program_checkpoint_callback]\n    )\n\n    # The program (that are considered the best) can be loaded as -\n    synalinks.programs.load_program(checkpoint_filepath)\n\n    # Alternatively, one could checkpoint just the program variables as -\n    checkpoint_filepath = '/tmp/synalinks/checkpoint.variables.json'\n    program_checkpoint_callback = synalinks.callbacks.ProgramCheckpoint(\n        filepath=checkpoint_filepath,\n        save_variables_only=True,\n        monitor='val_accuracy',\n        mode='max',\n        save_best_only=True,\n    )\n\n    # Program variables are saved at the end of every epoch, if it's the best seen\n    # so far.\n    program.fit(\n        x=x_train,\n        y=y_train,\n        validation_data=(x_test, y_test),\n        epochs=EPOCHS,\n        callbacks=[program_checkpoint_callback]\n    )\n\n    # The program variables (that are considered the best) can be loaded as -\n    program.load_variables(checkpoint_filepath)\n    ```\n\n    Args:\n        filepath (str | os.PathLike): string or `PathLike`, path to save the program file.\n            `filepath` can contain named formatting options,\n            which will be filled the value of `epoch` and keys in `logs`\n            (passed in `on_epoch_end`).\n            The `filepath` name needs to end with `\".variables.json\"` when\n            `save_variables_only=True` or should end with `\".json\"`\n            when checkpoint saving the whole program (default).\n            For example, if `filepath` is `\"{epoch:02d}-{val_loss:.2f}.json\"` or\n            \"{epoch:02d}-{val_loss:.2f}.variables.json\"`, then the program\n            checkpoints will be saved with the epoch number and the validation\n            loss in the filename. The directory of the filepath\n            should not be reused by any other callbacks to avoid conflicts.\n        monitor (str): The metric name to monitor. Typically the metrics are set by\n            the `Program.compile` method. Note:\n            * Prefix the name with `\"val_\"` to monitor validation metrics.\n            * Use `\"reward\"` or `\"val_reward\"` to monitor the program's total reward.\n            * If you specify metrics as strings, like `\"accuracy\"`, pass the\n                same string (with or without the `\"val_\"` prefix).\n            * If you pass `metrics.Metric` objects, `monitor` should be set to\n                `metric.name`\n            * If you're not sure about the metric names you can check the\n                contents of the `history.history` dictionary returned by\n                `history = program.fit()`\n            * Multi-output programs set additional prefixes on the metric names.\n        verbose (str | int): Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n            displays messages when the callback takes an action.\n        save_best_only (bool): if `save_best_only=True`, it only saves when the program\n            is considered the \"best\" and the latest best program according to the\n            quantity monitored will not be overwritten. If `filepath` doesn't\n            contain formatting options like `{epoch}` then `filepath` will be\n            overwritten by each new better program.\n        mode (str): one of {`\"auto\"`, `\"min\"`, `\"max\"`}. If `save_best_only=True`, the\n            decision to overwrite the current save file is made based on either\n            the maximization or the minimization of the monitored quantity.\n            In `\"auto\"` mode, the mode is set to `\"max\"`.\n        save_variables_only (bool): if `True`, then only the program's variables will be\n            saved (`program.save_variables(filepath)`), else the full program is\n            saved (`program.save(filepath)`).\n        save_freq (str | int): `\"epoch\"` or integer. When using `\"epoch\"`, the callback\n            saves the program after each epoch. When using integer, the callback\n            saves the program at end of this many batches. If the `Program` is\n            compiled with `steps_per_execution=N`, then the saving criteria will\n            be checked every Nth batch. Note that if the saving isn't aligned to\n            epochs, the monitored metric may potentially be less reliable (it\n            could reflect as little as 1 batch, since the metrics get reset\n            every epoch). Defaults to `\"epoch\"`.\n        initial_value_threshold (float): Floating point initial \"best\" value of the\n            metric to be monitored. Only applies if `save_best_value=True`. Only\n            overwrites the program variables already saved if the performance of\n            current program is better than this value.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath,\n        monitor=\"val_reward\",\n        verbose=0,\n        save_best_only=False,\n        save_variables_only=False,\n        mode=\"auto\",\n        save_freq=\"epoch\",\n        initial_value_threshold=None,\n    ):\n        super().__init__()\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = file_utils.path_to_string(filepath)\n        self.save_best_only = save_best_only\n        self.save_variables_only = save_variables_only\n        self.save_freq = save_freq\n        self._batches_seen_since_last_saving = 0\n        self._last_batch_seen = 0\n        self.best = initial_value_threshold\n\n        if mode not in [\"auto\", \"min\", \"max\"]:\n            warnings.warn(\n                f\"ProgramCheckpoint mode '{mode}' is unknown, fallback to auto mode.\",\n                stacklevel=2,\n            )\n            mode = \"auto\"\n\n        if mode == \"min\":\n            self.monitor_op = np.less\n            if self.best is None:\n                self.best = np.inf\n        elif mode == \"max\":\n            self.monitor_op = np.greater\n            if self.best is None:\n                self.best = -np.inf\n        else:\n            self.monitor_op = np.greater\n            if self.best is None:\n                self.best = -np.inf\n\n        if self.save_freq != \"epoch\" and not isinstance(self.save_freq, int):\n            raise ValueError(\n                f\"Unrecognized save_freq: {self.save_freq}. \"\n                \"Expected save_freq are 'epoch' or integer values\"\n            )\n\n        if save_variables_only:\n            if not self.filepath.endswith(\".variables.json\"):\n                raise ValueError(\n                    \"When using `save_variables_only=True` in `ProgramCheckpoint`\"\n                    \", the filepath provided must end in `.variables.json` \"\n                    \"(Synalinks variables format). Received: \"\n                    f\"filepath={self.filepath}\"\n                )\n        else:\n            if not self.filepath.endswith(\".json\"):\n                raise ValueError(\n                    \"The filepath provided must end in `.json` \"\n                    \"(Synalinks program format). Received: \"\n                    f\"filepath={self.filepath}\"\n                )\n\n    def on_train_batch_end(self, batch, logs=None):\n        if self._should_save_on_batch(batch):\n            self._save_program(epoch=self._current_epoch, batch=batch, logs=logs)\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self._current_epoch = epoch\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.save_freq == \"epoch\":\n            self._save_program(epoch=epoch, batch=None, logs=logs)\n\n    def _should_save_on_batch(self, batch):\n        \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n        if self.save_freq == \"epoch\":\n            return False\n        if batch &lt;= self._last_batch_seen:  # New epoch.\n            add_batches = batch + 1  # batches are zero-indexed.\n        else:\n            add_batches = batch - self._last_batch_seen\n        self._batches_seen_since_last_saving += add_batches\n        self._last_batch_seen = batch\n\n        if self._batches_seen_since_last_saving &gt;= self.save_freq:\n            self._batches_seen_since_last_saving = 0\n            return True\n        return False\n\n    def _save_program(self, epoch, batch, logs):\n        \"\"\"Saves the program.\n\n        Args:\n            epoch (int): the epoch this iteration is in.\n            batch (int): the batch this iteration is in. `None` if the `save_freq`\n                is set to `\"epoch\"`.\n            logs (dict): the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n        \"\"\"\n        logs = logs or {}\n\n        filepath = self._get_file_path(epoch, batch, logs)\n        # Create host directory if it doesn't exist.\n        dirname = os.path.dirname(filepath)\n        if dirname and not file_utils.exists(dirname):\n            file_utils.makedirs(dirname)\n\n        try:\n            if self.save_best_only:\n                current = logs.get(self.monitor)\n                if current is None:\n                    warnings.warn(\n                        f\"Can save best model only with {self.monitor} \"\n                        \"available, skipping.\",\n                        stacklevel=2,\n                    )\n                elif isinstance(current, np.ndarray) and len(current.shape) &gt; 0:\n                    warnings.warn(\n                        \"Can save best model only when `monitor` is \"\n                        f\"a scalar value. Received: {current}. \"\n                        \"Falling back to `save_best_only=False`.\"\n                    )\n                    self.program.save(filepath, overwrite=True)\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose &gt; 0:\n                            io_utils.print_msg(\n                                f\"\\nEpoch {epoch + 1}: {self.monitor} \"\n                                \"improved \"\n                                f\"from {self.best:.5f} to {current:.5f}, \"\n                                f\"saving program to {filepath}\"\n                            )\n                        self.best = current\n                        if self.save_variables_only:\n                            self.program.save_variables(filepath, overwrite=True)\n                        else:\n                            self.program.save(filepath, overwrite=True)\n                    else:\n                        if self.verbose &gt; 0:\n                            io_utils.print_msg(\n                                f\"\\nEpoch {epoch + 1}: \"\n                                f\"{self.monitor} did not improve \"\n                                f\"from {self.best:.5f}\"\n                            )\n            else:\n                if self.verbose &gt; 0:\n                    io_utils.print_msg(f\"\\nEpoch {epoch + 1}: saving model to {filepath}\")\n                if self.save_variables_only:\n                    self.program.save_variables(filepath, overwrite=True)\n                else:\n                    self.program.save(filepath, overwrite=True)\n        except IsADirectoryError:  # h5py 3.x\n            raise IOError(\n                \"Please specify a non-directory filepath for \"\n                \"ProgramCheckpoint. Filepath used is an existing \"\n                f\"directory: {filepath}\"\n            )\n        except IOError as e:  # h5py 2.x\n            # `e.errno` appears to be `None` so checking the content of\n            # `e.args[0]`.\n            if \"is a directory\" in str(e.args[0]).lower():\n                raise IOError(\n                    \"Please specify a non-directory filepath for \"\n                    \"ModelCheckpoint. Filepath used is an existing \"\n                    f\"directory: {filepath}\"\n                )\n            # Re-throw the error for any other causes.\n            raise e\n\n    def _get_file_path(self, epoch, batch, logs):\n        \"\"\"Returns the file path for checkpoint.\"\"\"\n\n        try:\n            # `filepath` may contain placeholders such as\n            # `{epoch:02d}`,`{batch:02d}` and `{mape:.2f}`. A mismatch between\n            # logged metrics and the path's placeholders can cause formatting to\n            # fail.\n            if batch is None or \"batch\" in logs:\n                file_path = self.filepath.format(epoch=epoch + 1, **logs)\n            else:\n                file_path = self.filepath.format(epoch=epoch + 1, batch=batch + 1, **logs)\n        except KeyError as e:\n            raise KeyError(\n                f'Failed to format this callback filepath: \"{self.filepath}\". Reason: {e}'\n            )\n        return file_path\n\n    def _checkpoint_exists(self, filepath):\n        \"\"\"Returns whether the checkpoint `filepath` refers to exists.\"\"\"\n        return file_utils.exists(filepath)\n\n    def _get_most_recently_modified_file_matching_pattern(self, pattern):\n        \"\"\"Returns the most recently modified filepath matching pattern.\n\n        In the rare case where there are more than one pattern-matching file\n        having the same modified time that is most recent among all, return the\n        filepath that is largest (by `&gt;` operator, lexicographically using the\n        numeric equivalents). This provides a tie-breaker when multiple files\n        are most recent. Note that a larger `filepath` can sometimes indicate a\n        later time of modification (for instance, when epoch/batch is used as\n        formatting option), but not necessarily (when accuracy or loss is used).\n        The tie-breaker is put in the logic as best effort to return the most\n        recent, and to avoid nondeterministic result.\n\n        Modified time of a file is obtained with `os.path.getmtime()`.\n\n        This utility function is best demonstrated via an example:\n\n        ```python\n        file_pattern = 'batch{batch:02d}epoch{epoch:02d}.json'\n        test_dir = self.get_temp_dir()\n        path_pattern = os.path.join(test_dir, file_pattern)\n        file_paths = [\n            os.path.join(test_dir, file_name) for file_name in\n            ['batch03epoch02.json',\n             'batch02epoch02.json', 'batch01epoch01.json']\n        ]\n        for file_path in file_paths:\n            # Write something to each of the files\n            ...\n        self.assertEqual(\n            _get_most_recently_modified_file_matching_pattern(path_pattern),\n            file_paths[-1])\n        ```\n\n        Args:\n            pattern (str): The file pattern that may optionally contain python\n                placeholder such as `{epoch:02d}`.\n\n        Returns:\n            (str): The most recently modified file's full filepath matching `pattern`.\n                If `pattern` does not contain any placeholder, this returns the\n                filepath that exactly matches `pattern`. Returns `None` if no match\n                is found.\n        \"\"\"\n        dir_name = os.path.dirname(pattern)\n        base_name = os.path.basename(pattern)\n        base_name_regex = \"^\" + re.sub(r\"{.*}\", r\".*\", base_name) + \"$\"\n\n        latest_mod_time = 0\n        file_path_with_latest_mod_time = None\n        n_file_with_latest_mod_time = 0\n        file_path_with_largest_file_name = None\n\n        if file_utils.exists(dir_name):\n            for file_name in os.listdir(dir_name):\n                # Only consider if `file_name` matches the pattern.\n                if re.match(base_name_regex, file_name):\n                    file_path = os.path.join(dir_name, file_name)\n                    mod_time = os.path.getmtime(file_path)\n                    if (\n                        file_path_with_largest_file_name is None\n                        or file_path &gt; file_path_with_largest_file_name\n                    ):\n                        file_path_with_largest_file_name = file_path\n                    if mod_time &gt; latest_mod_time:\n                        latest_mod_time = mod_time\n                        file_path_with_latest_mod_time = file_path\n                        # In the case a file with later modified time is found,\n                        # reset the counter for the number of files with latest\n                        # modified time.\n                        n_file_with_latest_mod_time = 1\n                    elif mod_time == latest_mod_time:\n                        # In the case a file has modified time tied with the\n                        # most recent, increment the counter for the number of\n                        # files with latest modified time by 1.\n                        n_file_with_latest_mod_time += 1\n\n        if n_file_with_latest_mod_time == 1:\n            # Return the sole file that has most recent modified time.\n            return file_path_with_latest_mod_time\n        else:\n            # If there are more than one file having latest modified time,\n            # return the file path with the largest file name.\n            return file_path_with_largest_file_name\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/","title":"Data Models API","text":""},{"location":"Synalinks%20API/Data%20Models%20API/#data-models-api","title":"Data Models API","text":"<p>Synalinks features four distinct types of data models, each serving a unique purpose within the framework:</p> <ul> <li> <p>DataModel: This is the backend-dependent data model, built on Pydantic's <code>BaseModel</code>. It is the primary model most users will interact with. It allows for schema and variable declarations, and is used to format datasets. When entering a workflow, this data model is automatically converted into a backend-independent format.</p> </li> <li> <p>JsonDataModel: This is the backend-independent data model that flows through the pipelines. It holds both a JSON schema and a JSON value, enabling it to perform computations. Unlike the backend-dependent model, this one is dynamically created and modified.</p> </li> <li> <p>SymbolicDataModel: This is the symbolic data model used during the functional API declaration to infer the pipeline's edges and nodes. It only holds a JSON schema, allowing the system to compute the pipeline from inputs and outputs without performing actual computations.</p> </li> <li> <p>Variable: This data model holds the module's state and can be updated during training. It includes a JSON schema and JSON value, enabling computations, and also contains metadata about training. <code>Optimizer</code>s can update it during the training process.</p> </li> </ul>"},{"location":"Synalinks%20API/Data%20Models%20API/#data-models-api-overview","title":"Data Models API Overview","text":"<ul> <li>The DataModel Class: The backend-dependent data models.</li> <li>The JsonDataModel Class: The backend-independent data models.</li> <li>The SymbolicDataModel Class: The symbolic data models.</li> <li>The Variable Class: The variable data models that the optimizers can act upon.</li> <li>The Base DataModels: A collection of basic backend-dependent data models.</li> </ul>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/","title":"The Base DataModels","text":"<p>We provide different backend-dependent <code>DataModel</code>s to use.</p> <p>These data models provide I/O for chatbots, agents, rags etc.</p> <p>The user can build new data models by inheriting from these base models.</p> <p>The check functions works for every type of data models (by checking the schema) e.g. <code>SymbolicDataModel</code>, <code>JsonDataModel</code>, <code>DataModel</code> or <code>Variable</code>.</p>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.ChatMessage","title":"<code>ChatMessage</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A chat message</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.ChatMessage\",\n        \"synalinks.ChatMessage\",\n    ]\n)\nclass ChatMessage(DataModel):\n    \"\"\"A chat message\"\"\"\n\n    role: ChatRole = Field(\n        description=\"The chat message role\",\n    )\n    content: Union[str, Dict[str, Any]] = Field(\n        description=\"The content of the message\",\n        default=\"\",\n    )\n    tool_call_id: Optional[str] = Field(\n        description=\"The id of the tool call if role is `tool`\",\n        default=None,\n    )\n    tool_calls: List[ToolCall] = Field(\n        description=\"The tool calls of the agent\",\n        default=[],\n    )\n    created_at: Optional[datetime] = Field(\n        description=\"The creation time\",\n        default=None,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.ChatMessages","title":"<code>ChatMessages</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A list of chat messages</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.ChatMessages\",\n        \"synalinks.ChatMessages\",\n    ]\n)\nclass ChatMessages(DataModel):\n    \"\"\"A list of chat messages\"\"\"\n\n    messages: List[ChatMessage] = Field(\n        description=\"The list of chat messages\",\n        default=[],\n    )\n\n    @field_validator(\"messages\", mode=\"before\")\n    @classmethod\n    def convert_dicts_to_chat_messages(cls, v):\n        \"\"\"Convert dict messages to ChatMessage objects.\"\"\"\n        if isinstance(v, list):\n            return [ChatMessage(**msg) if isinstance(msg, dict) else msg for msg in v]\n        return v\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.ChatMessages.convert_dicts_to_chat_messages","title":"<code>convert_dicts_to_chat_messages(v)</code>  <code>classmethod</code>","text":"<p>Convert dict messages to ChatMessage objects.</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@field_validator(\"messages\", mode=\"before\")\n@classmethod\ndef convert_dicts_to_chat_messages(cls, v):\n    \"\"\"Convert dict messages to ChatMessage objects.\"\"\"\n    if isinstance(v, list):\n        return [ChatMessage(**msg) if isinstance(msg, dict) else msg for msg in v]\n    return v\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.ChatRole","title":"<code>ChatRole</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The chat message roles</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.ChatRole\",\n        \"synalinks.ChatRole\",\n    ]\n)\nclass ChatRole(str, Enum):\n    \"\"\"The chat message roles\"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>An embedding vector</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.Embedding\",\n    ]\n)\nclass Embedding(DataModel):\n    \"\"\"An embedding vector\"\"\"\n\n    embedding: List[float] = Field(\n        description=\"The embedding vector\",\n        default=[],\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.Embeddings","title":"<code>Embeddings</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A list of embeddings</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.Embeddings\",\n        \"synalinks.Embeddings\",\n    ]\n)\nclass Embeddings(DataModel):\n    \"\"\"A list of embeddings\"\"\"\n\n    embeddings: List[List[float]] = Field(\n        description=\"The list of embedding vectors\",\n        default=[],\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.GenericIO","title":"<code>GenericIO</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A pair of generic inputs/outputs</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.GenericIO\",\n        \"synalinks.GenericIO\",\n    ]\n)\nclass GenericIO(DataModel):\n    \"\"\"A pair of generic inputs/outputs\"\"\"\n\n    inputs: Dict[str, Any] = Field(\n        description=\"The inputs\",\n    )\n    outputs: Dict[str, Any] = Field(\n        description=\"The outputs\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.GenericInputs","title":"<code>GenericInputs</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A generic inputs</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.GenericInputs\",\n        \"synalinks.GenericInputs\",\n    ]\n)\nclass GenericInputs(DataModel):\n    \"\"\"A generic inputs\"\"\"\n\n    inputs: Dict[str, Any] = Field(\n        description=\"The inputs\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.GenericOutputs","title":"<code>GenericOutputs</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A generic outputs</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.GenericOutputs\",\n        \"synalinks.GenericOutputs\",\n    ]\n)\nclass GenericOutputs(DataModel):\n    \"\"\"A generic outputs\"\"\"\n\n    outputs: Dict[str, Any] = Field(\n        description=\"The outputs\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.GenericResult","title":"<code>GenericResult</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A generic result</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.GenericResult\",\n        \"synalinks.GenericResult\",\n    ]\n)\nclass GenericResult(DataModel):\n    \"\"\"A generic result\"\"\"\n\n    result: List[Any] = Field(\n        description=\"The result\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.Instructions","title":"<code>Instructions</code>","text":"<p>               Bases: <code>Trainable</code></p> <p>The instructions for the language model</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.Instructions\",\n        \"synalinks.Instructions\",\n    ]\n)\nclass Instructions(Trainable):\n    \"\"\"The instructions for the language model\"\"\"\n\n    instructions: Optional[str] = Field(\n        description=\"The instructions for the language model\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_chat_message","title":"<code>is_chat_message(x)</code>","text":"<p>Checks if the given data model is a chat message</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_chat_message\",\n        \"synalinks.is_chat_message\",\n    ]\n)\ndef is_chat_message(x):\n    \"\"\"Checks if the given data model is a chat message\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), ChatMessage.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_chat_messages","title":"<code>is_chat_messages(x)</code>","text":"<p>Checks if the given data model are chat messages</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_chat_messages\",\n        \"synalinks.is_chat_messages\",\n    ]\n)\ndef is_chat_messages(x):\n    \"\"\"Checks if the given data model are chat messages\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), ChatMessages.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_embedded","title":"<code>is_embedded(x)</code>","text":"<p>Checks if the given data model is an embedded entity</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_embedded\",\n        \"synalinks.is_embedded\",\n    ]\n)\ndef is_embedded(x):\n    \"\"\"Checks if the given data model is an embedded entity\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    schema = x.get_schema()\n    properties = schema.get(\"properties\", None)\n    if properties:\n        if properties.get(\"embedding\", None):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_embedding","title":"<code>is_embedding(x)</code>","text":"<p>Checks if the given data model is an embedding</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_embedding\",\n        \"synalinks.is_embedding\",\n    ]\n)\ndef is_embedding(x):\n    \"\"\"Checks if the given data model is an embedding\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), Embedding.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_embeddings","title":"<code>is_embeddings(x)</code>","text":"<p>Checks if the given data model are embeddings</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_embeddings\",\n        \"synalinks.is_embeddings\",\n    ]\n)\ndef is_embeddings(x):\n    \"\"\"Checks if the given data model are embeddings\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), Embeddings.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_instructions","title":"<code>is_instructions(x)</code>","text":"<p>Checks if the given data model is an instructions data model</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_instructions\",\n        \"synalinks.is_instructions\",\n    ]\n)\ndef is_instructions(x):\n    \"\"\"Checks if the given data model is an instructions data model\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), Instructions.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_prediction","title":"<code>is_prediction(x)</code>","text":"<p>Checks if the given data model is a prediction</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_prediction\",\n        \"synalinks.is_prediction\",\n    ]\n)\ndef is_prediction(x):\n    \"\"\"Checks if the given data model is a prediction\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), Prediction.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_similarity_search","title":"<code>is_similarity_search(x)</code>","text":"<p>Checks if is a similarity search data model</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_similarity_search\",\n        \"synalinks.is_similarity_search\",\n    ]\n)\ndef is_similarity_search(x):\n    \"\"\"Checks if is a similarity search data model\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    schema = x.get_schema()\n    properties = schema.get(\"properties\", None)\n    if properties:\n        if properties.get(\"entity_label\", None) and properties.get(\n            \"similarity_search\", None\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_stamped","title":"<code>is_stamped(x)</code>","text":"<p>Checks if the given data model is stamped</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_stamped\",\n        \"synalinks.is_stamped\",\n    ]\n)\ndef is_stamped(x):\n    \"\"\"Checks if the given data model is stamped\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    schema = x.get_schema()\n    properties = schema.get(\"properties\", None)\n    if properties:\n        if properties.get(\"created_at\", None):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_tool_call","title":"<code>is_tool_call(x)</code>","text":"<p>Checks if the given data model is a tool call</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_tool_call\",\n        \"synalinks.is_tool_call\",\n    ]\n)\ndef is_tool_call(x):\n    \"\"\"Checks if the given data model is a tool call\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), ToolCall.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_trainable","title":"<code>is_trainable(x)</code>","text":"<p>Checks if the given data model is Trainable</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_trainable\",\n        \"synalinks.is_trainable\",\n    ]\n)\ndef is_trainable(x):\n    \"\"\"Checks if the given data model is Trainable\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    if contains_schema(x.get_schema(), Trainable.get_schema()):\n        return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Base%20DataModels/#synalinks.src.backend.pydantic.base.is_triplet_search","title":"<code>is_triplet_search(x)</code>","text":"<p>Checks if is a triplet seach data model</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataModel | JsonDataModel | SymbolicDataModel | Variable</code> <p>The data model to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the condition is met</p> Source code in <code>synalinks/src/backend/pydantic/base.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.backend.is_triplet_search\",\n        \"synalinks.is_triplet_search\",\n    ]\n)\ndef is_triplet_search(x):\n    \"\"\"Checks if is a triplet seach data model\n\n    Args:\n        x (DataModel | JsonDataModel | SymbolicDataModel | Variable):\n            The data model to check.\n\n    Returns:\n        (bool): True if the condition is met\n    \"\"\"\n    schema = x.get_schema()\n    properties = schema.get(\"properties\", None)\n    if properties:\n        if (\n            properties.get(\"subject_label\", None)\n            and properties.get(\"subject_similarity_search\", None)\n            and properties.get(\"relation_label\", None)\n            and properties.get(\"object_label\", None)\n            and properties.get(\"object_similarity_search\", None)\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/","title":"The DataModel class","text":""},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel","title":"<code>DataModel</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>SynalinksSaveable</code></p> <p>The backend-dependent data model.</p> <p>This data model uses Pydantic to provide, JSON schema inference and JSON serialization.</p> <p>Examples:</p> <p>Creating a DataModel for structured output</p> <pre><code>class AnswerWithReflection(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    reflection: str = synalinks.Field(\n        description=\"The reflection about your thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n\nlanguage_model = synalinks.LanguageModel(\"ollama/mistral\")\n\ngenerator = synalinks.Generator(\n    data_model=AnswerWithReflection,\n    language_model=language_model,\n)\n</code></pre> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>class DataModel(pydantic.BaseModel, SynalinksSaveable, metaclass=MetaDataModel):\n    \"\"\"The backend-dependent data model.\n\n    This data model uses Pydantic to provide, JSON schema inference\n    and JSON serialization.\n\n    Examples:\n\n    **Creating a DataModel for structured output**\n\n    ```python\n    class AnswerWithReflection(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking\",\n        )\n        reflection: str = synalinks.Field(\n            description=\"The reflection about your thinking\",\n        )\n        answer: str = synalinks.Field(\n            description=\"The correct answer\",\n        )\n\n    language_model = synalinks.LanguageModel(\"ollama/mistral\")\n\n    generator = synalinks.Generator(\n        data_model=AnswerWithReflection,\n        language_model=language_model,\n    )\n    ```\n    \"\"\"\n\n    model_config: ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra=\"forbid\")\n\n    @classmethod\n    def get_schema(cls):\n        \"\"\"Gets the JSON schema of the data model.\n\n        Returns:\n            (dict): The JSON schema.\n        \"\"\"\n        return cls.model_json_schema()\n\n    @classmethod\n    def keys(cls):\n        \"\"\"Gets the JSON properties keys of the data model.\n\n        Returns:\n            (dict): The JSON schema.\n        \"\"\"\n        schema = cls.get_schema()\n        if \"properties\" in schema:\n            return schema[\"properties\"].keys()\n        else:\n            return []\n\n    @classmethod\n    def prettify_schema(cls):\n        \"\"\"Get a pretty version of the JSON schema for display.\n\n        Returns:\n            (str): The indented JSON schema.\n        \"\"\"\n        import orjson\n\n        return orjson.dumps(cls.get_schema(), option=orjson.OPT_INDENT_2).decode()\n\n    @classmethod\n    def to_symbolic_data_model(cls, name=None):\n        \"\"\"Converts the data model to a symbolic data model.\n\n        Args:\n            name (str): Optional. The name of the symbolic data model.\n                If None, a name will be given automatically.\n\n        Returns:\n            (SymbolicDataModel): The symbolic data model.\n        \"\"\"\n        return SymbolicDataModel(schema=cls.get_schema(), name=name)\n\n    def get_json(self):\n        \"\"\"Gets the JSON value of the data model.\n\n        Returns:\n            (dict): The JSON value.\n        \"\"\"\n        return self.model_dump(mode=\"json\")\n\n    def prettify_json(self):\n        \"\"\"Get a pretty version of the JSON object for display.\n\n        Returns:\n            (str): The indented JSON object.\n        \"\"\"\n        import orjson\n\n        return orjson.dumps(self.get_json(), option=orjson.OPT_INDENT_2).decode()\n\n    def __repr__(self):\n        return f\"&lt;DataModel json={self.get_json()}, schema={self.get_schema()}&gt;\"\n\n    def get(self, key, default=None):\n        \"\"\"Get wrapper to make it easier to access JSON fields.\n\n        Args:\n            key (str): The key to access.\n            default (any): The default value if key not found.\n        \"\"\"\n        try:\n            return self.__getattribute__(key)\n        except Exception:\n            return default\n\n    def __getitem__(self, key):\n        \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n        Args:\n            key (str): The key to access.\n        \"\"\"\n        return self.__getattribute__(key)\n\n    def to_json_data_model(self, name=None):\n        \"\"\"Converts the data model to a backend-independent data model.\n\n        Args:\n            name (str): Optional. The name of the json data model.\n                If None, a name will be given automatically.\n\n        Returns:\n            (JsonDataModel): The backend-independent data model.\n        \"\"\"\n        return JsonDataModel(\n            schema=self.get_schema(),\n            json=self.get_json(),\n            name=name,\n        )\n\n    def __add__(self, other):\n        \"\"\"Concatenates this data model with another.\n\n        Args:\n            other (JsonDataModel | DataModel | SymbolicDataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | SymbolicDataModel): The concatenated data model.\n                If one of them is a metaclass or symbolic data model,\n                then output a `SymbolicDataModel`.\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(self, other):\n            return run_maybe_nested(ops.Concat().symbolic_call(self, other))\n        else:\n            return run_maybe_nested(ops.Concat()(self, other))\n\n    def __radd__(self, other):\n        \"\"\"Concatenates another data model with this one.\n\n        Args:\n            other (JsonDataModel | DataModel | SymbolicDataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | SymbolicDataModel): The concatenated data model.\n                If one of them is a metaclass or symbolic data model,\n                then output a `SymbolicDataModel`.\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(self, other):\n            return run_maybe_nested(\n                ops.Concat().symbolic_call(other, self),\n            )\n        else:\n            return run_maybe_nested(\n                ops.Concat()(other, self),\n            )\n\n    def __and__(self, other):\n        \"\"\"Perform a `logical_and` with another data model.\n\n        If one of them is None, output None. If both are provided,\n        then concatenates the other data model with this one.\n\n        If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n        Args:\n            other (JsonDataModel | SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | SymbolicDataModel | None): The concatenated data model or\n                `None` based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(self, other):\n            return run_maybe_nested(\n                ops.And().symbolic_call(self, other),\n            )\n        else:\n            return run_maybe_nested(\n                ops.And()(self, other),\n            )\n\n    def __rand__(self, other):\n        \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n        If one of them is None, output None. If both are provided,\n        then concatenates the other data model with this one.\n\n        If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n        Args:\n            other (JsonDataModel | SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | SymbolicDataModel | None): The concatenated data model or\n                `None` based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(other, self):\n            return run_maybe_nested(\n                ops.And().symbolic_call(other, self),\n            )\n        else:\n            return run_maybe_nested(\n                ops.And()(other, self),\n            )\n\n    def __or__(self, other):\n        \"\"\"Perform a `logical_or` with another data model\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates this data model with the other.\n\n        If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n        Args:\n            other (JsonDataModel | SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | SymbolicDataModel | None): The concatenation of data model\n                if both are provided, or the non-None data model or None if none are\n                provided. (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(self, other):\n            return run_maybe_nested(\n                ops.Or().symbolic_call(self, other),\n            )\n        else:\n            return run_maybe_nested(\n                ops.Or()(self, other),\n            )\n\n    def __ror__(self, other):\n        \"\"\"Perform a `logical_or` (reverse) with another data model\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n        Args:\n            other (JsonDataModel | SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | SymbolicDataModel | None): The concatenation of data model\n                if both are provided, or the non-None data model or None if none are\n                provided. (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(other, self):\n            return run_maybe_nested(\n                ops.Or().symbolic_call(other, self),\n            )\n        else:\n            return run_maybe_nested(\n                ops.Or()(other, self),\n            )\n\n    def __xor__(self, other):\n        \"\"\"Perform a `logical_xor` with another data model.\n\n        If one of them is None, output the other one. If both are provided,\n        then output None.\n\n        Args:\n            other (SymbolicDataModel): The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(self, other):\n            return run_maybe_nested(\n                ops.Xor().symbolic_call(self, other),\n            )\n        else:\n            return run_maybe_nested(\n                ops.Xor()(self, other),\n            )\n\n    def __rxor__(self, other):\n        \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n        If one of them is None, output the other one. If both are provided,\n        then output None.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(other, self):\n            return run_maybe_nested(\n                ops.Xor().symbolic_call(other, self),\n            )\n        else:\n            return run_maybe_nested(\n                ops.Xor()(other, self),\n            )\n\n    def __invert__(self):\n        \"\"\"Perform an invertion/negation\n\n        When an input is provided, invert it by outputing `None`\n\n        Returns:\n            (SymbolicDataModel | None): `None` if used with an instance/class,\n                and a symbolic data model if used on a metaclass or symbolic model.\n        \"\"\"\n        from synalinks.src import ops\n\n        if any_meta_class(self):\n            return run_maybe_nested(\n                ops.Not().symbolic_call(self),\n            )\n        else:\n            return run_maybe_nested(\n                ops.Not()(self),\n            )\n\n    def __contains__(cls, other):\n        \"\"\"Check if the schema of `other` is contained in this one,\n        or if a string key exists.\n\n        Args:\n            other (SymbolicDataModel | DataModel | str): The other data model to compare\n                with, or a string key to check for in the schema properties.\n\n        Returns:\n            (bool): True if all properties of `other` are present in this one,\n                or if the string key exists in the schema properties.\n        \"\"\"\n        if isinstance(other, str):\n            schema = cls.get_schema()\n            return other in schema.get(\"properties\", {})\n        from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n        return contains_schema(cls.get_schema(), other.get_schema())\n\n    def get_config(self):\n        return self.get_json()\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenates this data model with another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel | SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The concatenated data model. If one of them is a metaclass or symbolic data model, then output a <code>SymbolicDataModel</code>.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __add__(self, other):\n    \"\"\"Concatenates this data model with another.\n\n    Args:\n        other (JsonDataModel | DataModel | SymbolicDataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The concatenated data model.\n            If one of them is a metaclass or symbolic data model,\n            then output a `SymbolicDataModel`.\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(self, other):\n        return run_maybe_nested(ops.Concat().symbolic_call(self, other))\n    else:\n        return run_maybe_nested(ops.Concat()(self, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__and__","title":"<code>__and__(other)</code>","text":"<p>Perform a <code>logical_and</code> with another data model.</p> <p>If one of them is None, output None. If both are provided, then concatenates the other data model with this one.</p> <p>If the other is a metaclass or symbolic data model, output a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The concatenated data model or <code>None</code> based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __and__(self, other):\n    \"\"\"Perform a `logical_and` with another data model.\n\n    If one of them is None, output None. If both are provided,\n    then concatenates the other data model with this one.\n\n    If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n    Args:\n        other (JsonDataModel | SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The concatenated data model or\n            `None` based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(self, other):\n        return run_maybe_nested(\n            ops.And().symbolic_call(self, other),\n        )\n    else:\n        return run_maybe_nested(\n            ops.And()(self, other),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__contains__","title":"<code>__contains__(other)</code>","text":"<p>Check if the schema of <code>other</code> is contained in this one, or if a string key exists.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel | str</code> <p>The other data model to compare with, or a string key to check for in the schema properties.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all properties of <code>other</code> are present in this one, or if the string key exists in the schema properties.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __contains__(cls, other):\n    \"\"\"Check if the schema of `other` is contained in this one,\n    or if a string key exists.\n\n    Args:\n        other (SymbolicDataModel | DataModel | str): The other data model to compare\n            with, or a string key to check for in the schema properties.\n\n    Returns:\n        (bool): True if all properties of `other` are present in this one,\n            or if the string key exists in the schema properties.\n    \"\"\"\n    if isinstance(other, str):\n        schema = cls.get_schema()\n        return other in schema.get(\"properties\", {})\n    from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n    return contains_schema(cls.get_schema(), other.get_schema())\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get item wrapper to make it easier to access JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n    Args:\n        key (str): The key to access.\n    \"\"\"\n    return self.__getattribute__(key)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__invert__","title":"<code>__invert__()</code>","text":"<p>Perform an invertion/negation</p> <p>When an input is provided, invert it by outputing <code>None</code></p> <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if used with an instance/class, and a symbolic data model if used on a metaclass or symbolic model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __invert__(self):\n    \"\"\"Perform an invertion/negation\n\n    When an input is provided, invert it by outputing `None`\n\n    Returns:\n        (SymbolicDataModel | None): `None` if used with an instance/class,\n            and a symbolic data model if used on a metaclass or symbolic model.\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(self):\n        return run_maybe_nested(\n            ops.Not().symbolic_call(self),\n        )\n    else:\n        return run_maybe_nested(\n            ops.Not()(self),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__or__","title":"<code>__or__(other)</code>","text":"<p>Perform a <code>logical_or</code> with another data model</p> <p>If one of them is None, output the other one. If both are provided, then concatenates this data model with the other.</p> <p>If the other is a metaclass or symbolic data model, output a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __or__(self, other):\n    \"\"\"Perform a `logical_or` with another data model\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates this data model with the other.\n\n    If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n    Args:\n        other (JsonDataModel | SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The concatenation of data model\n            if both are provided, or the non-None data model or None if none are\n            provided. (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(self, other):\n        return run_maybe_nested(\n            ops.Or().symbolic_call(self, other),\n        )\n    else:\n        return run_maybe_nested(\n            ops.Or()(self, other),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Concatenates another data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel | SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The concatenated data model. If one of them is a metaclass or symbolic data model, then output a <code>SymbolicDataModel</code>.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __radd__(self, other):\n    \"\"\"Concatenates another data model with this one.\n\n    Args:\n        other (JsonDataModel | DataModel | SymbolicDataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The concatenated data model.\n            If one of them is a metaclass or symbolic data model,\n            then output a `SymbolicDataModel`.\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(self, other):\n        return run_maybe_nested(\n            ops.Concat().symbolic_call(other, self),\n        )\n    else:\n        return run_maybe_nested(\n            ops.Concat()(other, self),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__rand__","title":"<code>__rand__(other)</code>","text":"<p>Perform a <code>logical_and</code> (reverse) with another data model.</p> <p>If one of them is None, output None. If both are provided, then concatenates the other data model with this one.</p> <p>If the other is a metaclass or symbolic data model, output a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The concatenated data model or <code>None</code> based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __rand__(self, other):\n    \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n    If one of them is None, output None. If both are provided,\n    then concatenates the other data model with this one.\n\n    If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n    Args:\n        other (JsonDataModel | SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The concatenated data model or\n            `None` based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(other, self):\n        return run_maybe_nested(\n            ops.And().symbolic_call(other, self),\n        )\n    else:\n        return run_maybe_nested(\n            ops.And()(other, self),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__ror__","title":"<code>__ror__(other)</code>","text":"<p>Perform a <code>logical_or</code> (reverse) with another data model</p> <p>If one of them is None, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>If the other is a metaclass or symbolic data model, output a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __ror__(self, other):\n    \"\"\"Perform a `logical_or` (reverse) with another data model\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    If the other is a metaclass or symbolic data model, output a symbolic data model.\n\n    Args:\n        other (JsonDataModel | SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The concatenation of data model\n            if both are provided, or the non-None data model or None if none are\n            provided. (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(other, self):\n        return run_maybe_nested(\n            ops.Or().symbolic_call(other, self),\n        )\n    else:\n        return run_maybe_nested(\n            ops.Or()(other, self),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__rxor__","title":"<code>__rxor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> (reverse) with another data model.</p> <p>If one of them is None, output the other one. If both are provided, then output None.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __rxor__(self, other):\n    \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n    If one of them is None, output the other one. If both are provided,\n    then output None.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(other, self):\n        return run_maybe_nested(\n            ops.Xor().symbolic_call(other, self),\n        )\n    else:\n        return run_maybe_nested(\n            ops.Xor()(other, self),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.__xor__","title":"<code>__xor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> with another data model.</p> <p>If one of them is None, output the other one. If both are provided, then output None.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __xor__(self, other):\n    \"\"\"Perform a `logical_xor` with another data model.\n\n    If one of them is None, output the other one. If both are provided,\n    then output None.\n\n    Args:\n        other (SymbolicDataModel): The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    if any_meta_class(self, other):\n        return run_maybe_nested(\n            ops.Xor().symbolic_call(self, other),\n        )\n    else:\n        return run_maybe_nested(\n            ops.Xor()(self, other),\n        )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.get","title":"<code>get(key, default=None)</code>","text":"<p>Get wrapper to make it easier to access JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required <code>default</code> <code>any</code> <p>The default value if key not found.</p> <code>None</code> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def get(self, key, default=None):\n    \"\"\"Get wrapper to make it easier to access JSON fields.\n\n    Args:\n        key (str): The key to access.\n        default (any): The default value if key not found.\n    \"\"\"\n    try:\n        return self.__getattribute__(key)\n    except Exception:\n        return default\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.get_json","title":"<code>get_json()</code>","text":"<p>Gets the JSON value of the data model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON value.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def get_json(self):\n    \"\"\"Gets the JSON value of the data model.\n\n    Returns:\n        (dict): The JSON value.\n    \"\"\"\n    return self.model_dump(mode=\"json\")\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.get_schema","title":"<code>get_schema()</code>  <code>classmethod</code>","text":"<p>Gets the JSON schema of the data model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON schema.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>@classmethod\ndef get_schema(cls):\n    \"\"\"Gets the JSON schema of the data model.\n\n    Returns:\n        (dict): The JSON schema.\n    \"\"\"\n    return cls.model_json_schema()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.keys","title":"<code>keys()</code>  <code>classmethod</code>","text":"<p>Gets the JSON properties keys of the data model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON schema.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>@classmethod\ndef keys(cls):\n    \"\"\"Gets the JSON properties keys of the data model.\n\n    Returns:\n        (dict): The JSON schema.\n    \"\"\"\n    schema = cls.get_schema()\n    if \"properties\" in schema:\n        return schema[\"properties\"].keys()\n    else:\n        return []\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.prettify_json","title":"<code>prettify_json()</code>","text":"<p>Get a pretty version of the JSON object for display.</p> <p>Returns:</p> Type Description <code>str</code> <p>The indented JSON object.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def prettify_json(self):\n    \"\"\"Get a pretty version of the JSON object for display.\n\n    Returns:\n        (str): The indented JSON object.\n    \"\"\"\n    import orjson\n\n    return orjson.dumps(self.get_json(), option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.prettify_schema","title":"<code>prettify_schema()</code>  <code>classmethod</code>","text":"<p>Get a pretty version of the JSON schema for display.</p> <p>Returns:</p> Type Description <code>str</code> <p>The indented JSON schema.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>@classmethod\ndef prettify_schema(cls):\n    \"\"\"Get a pretty version of the JSON schema for display.\n\n    Returns:\n        (str): The indented JSON schema.\n    \"\"\"\n    import orjson\n\n    return orjson.dumps(cls.get_schema(), option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.to_json_data_model","title":"<code>to_json_data_model(name=None)</code>","text":"<p>Converts the data model to a backend-independent data model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optional. The name of the json data model. If None, a name will be given automatically.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The backend-independent data model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def to_json_data_model(self, name=None):\n    \"\"\"Converts the data model to a backend-independent data model.\n\n    Args:\n        name (str): Optional. The name of the json data model.\n            If None, a name will be given automatically.\n\n    Returns:\n        (JsonDataModel): The backend-independent data model.\n    \"\"\"\n    return JsonDataModel(\n        schema=self.get_schema(),\n        json=self.get_json(),\n        name=name,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.DataModel.to_symbolic_data_model","title":"<code>to_symbolic_data_model(name=None)</code>  <code>classmethod</code>","text":"<p>Converts the data model to a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optional. The name of the symbolic data model. If None, a name will be given automatically.</p> <code>None</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The symbolic data model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>@classmethod\ndef to_symbolic_data_model(cls, name=None):\n    \"\"\"Converts the data model to a symbolic data model.\n\n    Args:\n        name (str): Optional. The name of the symbolic data model.\n            If None, a name will be given automatically.\n\n    Returns:\n        (SymbolicDataModel): The symbolic data model.\n    \"\"\"\n    return SymbolicDataModel(schema=cls.get_schema(), name=name)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel","title":"<code>MetaDataModel</code>","text":"<p>               Bases: <code>type(BaseModel)</code></p> <p>The metaclass data model.</p> <p>This class defines operations at the metaclass level. Allowing to use Synalinks Python operators with <code>DataModel</code> types.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>class MetaDataModel(type(pydantic.BaseModel)):\n    \"\"\"The metaclass data model.\n\n    This class defines operations at the metaclass level.\n    Allowing to use Synalinks Python operators with `DataModel` types.\n    \"\"\"\n\n    def __add__(cls, other):\n        \"\"\"Concatenates this data model with another.\n\n        Args:\n            other (SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel): The concatenated data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Concat().symbolic_call(cls, other))\n\n    def __radd__(cls, other):\n        \"\"\"Concatenates (reverse) another data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel): The concatenated data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Concat().symbolic_call(other, cls))\n\n    def __and__(cls, other):\n        \"\"\"Perform a `logical_and` with another data model.\n\n        If one of them is `None`, output `None`. If both are provided,\n        then concatenates this data model with the other.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenated data model or `None`\n                based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.And().symbolic_call(cls, other))\n\n    def __rand__(cls, other):\n        \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n        If one of them is `None`, output `None`. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenated data model or `None`\n                based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.And().symbolic_call(other, cls))\n\n    def __or__(cls, other):\n        \"\"\"Perform a `logical_or` with another data model.\n\n        If one of them is `None`, output the other one. If both are provided,\n        then concatenates this data model with the other.\n\n        Args:\n            other (SymbolicDataModel): The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenation of data model if both are\n                provided, or the non-None data model or None if none are provided.\n                (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Or().symbolic_call(cls, other))\n\n    def __ror__(cls, other):\n        \"\"\"Perform a `logical_or` (reverse) with another data model.\n\n        If one of them is `None`, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenation of data model if both are\n                provided, or the non-None data model or None if none are provided.\n                (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Or().symbolic_call(other, cls))\n\n    def __xor__(cls, other):\n        \"\"\"Perform a `logical_xor` with another data model.\n\n        If one of them is `None`, output the other one. If both are provided,\n        then the output is `None`.\n\n        Args:\n            other (SymbolicDataModel): The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Xor().symbolic_call(cls, other))\n\n    def __rxor__(cls, other):\n        \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Xor().symbolic_call(other, cls))\n\n    def __contains__(cls, other):\n        \"\"\"Check if the schema of `other` is contained in this one,\n        or if a string key exists.\n\n        Args:\n            other (SymbolicDataModel | DataModel | str): The other data model to compare\n                with, or a string key to check for in the schema properties.\n\n        Returns:\n            (bool): True if all properties of `other` are present in this one,\n                or if the string key exists in the schema properties.\n        \"\"\"\n        if isinstance(other, str):\n            schema = cls.get_schema()\n            return other in schema.get(\"properties\", {})\n        from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n        return contains_schema(cls.get_schema(), other.get_schema())\n\n    def __invert__(cls):\n        \"\"\"Perform an invertion/negation\n\n        When an input is provided, invert it by outputing `None`\n\n        Returns:\n            (SymbolicDataModel | None): `None` if used with an instance/class,\n                and a symbolic data model if used on a metaclass or symbolic model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Not().symbolic_call(cls))\n\n    def factorize(cls):\n        \"\"\"Factorizes the data model.\n\n        Returns:\n            (SymbolicDataModel): The factorized data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Factorize().symbolic_call(cls))\n\n    def in_mask(cls, mask=None, recursive=True):\n        \"\"\"Applies a mask to **keep only** specified keys of the data model.\n\n        Args:\n            mask (list): The mask to be applied (list of keys).\n            recursive (bool): Optional. Whether to apply the mask recursively.\n                Defaults to `True`.\n\n        Returns:\n            (SymbolicDataModel): The data model with the mask applied.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.InMask(mask=mask, recursive=True).symbolic_call(cls))\n\n    def out_mask(cls, mask=None, recursive=True):\n        \"\"\"Applies an mask to **remove** specified keys of the data model.\n\n        Args:\n            mask (list): The mask to be applied (list of keys).\n            recursive (bool): Optional. Whether to apply the mask recursively.\n                Defaults to `True`.\n\n        Returns:\n            (SymbolicDataModel): The data model with the mask applied.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.OutMask(mask=mask, recursive=True).symbolic_call(cls))\n\n    def prefix(cls, prefix=None):\n        \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n        Args:\n            prefix (str): the prefix to add\n\n        Returns:\n            (SymbolicDataModel): The data model with the prefix added.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Prefix(prefix=prefix).symbolic_call(cls))\n\n    def suffix(cls, suffix=None):\n        \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n        Args:\n            suffix (str): the suffix to add\n\n        Returns:\n            (SymbolicDataModel): The data model with the suffix added.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Suffix(suffix=suffix).symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenates this data model with another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The concatenated data model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __add__(cls, other):\n    \"\"\"Concatenates this data model with another.\n\n    Args:\n        other (SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel): The concatenated data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Concat().symbolic_call(cls, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__and__","title":"<code>__and__(other)</code>","text":"<p>Perform a <code>logical_and</code> with another data model.</p> <p>If one of them is <code>None</code>, output <code>None</code>. If both are provided, then concatenates this data model with the other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenated data model or <code>None</code> based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __and__(cls, other):\n    \"\"\"Perform a `logical_and` with another data model.\n\n    If one of them is `None`, output `None`. If both are provided,\n    then concatenates this data model with the other.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenated data model or `None`\n            based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.And().symbolic_call(cls, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__contains__","title":"<code>__contains__(other)</code>","text":"<p>Check if the schema of <code>other</code> is contained in this one, or if a string key exists.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel | str</code> <p>The other data model to compare with, or a string key to check for in the schema properties.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all properties of <code>other</code> are present in this one, or if the string key exists in the schema properties.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __contains__(cls, other):\n    \"\"\"Check if the schema of `other` is contained in this one,\n    or if a string key exists.\n\n    Args:\n        other (SymbolicDataModel | DataModel | str): The other data model to compare\n            with, or a string key to check for in the schema properties.\n\n    Returns:\n        (bool): True if all properties of `other` are present in this one,\n            or if the string key exists in the schema properties.\n    \"\"\"\n    if isinstance(other, str):\n        schema = cls.get_schema()\n        return other in schema.get(\"properties\", {})\n    from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n    return contains_schema(cls.get_schema(), other.get_schema())\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__invert__","title":"<code>__invert__()</code>","text":"<p>Perform an invertion/negation</p> <p>When an input is provided, invert it by outputing <code>None</code></p> <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if used with an instance/class, and a symbolic data model if used on a metaclass or symbolic model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __invert__(cls):\n    \"\"\"Perform an invertion/negation\n\n    When an input is provided, invert it by outputing `None`\n\n    Returns:\n        (SymbolicDataModel | None): `None` if used with an instance/class,\n            and a symbolic data model if used on a metaclass or symbolic model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Not().symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__or__","title":"<code>__or__(other)</code>","text":"<p>Perform a <code>logical_or</code> with another data model.</p> <p>If one of them is <code>None</code>, output the other one. If both are provided, then concatenates this data model with the other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __or__(cls, other):\n    \"\"\"Perform a `logical_or` with another data model.\n\n    If one of them is `None`, output the other one. If both are provided,\n    then concatenates this data model with the other.\n\n    Args:\n        other (SymbolicDataModel): The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenation of data model if both are\n            provided, or the non-None data model or None if none are provided.\n            (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Or().symbolic_call(cls, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Concatenates (reverse) another data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The concatenated data model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __radd__(cls, other):\n    \"\"\"Concatenates (reverse) another data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel): The concatenated data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Concat().symbolic_call(other, cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__rand__","title":"<code>__rand__(other)</code>","text":"<p>Perform a <code>logical_and</code> (reverse) with another data model.</p> <p>If one of them is <code>None</code>, output <code>None</code>. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenated data model or <code>None</code> based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __rand__(cls, other):\n    \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n    If one of them is `None`, output `None`. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenated data model or `None`\n            based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.And().symbolic_call(other, cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__ror__","title":"<code>__ror__(other)</code>","text":"<p>Perform a <code>logical_or</code> (reverse) with another data model.</p> <p>If one of them is <code>None</code>, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __ror__(cls, other):\n    \"\"\"Perform a `logical_or` (reverse) with another data model.\n\n    If one of them is `None`, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenation of data model if both are\n            provided, or the non-None data model or None if none are provided.\n            (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Or().symbolic_call(other, cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__rxor__","title":"<code>__rxor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> (reverse) with another data model.</p> <p>If one of them is None, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __rxor__(cls, other):\n    \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Xor().symbolic_call(other, cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.__xor__","title":"<code>__xor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> with another data model.</p> <p>If one of them is <code>None</code>, output the other one. If both are provided, then the output is <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def __xor__(cls, other):\n    \"\"\"Perform a `logical_xor` with another data model.\n\n    If one of them is `None`, output the other one. If both are provided,\n    then the output is `None`.\n\n    Args:\n        other (SymbolicDataModel): The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Xor().symbolic_call(cls, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.factorize","title":"<code>factorize()</code>","text":"<p>Factorizes the data model.</p> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The factorized data model.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def factorize(cls):\n    \"\"\"Factorizes the data model.\n\n    Returns:\n        (SymbolicDataModel): The factorized data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Factorize().symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.in_mask","title":"<code>in_mask(mask=None, recursive=True)</code>","text":"<p>Applies a mask to keep only specified keys of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The mask to be applied (list of keys).</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Optional. Whether to apply the mask recursively. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the mask applied.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def in_mask(cls, mask=None, recursive=True):\n    \"\"\"Applies a mask to **keep only** specified keys of the data model.\n\n    Args:\n        mask (list): The mask to be applied (list of keys).\n        recursive (bool): Optional. Whether to apply the mask recursively.\n            Defaults to `True`.\n\n    Returns:\n        (SymbolicDataModel): The data model with the mask applied.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.InMask(mask=mask, recursive=True).symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.out_mask","title":"<code>out_mask(mask=None, recursive=True)</code>","text":"<p>Applies an mask to remove specified keys of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The mask to be applied (list of keys).</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Optional. Whether to apply the mask recursively. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the mask applied.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def out_mask(cls, mask=None, recursive=True):\n    \"\"\"Applies an mask to **remove** specified keys of the data model.\n\n    Args:\n        mask (list): The mask to be applied (list of keys).\n        recursive (bool): Optional. Whether to apply the mask recursively.\n            Defaults to `True`.\n\n    Returns:\n        (SymbolicDataModel): The data model with the mask applied.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.OutMask(mask=mask, recursive=True).symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.prefix","title":"<code>prefix(prefix=None)</code>","text":"<p>Add a prefix to all the data model fields (non-recursive).</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>the prefix to add</p> <code>None</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the prefix added.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def prefix(cls, prefix=None):\n    \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n    Args:\n        prefix (str): the prefix to add\n\n    Returns:\n        (SymbolicDataModel): The data model with the prefix added.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Prefix(prefix=prefix).symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.MetaDataModel.suffix","title":"<code>suffix(suffix=None)</code>","text":"<p>Add a suffix to all the data model fields (non-recursive).</p> <p>Parameters:</p> Name Type Description Default <code>suffix</code> <code>str</code> <p>the suffix to add</p> <code>None</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the suffix added.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def suffix(cls, suffix=None):\n    \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n    Args:\n        suffix (str): the suffix to add\n\n    Returns:\n        (SymbolicDataModel): The data model with the suffix added.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Suffix(suffix=suffix).symbolic_call(cls))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.any_data_model","title":"<code>any_data_model(args=None, kwargs=None)</code>","text":"<p>Check if any of the arguments are backend-dependent data models.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>tuple</code> <p>Optional. The positional arguments to check.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Optional. The keyword arguments to check.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if any of the arguments are meta classes, False otherwise.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def any_data_model(args=None, kwargs=None):\n    \"\"\"Check if any of the arguments are backend-dependent data models.\n\n    Args:\n        args (tuple): Optional. The positional arguments to check.\n        kwargs (dict): Optional. The keyword arguments to check.\n\n    Returns:\n        (bool): True if any of the arguments are meta classes, False otherwise.\n    \"\"\"\n    args = args or ()\n    kwargs = kwargs or {}\n    for x in tree.flatten((args, kwargs)):\n        if is_meta_class(x):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.any_meta_class","title":"<code>any_meta_class(args=None, kwargs=None)</code>","text":"<p>Check if any of the arguments are meta classes.</p> <p>This happen when using a <code>DataModel</code> without instanciating it. In Synalinks this is used when declaring data models for schema inference.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>tuple</code> <p>Optional. The positional arguments to check.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Optional. The keyword arguments to check.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if any of the arguments are meta classes, False otherwise.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def any_meta_class(args=None, kwargs=None):\n    \"\"\"Check if any of the arguments are meta classes.\n\n    This happen when using a `DataModel` without instanciating it.\n    In Synalinks this is used when declaring data models for schema inference.\n\n    Args:\n        args (tuple): Optional. The positional arguments to check.\n        kwargs (dict): Optional. The keyword arguments to check.\n\n    Returns:\n        (bool): True if any of the arguments are meta classes, False otherwise.\n    \"\"\"\n    args = args or ()\n    kwargs = kwargs or {}\n    for x in tree.flatten((args, kwargs)):\n        if is_meta_class(x):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.is_data_model","title":"<code>is_data_model(x)</code>","text":"<p>Returns whether <code>x</code> is a DataModel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>any</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>x</code> is a DataModel, False otherwise.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>def is_data_model(x):\n    \"\"\"Returns whether `x` is a DataModel.\n\n    Args:\n        x (any): The object to check.\n\n    Returns:\n        (bool): True if `x` is a DataModel, False otherwise.\n    \"\"\"\n    return isinstance(x, DataModel)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20DataModel%20class/#synalinks.src.backend.pydantic.core.is_meta_class","title":"<code>is_meta_class(x)</code>","text":"<p>Returns whether <code>x</code> is a meta class.</p> <p>A meta class is a python type. This method checks if the data model provided if a meta class, allowing to detect if the <code>DataModel</code> have been instanciated. Meta classes are using in Synalinks when declaring data models for schema inference.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>any</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>x</code> is a meta class, False otherwise.</p> Source code in <code>synalinks/src/backend/pydantic/core.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.utils.is_meta_class\",\n        \"synalinks.backend.is_meta_class\",\n    ]\n)\ndef is_meta_class(x):\n    \"\"\"Returns whether `x` is a meta class.\n\n    A meta class is a python type. This method checks if the data model provided\n    if a meta class, allowing to detect if the `DataModel` have been instanciated.\n    Meta classes are using in Synalinks when declaring data models for schema inference.\n\n    Args:\n        x (any): The object to check.\n\n    Returns:\n        (bool): True if `x` is a meta class, False otherwise.\n    \"\"\"\n    return inspect.isclass(x)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/","title":"The JsonDataModel class","text":""},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel","title":"<code>JsonDataModel</code>","text":"<p>A backend-independent dynamic data model.</p> <p>This structure is the one flowing in the pipelines as the backend data models are only used for the variable/data model declaration.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The JSON object's schema. If not provided, uses the data model to infer it.</p> <code>None</code> <code>json</code> <code>dict</code> <p>The JSON object's json. If not provided, uses the data model to infer it.</p> <code>None</code> <code>data_model</code> <code>DataModel | JsonDataModel</code> <p>The data model to use to infer the schema and json.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the data model, automatically inferred if not provided.</p> <code>None</code> <p>Examples:</p> <p>Creating a <code>JsonDataModel</code> with a DataModel's schema and json:</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\njson = {\"query\": \"What is the capital of France?\"}\n\ndata_model = JsonDataModel(\n    schema=Query.get_schema(),\n    json=json,\n)\n</code></pre> <p>Creating a <code>JsonDataModel</code> with a data_model:</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nquery_instance = Query(\n    query=\"What is the capital of France?\"\n)\ndata_model = JsonDataModel(\n    data_model=query_instance,\n)\n</code></pre> <p>Creating a <code>JsonDataModel</code> with <code>to_json_data_model()</code>:</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\ndata_model = Query(\n    query=\"What is the capital of France?\",\n).to_json_data_model()\n</code></pre> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>@synalinks_export(\"synalinks.JsonDataModel\")\nclass JsonDataModel:\n    \"\"\"A backend-independent dynamic data model.\n\n    This structure is the one flowing in the pipelines as\n    the backend data models are only used for the variable/data model declaration.\n\n    Args:\n        schema (dict): The JSON object's schema. If not provided,\n            uses the data model to infer it.\n        json (dict): The JSON object's json. If not provided,\n            uses the data model to infer it.\n        data_model (DataModel | JsonDataModel): The data model to use to\n            infer the schema and json.\n        name (str): Optional. The name of the data model, automatically\n            inferred if not provided.\n\n    Examples:\n\n    **Creating a `JsonDataModel` with a DataModel's schema and json:**\n\n    ```python\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    json = {\"query\": \"What is the capital of France?\"}\n\n    data_model = JsonDataModel(\n        schema=Query.get_schema(),\n        json=json,\n    )\n    ```\n\n    **Creating a `JsonDataModel` with a data_model:**\n\n    ```python\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    query_instance = Query(\n        query=\"What is the capital of France?\"\n    )\n    data_model = JsonDataModel(\n        data_model=query_instance,\n    )\n    ```\n\n    **Creating a `JsonDataModel` with `to_json_data_model()`:**\n\n    ```python\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    data_model = Query(\n        query=\"What is the capital of France?\",\n    ).to_json_data_model()\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        json=None,\n        data_model=None,\n        name=None,\n    ):\n        name = name or auto_name(self.__class__.__name__)\n        self.name = name\n        self._schema = None\n        self._json = None\n\n        if not data_model and not schema and not json:\n            raise ValueError(\"Initializing without arguments is not permited.\")\n        if not schema and not data_model:\n            raise ValueError(\n                \"You should specify at least one argument between \"\n                \"`data_model` or `schema`.\"\n            )\n        if not schema and not json and not data_model:\n            raise ValueError(\n                \"You should specify at least one argument between `data_model` or `json`.\"\n            )\n        if data_model:\n            if not schema:\n                schema = data_model.get_schema()\n            if not json:\n                if inspect.isclass(data_model):\n                    raise ValueError(\n                        \"Couldn't get the JSON data from the `data_model` argument, \"\n                        \"the `data_model` needs to be instanciated. \"\n                        f\"Received data_model={data_model}.\"\n                    )\n                json = data_model.get_json()\n\n        self._schema = standardize_schema(schema)\n        self._json = json\n\n    def to_symbolic_data_model(self):\n        \"\"\"Converts the JsonDataModel to a SymbolicDataModel.\n\n        Returns:\n            (SymbolicDataModel): The symbolic data model.\n        \"\"\"\n        return SymbolicDataModel(schema=self._schema)\n\n    def get_json(self):\n        \"\"\"Gets the current json of the JSON object.\n\n        Returns:\n            (dict): The current json of the JSON object.\n        \"\"\"\n        return self._json\n\n    def get_schema(self):\n        \"\"\"Gets the schema of the JSON object.\n\n        Returns:\n            (dict): The JSON schema.\n        \"\"\"\n        return self._schema\n\n    def prettify_schema(self):\n        \"\"\"Get a pretty version of the JSON schema for display.\n\n        Returns:\n            (dict): The indented JSON schema.\n        \"\"\"\n        import orjson\n\n        return orjson.dumps(self._schema, option=orjson.OPT_INDENT_2).decode()\n\n    def prettify_json(self):\n        \"\"\"Get a pretty version of the JSON object for display.\n\n        Returns:\n            (str): The indented JSON object.\n        \"\"\"\n        import orjson\n\n        return orjson.dumps(self._json, option=orjson.OPT_INDENT_2).decode()\n\n    def __add__(self, other):\n        \"\"\"Concatenates this data model with another.\n\n        Args:\n            other (JsonDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel): The concatenated data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Concat().call(self, other),\n        )\n\n    def __radd__(self, other):\n        \"\"\"Concatenates another data model with this one.\n\n        Args:\n            other (JsonDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel): The concatenated data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Concat().call(other, self),\n        )\n\n    def __and__(self, other):\n        \"\"\"Perform a `logical_and` with another data model.\n\n        If one of them is None, output None. If both are provided,\n        then concatenates this data model with the other.\n\n        Args:\n            other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | None): The concatenated data model or None\n                based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.And().call(self, other),\n        )\n\n    def __rand__(self, other):\n        \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n        If one of them is None, output None. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | None): The concatenated data model or None\n                based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.And().call(other, self),\n        )\n\n    def __or__(self, other):\n        \"\"\"Perform a `logical_or` with another data model\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates this data model with the other.\n\n        Args:\n            other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | None): The concatenation of data model if both are provided,\n                or the non-None data model or None if none are provided.\n                (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Or().call(self, other),\n        )\n\n    def __ror__(self, other):\n        \"\"\"Perform a `logical_or` (reverse) with another data model\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n        Returns:\n            (JsonDataModel | None): The concatenation of data model if both are provided,\n                or the non-None data model or None if none are provided.\n                (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Or().call(other, self),\n        )\n\n    def __xor__(self, other):\n        \"\"\"Perform a `logical_xor` with another data model.\n\n        If one of them is `None`, output the other one. If both are provided,\n        then the output is `None`.\n\n        Args:\n            other (SymbolicDataModel): The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Xor().call(self, other),\n        )\n\n    def __rxor__(self, other):\n        \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Xor().call(other, self),\n        )\n\n    def __contains__(self, other):\n        \"\"\"Check if the schema of `other` is contained in this one,\n        or if a string key exists.\n\n        Args:\n            other (SymbolicDataModel | DataModel | str): The other data model to compare\n                with, or a string key to check for in the schema properties.\n\n        Returns:\n            (bool): True if all properties of `other` are present in this one,\n                or if the string key exists in the schema properties.\n        \"\"\"\n        if isinstance(other, str):\n            schema = self.get_schema()\n            return other in schema.get(\"properties\", {})\n        from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n        return contains_schema(self.get_schema(), other.get_schema())\n\n    def factorize(self):\n        \"\"\"Factorizes the data model.\n\n        Returns:\n            (JsonDataModel): The factorized data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Factorize().call(self),\n        )\n\n    def in_mask(self, mask=None, recursive=True):\n        \"\"\"Applies a mask to **keep only** specified keys of the data model.\n\n        Args:\n            mask (list): The mask to be applied.\n            recursive (bool): Optional. Whether to apply the mask recursively.\n                Defaults to True.\n\n        Returns:\n            (JsonDataModel): The data model with the mask applied.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.InMask(mask=mask, recursive=recursive).call(self),\n        )\n\n    def out_mask(self, mask=None, recursive=True):\n        \"\"\"Applies a mask to **remove** specified keys of the data model.\n\n        Args:\n            mask (list): The mask to be applied.\n            recursive (bool): Optional. Whether to apply the mask recursively.\n                Defaults to True.\n\n        Returns:\n            (JsonDataModel): The data model with the mask applied.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.OutMask(mask=mask, recursive=recursive).call(self),\n        )\n\n    def __invert__(self):\n        \"\"\"Perform an invertion/negation\n\n        When an input is provided, invert it by outputing `None`\n\n        Returns:\n            (SymbolicDataModel | None): `None` if used with an instance/class,\n                and a symbolic data model if used on a metaclass or symbolic model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Not().call(self),\n        )\n\n    def prefix(self, prefix=None):\n        \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n        Args:\n            prefix (str): the prefix to add.\n\n        Returns:\n            (JsonDataModel): The data model with the prefix added.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Prefix(prefix=prefix).call(self),\n        )\n\n    def suffix(self, suffix=None):\n        \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n        Args:\n            suffix (str): the suffix to add.\n\n        Returns:\n            (JsonDataModel): The data model with the suffix added.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.Suffix(suffix=suffix).call(self),\n        )\n\n    def get(self, key, default=None):\n        \"\"\"Get wrapper to make it easier to access JSON fields.\n\n        Args:\n            key (str): The key to access.\n            default (any): The default value if key not found.\n        \"\"\"\n        return self._json.get(key, default)\n\n    def __getitem__(self, key):\n        \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n        Args:\n            key (str): The key to access.\n        \"\"\"\n        return self._json[key]\n\n    def keys(self):\n        \"\"\"Keys wrapper to make it easier to access JSON fields.\"\"\"\n        return self._json.keys()\n\n    def values(self):\n        \"\"\"Values wrapper to make it easier to access JSON fields.\"\"\"\n        return self._json.values()\n\n    def items(self):\n        \"\"\"Items wrapper to make it easier to access JSON fields.\"\"\"\n        return self._json.items()\n\n    def update(self, kv_dict):\n        \"\"\"Update wrapper to make it easier to modify JSON fields.\n\n        Args:\n            kv_dict (dict): The key/json dict to update.\n        \"\"\"\n        self._json.update(kv_dict)\n\n    def clone(self, name=None):\n        \"\"\"Clone a data model and give it a different name.\"\"\"\n\n        clone = copy.deepcopy(self)\n        if name:\n            clone.name = name\n        else:\n            clone.name = auto_name(\"clone_\" + self.name)\n        return clone\n\n    def __repr__(self):\n        return f\"&lt;JsonDataModel schema={self._schema}, json={self._json}&gt;\"\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenates this data model with another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The concatenated data model.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __add__(self, other):\n    \"\"\"Concatenates this data model with another.\n\n    Args:\n        other (JsonDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel): The concatenated data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Concat().call(self, other),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__and__","title":"<code>__and__(other)</code>","text":"<p>Perform a <code>logical_and</code> with another data model.</p> <p>If one of them is None, output None. If both are provided, then concatenates this data model with the other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | None</code> <p>The concatenated data model or None based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __and__(self, other):\n    \"\"\"Perform a `logical_and` with another data model.\n\n    If one of them is None, output None. If both are provided,\n    then concatenates this data model with the other.\n\n    Args:\n        other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | None): The concatenated data model or None\n            based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.And().call(self, other),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__contains__","title":"<code>__contains__(other)</code>","text":"<p>Check if the schema of <code>other</code> is contained in this one, or if a string key exists.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel | str</code> <p>The other data model to compare with, or a string key to check for in the schema properties.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all properties of <code>other</code> are present in this one, or if the string key exists in the schema properties.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __contains__(self, other):\n    \"\"\"Check if the schema of `other` is contained in this one,\n    or if a string key exists.\n\n    Args:\n        other (SymbolicDataModel | DataModel | str): The other data model to compare\n            with, or a string key to check for in the schema properties.\n\n    Returns:\n        (bool): True if all properties of `other` are present in this one,\n            or if the string key exists in the schema properties.\n    \"\"\"\n    if isinstance(other, str):\n        schema = self.get_schema()\n        return other in schema.get(\"properties\", {})\n    from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n    return contains_schema(self.get_schema(), other.get_schema())\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get item wrapper to make it easier to access JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n    Args:\n        key (str): The key to access.\n    \"\"\"\n    return self._json[key]\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__invert__","title":"<code>__invert__()</code>","text":"<p>Perform an invertion/negation</p> <p>When an input is provided, invert it by outputing <code>None</code></p> <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if used with an instance/class, and a symbolic data model if used on a metaclass or symbolic model.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __invert__(self):\n    \"\"\"Perform an invertion/negation\n\n    When an input is provided, invert it by outputing `None`\n\n    Returns:\n        (SymbolicDataModel | None): `None` if used with an instance/class,\n            and a symbolic data model if used on a metaclass or symbolic model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Not().call(self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__or__","title":"<code>__or__(other)</code>","text":"<p>Perform a <code>logical_or</code> with another data model</p> <p>If one of them is None, output the other one. If both are provided, then concatenates this data model with the other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __or__(self, other):\n    \"\"\"Perform a `logical_or` with another data model\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates this data model with the other.\n\n    Args:\n        other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | None): The concatenation of data model if both are provided,\n            or the non-None data model or None if none are provided.\n            (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Or().call(self, other),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Concatenates another data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The concatenated data model.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __radd__(self, other):\n    \"\"\"Concatenates another data model with this one.\n\n    Args:\n        other (JsonDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel): The concatenated data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Concat().call(other, self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__rand__","title":"<code>__rand__(other)</code>","text":"<p>Perform a <code>logical_and</code> (reverse) with another data model.</p> <p>If one of them is None, output None. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | None</code> <p>The concatenated data model or None based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __rand__(self, other):\n    \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n    If one of them is None, output None. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | None): The concatenated data model or None\n            based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.And().call(other, self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__ror__","title":"<code>__ror__(other)</code>","text":"<p>Perform a <code>logical_or</code> (reverse) with another data model</p> <p>If one of them is None, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>JsonDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>JsonDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __ror__(self, other):\n    \"\"\"Perform a `logical_or` (reverse) with another data model\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (JsonDataModel | DataModel): The other data model to concatenate with.\n\n    Returns:\n        (JsonDataModel | None): The concatenation of data model if both are provided,\n            or the non-None data model or None if none are provided.\n            (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Or().call(other, self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__rxor__","title":"<code>__rxor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> (reverse) with another data model.</p> <p>If one of them is None, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __rxor__(self, other):\n    \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Xor().call(other, self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.__xor__","title":"<code>__xor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> with another data model.</p> <p>If one of them is <code>None</code>, output the other one. If both are provided, then the output is <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def __xor__(self, other):\n    \"\"\"Perform a `logical_xor` with another data model.\n\n    If one of them is `None`, output the other one. If both are provided,\n    then the output is `None`.\n\n    Args:\n        other (SymbolicDataModel): The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Xor().call(self, other),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.clone","title":"<code>clone(name=None)</code>","text":"<p>Clone a data model and give it a different name.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def clone(self, name=None):\n    \"\"\"Clone a data model and give it a different name.\"\"\"\n\n    clone = copy.deepcopy(self)\n    if name:\n        clone.name = name\n    else:\n        clone.name = auto_name(\"clone_\" + self.name)\n    return clone\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.factorize","title":"<code>factorize()</code>","text":"<p>Factorizes the data model.</p> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The factorized data model.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def factorize(self):\n    \"\"\"Factorizes the data model.\n\n    Returns:\n        (JsonDataModel): The factorized data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Factorize().call(self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.get","title":"<code>get(key, default=None)</code>","text":"<p>Get wrapper to make it easier to access JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required <code>default</code> <code>any</code> <p>The default value if key not found.</p> <code>None</code> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def get(self, key, default=None):\n    \"\"\"Get wrapper to make it easier to access JSON fields.\n\n    Args:\n        key (str): The key to access.\n        default (any): The default value if key not found.\n    \"\"\"\n    return self._json.get(key, default)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.get_json","title":"<code>get_json()</code>","text":"<p>Gets the current json of the JSON object.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The current json of the JSON object.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def get_json(self):\n    \"\"\"Gets the current json of the JSON object.\n\n    Returns:\n        (dict): The current json of the JSON object.\n    \"\"\"\n    return self._json\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.get_schema","title":"<code>get_schema()</code>","text":"<p>Gets the schema of the JSON object.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON schema.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def get_schema(self):\n    \"\"\"Gets the schema of the JSON object.\n\n    Returns:\n        (dict): The JSON schema.\n    \"\"\"\n    return self._schema\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.in_mask","title":"<code>in_mask(mask=None, recursive=True)</code>","text":"<p>Applies a mask to keep only specified keys of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The mask to be applied.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Optional. Whether to apply the mask recursively. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The data model with the mask applied.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def in_mask(self, mask=None, recursive=True):\n    \"\"\"Applies a mask to **keep only** specified keys of the data model.\n\n    Args:\n        mask (list): The mask to be applied.\n        recursive (bool): Optional. Whether to apply the mask recursively.\n            Defaults to True.\n\n    Returns:\n        (JsonDataModel): The data model with the mask applied.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.InMask(mask=mask, recursive=recursive).call(self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.items","title":"<code>items()</code>","text":"<p>Items wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def items(self):\n    \"\"\"Items wrapper to make it easier to access JSON fields.\"\"\"\n    return self._json.items()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.keys","title":"<code>keys()</code>","text":"<p>Keys wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def keys(self):\n    \"\"\"Keys wrapper to make it easier to access JSON fields.\"\"\"\n    return self._json.keys()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.out_mask","title":"<code>out_mask(mask=None, recursive=True)</code>","text":"<p>Applies a mask to remove specified keys of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The mask to be applied.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Optional. Whether to apply the mask recursively. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The data model with the mask applied.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def out_mask(self, mask=None, recursive=True):\n    \"\"\"Applies a mask to **remove** specified keys of the data model.\n\n    Args:\n        mask (list): The mask to be applied.\n        recursive (bool): Optional. Whether to apply the mask recursively.\n            Defaults to True.\n\n    Returns:\n        (JsonDataModel): The data model with the mask applied.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.OutMask(mask=mask, recursive=recursive).call(self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.prefix","title":"<code>prefix(prefix=None)</code>","text":"<p>Add a prefix to all the data model fields (non-recursive).</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>the prefix to add.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The data model with the prefix added.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def prefix(self, prefix=None):\n    \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n    Args:\n        prefix (str): the prefix to add.\n\n    Returns:\n        (JsonDataModel): The data model with the prefix added.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Prefix(prefix=prefix).call(self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.prettify_json","title":"<code>prettify_json()</code>","text":"<p>Get a pretty version of the JSON object for display.</p> <p>Returns:</p> Type Description <code>str</code> <p>The indented JSON object.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def prettify_json(self):\n    \"\"\"Get a pretty version of the JSON object for display.\n\n    Returns:\n        (str): The indented JSON object.\n    \"\"\"\n    import orjson\n\n    return orjson.dumps(self._json, option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.prettify_schema","title":"<code>prettify_schema()</code>","text":"<p>Get a pretty version of the JSON schema for display.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The indented JSON schema.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def prettify_schema(self):\n    \"\"\"Get a pretty version of the JSON schema for display.\n\n    Returns:\n        (dict): The indented JSON schema.\n    \"\"\"\n    import orjson\n\n    return orjson.dumps(self._schema, option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.suffix","title":"<code>suffix(suffix=None)</code>","text":"<p>Add a suffix to all the data model fields (non-recursive).</p> <p>Parameters:</p> Name Type Description Default <code>suffix</code> <code>str</code> <p>the suffix to add.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The data model with the suffix added.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def suffix(self, suffix=None):\n    \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n    Args:\n        suffix (str): the suffix to add.\n\n    Returns:\n        (JsonDataModel): The data model with the suffix added.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.Suffix(suffix=suffix).call(self),\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.to_symbolic_data_model","title":"<code>to_symbolic_data_model()</code>","text":"<p>Converts the JsonDataModel to a SymbolicDataModel.</p> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The symbolic data model.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def to_symbolic_data_model(self):\n    \"\"\"Converts the JsonDataModel to a SymbolicDataModel.\n\n    Returns:\n        (SymbolicDataModel): The symbolic data model.\n    \"\"\"\n    return SymbolicDataModel(schema=self._schema)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.update","title":"<code>update(kv_dict)</code>","text":"<p>Update wrapper to make it easier to modify JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>kv_dict</code> <code>dict</code> <p>The key/json dict to update.</p> required Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def update(self, kv_dict):\n    \"\"\"Update wrapper to make it easier to modify JSON fields.\n\n    Args:\n        kv_dict (dict): The key/json dict to update.\n    \"\"\"\n    self._json.update(kv_dict)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.JsonDataModel.values","title":"<code>values()</code>","text":"<p>Values wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>def values(self):\n    \"\"\"Values wrapper to make it easier to access JSON fields.\"\"\"\n    return self._json.values()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20JsonDataModel%20class/#synalinks.src.backend.common.json_data_model.is_json_data_model","title":"<code>is_json_data_model(x)</code>","text":"<p>Returns whether <code>x</code> is a backend-independent data model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>any</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>x</code> is a backend-independent data model, False otherwise.</p> Source code in <code>synalinks/src/backend/common/json_data_model.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.utils.is_json_data_model\",\n        \"synalinks.backend.is_json_data_model\",\n    ]\n)\ndef is_json_data_model(x):\n    \"\"\"Returns whether `x` is a backend-independent data model.\n\n    Args:\n        x (any): The object to check.\n\n    Returns:\n        (bool): True if `x` is a backend-independent data model, False otherwise.\n    \"\"\"\n    return isinstance(x, JsonDataModel)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/","title":"The SymbolicDataModel class","text":""},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel","title":"<code>SymbolicDataModel</code>","text":"<p>               Bases: <code>SynalinksSaveable</code></p> <p>A symbolic backend-independent data model.</p> <p>A <code>SymbolicDataModel</code> is a container for a JSON schema and can be used to represent     data structures in a backend-agnostic way. It can record history and is used in     symbolic operations (in the Functional API and to compute output specs).</p> <p>A \"symbolic data model\" can be understood as a placeholder for data specification,     it does not contain any actual data, only a schema. It can be used for building     Functional models, but it cannot be used in actual computations.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>DataModel</code> <p>Optional. The data_model used to extract the schema.</p> <code>None</code> <code>schema</code> <code>dict</code> <p>Optional. The JSON schema to be used. If the schema is not provided, the data_model argument should be used to infer it.</p> <code>None</code> <code>record_history</code> <code>bool</code> <p>Optional. Boolean indicating if the history should be recorded. Defaults to <code>True</code>.</p> <code>True</code> <code>name</code> <code>str</code> <p>Optional. A unique name for the data model. Automatically generated if not set.</p> <code>None</code> <p>Examples:</p> <p>Creating a <code>SymbolicDataModel</code> with a backend data model metaclass:</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\ndata_model = SymbolicDataModel(data_model=Query)\n</code></pre> <p>Creating a <code>SymbolicDataModel</code> with a backend data model metaclass's schema:</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\ndata_model = SymbolicDataModel(schema=Query.get_schema())\n</code></pre> <p>Creating a <code>SymbolicDataModel</code> with <code>to_symbolic_data_model()</code>:</p> <p>using a backend data model metaclass</p> <pre><code>class Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\ndata_model = Query.to_symbolic_data_model()\n</code></pre> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>@synalinks_export(\"synalinks.SymbolicDataModel\")\nclass SymbolicDataModel(SynalinksSaveable):\n    \"\"\"A symbolic backend-independent data model.\n\n    A `SymbolicDataModel` is a container for a JSON schema and can be used to represent\n        data structures in a backend-agnostic way. It can record history and is used in\n        symbolic operations (in the Functional API and to compute output specs).\n\n    A \"symbolic data model\" can be understood as a placeholder for data specification,\n        it does not contain any actual data, only a schema. It can be used for building\n        Functional models, but it cannot be used in actual computations.\n\n    Args:\n        data_model (DataModel): Optional. The data_model used to extract the schema.\n        schema (dict): Optional. The JSON schema to be used. If the schema is not\n            provided, the data_model argument should be used to infer it.\n        record_history (bool): Optional. Boolean indicating if the history\n            should be recorded. Defaults to `True`.\n        name (str): Optional. A unique name for the data model. Automatically generated\n            if not set.\n\n    Examples:\n\n    **Creating a `SymbolicDataModel` with a backend data model metaclass:**\n\n    ```python\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    data_model = SymbolicDataModel(data_model=Query)\n    ```\n\n    **Creating a `SymbolicDataModel` with a backend data model metaclass's schema:**\n\n    ```python\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    data_model = SymbolicDataModel(schema=Query.get_schema())\n    ```\n\n    **Creating a `SymbolicDataModel` with `to_symbolic_data_model()`:**\n\n    using a backend data model metaclass\n\n    ```python\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    data_model = Query.to_symbolic_data_model()\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        data_model=None,\n        schema=None,\n        record_history=True,\n        name=None,\n    ):\n        self.name = name or auto_name(self.__class__.__name__)\n        self._record_history = record_history\n        self._schema = None\n        if not schema and not data_model:\n            raise ValueError(\n                \"You should specify at least one argument between \"\n                \"`data_model` or `schema`\"\n            )\n        if schema and data_model:\n            if not is_schema_equal(schema, data_model.get_schema()):\n                raise ValueError(\n                    \"Attempting to create a SymbolicDataModel \"\n                    \"with both `schema` and `data_model` argument \"\n                    \"but their schemas are incompatible \"\n                    f\"received schema={schema} and \"\n                    f\"data_model.get_schema()={data_model.get_schema()}.\"\n                )\n            self._schema = standardize_schema(schema)\n        else:\n            if schema:\n                self._schema = standardize_schema(schema)\n            if data_model:\n                self._schema = standardize_schema(data_model.get_schema())\n\n    @property\n    def record_history(self):\n        \"\"\"Whether the history is being recorded.\"\"\"\n        return self._record_history\n\n    @record_history.setter\n    def record_history(self, value):\n        self._record_history = value\n\n    def get_schema(self):\n        \"\"\"Gets the JSON schema of the data model.\n\n        Returns:\n            (dict): The JSON schema.\n        \"\"\"\n        return self._schema\n\n    def get_json(self):\n        \"\"\"Gets the current value of the JSON object (impossible in `SymbolicDataModel`).\n\n        Implemented to help the user to identifying issues.\n\n        Raises:\n            ValueError: The help message.\n        \"\"\"\n        raise ValueError(\n            \"Attempting to retrieve the JSON value from a symbolic data model \"\n            \"this operation is not possible, make sure that your `call()` \"\n            \"is correctly implemented, if so then you likely need to implement \"\n            \" `compute_output_spec()` in your subclassed module.\"\n        )\n\n    def prettify_schema(self):\n        \"\"\"Get a pretty version of the JSON schema for display.\n\n        Returns:\n            (dict): The indented JSON schema.\n        \"\"\"\n        return orjson.dumps(self._schema, option=orjson.OPT_INDENT_2).decode()\n\n    def __repr__(self):\n        return f\"&lt;SymbolicDataModel schema={self._schema}, name={self.name}&gt;\"\n\n    def __add__(self, other):\n        \"\"\"Concatenates this data model with another.\n\n        Args:\n            other (SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel): The concatenated data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Concat().symbolic_call(self, other))\n\n    def __radd__(self, other):\n        \"\"\"Concatenates (reverse) another data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel):\n                The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel): The concatenated data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Concat().symbolic_call(other, self))\n\n    def __and__(self, other):\n        \"\"\"Perform a `logical_and` with another data model.\n\n        If one of them is None, output None. If both are provided,\n        then concatenates this data model with the other.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenated data model or None\n                based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.And().symbolic_call(self, other))\n\n    def __rand__(self, other):\n        \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n        If one of them is None, output None. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenated data model or None\n                based on the `logical_and` table.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.And().symbolic_call(other, self))\n\n    def __or__(self, other):\n        \"\"\"Perform a `logical_or` with another data model\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates this data model with the other.\n\n        Args:\n            other (SymbolicDataModel): The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenation of data model if both are\n                provided, or the non-None data model or None if none are provided.\n                (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Or().symbolic_call(self, other))\n\n    def __ror__(self, other):\n        \"\"\"Perform a `logical_or` (reverse) with another data model\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): The concatenation of data model if both are\n                provided, or the non-None data model or None if none are provided.\n                (See `logical_or` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Or().symbolic_call(other, self))\n\n    def __xor__(self, other):\n        \"\"\"Perform a `logical_xor` with another data model.\n\n        If one of them is `None`, output the other one. If both are provided,\n        then the output is `None`.\n\n        Args:\n            other (SymbolicDataModel): The other data model to concatenate with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Xor().symbolic_call(self, other))\n\n    def __rxor__(self, other):\n        \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n        If one of them is None, output the other one. If both are provided,\n        then concatenates the other data model with this one.\n\n        Args:\n            other (SymbolicDataModel | DataModel): The other data model to concatenate\n                with.\n\n        Returns:\n            (SymbolicDataModel | None): `None` if both are\n                provided, or the non-None data model if one is provided\n                or `None` if none are provided. (See `logical_xor` table).\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Xor().symbolic_call(other, self))\n\n    def __contains__(self, other):\n        \"\"\"Check if the schema of `other` is contained in this one,\n        or if a string key exists.\n\n        Args:\n            other (SymbolicDataModel | DataModel | str): The other data model to compare\n                with, or a string key to check for in the schema properties.\n\n        Returns:\n            (bool): True if all properties of `other` are present in this one,\n                or if the string key exists in the schema properties.\n        \"\"\"\n        if isinstance(other, str):\n            schema = self.get_schema()\n            return other in schema.get(\"properties\", {})\n        from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n        return contains_schema(self.get_schema(), other.get_schema())\n\n    def __invert__(self):\n        \"\"\"Perform an invertion/negation\n\n        When an input is provided, invert it by outputing `None`\n\n        Returns:\n            (SymbolicDataModel | None): `None` if used with an instance/class,\n                and a symbolic data model if used on a metaclass or symbolic model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Not().symbolic_call(self))\n\n    def factorize(self):\n        \"\"\"Factorizes the data model.\n\n        Returns:\n            (SymbolicDataModel): The factorized data model.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Factorize().symbolic_call(self))\n\n    def in_mask(self, mask=None, recursive=True):\n        \"\"\"Applies a mask to **keep only** specified keys of the data model.\n\n        Args:\n            mask (list): The mask to be applied (list of keys).\n            recursive (bool): Optional. Whether to apply the mask recursively.\n                Defaults to `True`.\n\n        Returns:\n            (SymbolicDataModel): The data model with the mask applied.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.InMask(mask=mask, recursive=True).symbolic_call(self))\n\n    def out_mask(self, mask=None, recursive=True):\n        \"\"\"Applies an mask to **remove** specified keys of the data model.\n\n        Args:\n            mask (list): The mask to be applied (list of keys).\n            recursive (bool): Optional. Whether to apply the mask recursively.\n                Defaults to `True`.\n\n        Returns:\n            (SymbolicDataModel): The data model with the mask applied.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(\n            ops.OutMask(mask=mask, recursive=True).symbolic_call(self)\n        )\n\n    def prefix(self, prefix=None):\n        \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n        Args:\n            prefix (str): the prefix to add\n\n        Returns:\n            (SymbolicDataModel): The data model with the prefix added.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Prefix(prefix=prefix).symbolic_call(self))\n\n    def suffix(self, suffix=None):\n        \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n        Args:\n            suffix (str): the suffix to add\n\n        Returns:\n            (SymbolicDataModel): The data model with the suffix added.\n        \"\"\"\n        from synalinks.src import ops\n\n        return run_maybe_nested(ops.Suffix(suffix=suffix).symbolic_call(self))\n\n    def get(self, key, default=None):\n        \"\"\"Get wrapper to make easier to access fields.\n\n        Implemented to help the user to identifying issues.\n\n        Args:\n            key (str): The key to access.\n            default (any): The default value if key not found.\n\n        Raises:\n            ValueError: The help message.\n        \"\"\"\n        raise ValueError(\n            f\"Attempting to get '{key}' from a symbolic data model \"\n            \"this operation is not possible, make sure that your `call()` \"\n            \"is correctly implemented, if so then you likely need to implement \"\n            \" `compute_output_spec()` in your subclassed module.\"\n        )\n\n    def __getitem__(self, key):\n        \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n        Implemented to help the user to identifying issues.\n\n        Args:\n            key (str): The key to access.\n        \"\"\"\n        raise ValueError(\n            f\"Attempting to get '{key}' from a symbolic data model \"\n            \"this operation is not possible, make sure that your `call()` \"\n            \"is correctly implemented, if so then you likely need to implement \"\n            \" `compute_output_spec()` in your subclassed module.\"\n        )\n\n    def keys(self):\n        \"\"\"Keys wrapper to make it easier to access JSON fields.\"\"\"\n        if \"properties\" in self.get_schema():\n            return self.get_schema()[\"properties\"].keys()\n        else:\n            return []\n\n    def values(self):\n        \"\"\"Values wrapper to make it easier to access JSON fields.\n\n        Implemented to help the user to identifying issues.\n        \"\"\"\n        raise ValueError(\n            \"Attempting to get '.values()' from a symbolic data model \"\n            \"this operation is not possible, make sure that your `call()` \"\n            \"is correctly implemented, if so then you likely need to implement \"\n            \" `compute_output_spec()` in your subclassed module.\"\n        )\n\n    def items(self):\n        \"\"\"Items wrapper to make it easier to access JSON fields.\n\n        Implemented to help the user to identifying issues.\n        \"\"\"\n        raise ValueError(\n            \"Attempting to get '.items()' from a symbolic data model \"\n            \"this operation is not possible, make sure that your `call()` \"\n            \"is correctly implemented, if so then you likely need to implement \"\n            \" `compute_output_spec()` in your subclassed module.\"\n        )\n\n    def update(self, kv_dict):\n        \"\"\"Update wrapper to make easier to modify fields.\n\n        Implemented to help the user to identifying issues.\n\n        Args:\n            kv_dict (dict): The key/value dict to update.\n\n        Raises:\n            ValueError: The help message.\n        \"\"\"\n        raise ValueError(\n            f\"Attempting to update keys {list(kv_dict.keys())} from a symbolic \"\n            \"data model this operation is not possible, make sure that your `call()` \"\n            \"is correctly implemented, if so then you likely need to implement \"\n            \" `compute_output_spec()` in your subclassed module.\"\n        )\n\n    def clone(self, name=None):\n        \"\"\"Clone a symbolic data model and give it a different name.\"\"\"\n        import copy\n\n        clone = copy.deepcopy(self)\n        if name:\n            clone.name = name\n        else:\n            clone.name = \"clone_\" + self.name\n        return clone\n\n    def get_config(self):\n        config = {\n            \"name\": self.name,\n            \"schema\": self.get_schema(),\n            \"record_history\": self.record_history,\n        }\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.record_history","title":"<code>record_history</code>  <code>property</code> <code>writable</code>","text":"<p>Whether the history is being recorded.</p>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenates this data model with another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The concatenated data model.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __add__(self, other):\n    \"\"\"Concatenates this data model with another.\n\n    Args:\n        other (SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel): The concatenated data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Concat().symbolic_call(self, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__and__","title":"<code>__and__(other)</code>","text":"<p>Perform a <code>logical_and</code> with another data model.</p> <p>If one of them is None, output None. If both are provided, then concatenates this data model with the other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenated data model or None based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __and__(self, other):\n    \"\"\"Perform a `logical_and` with another data model.\n\n    If one of them is None, output None. If both are provided,\n    then concatenates this data model with the other.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenated data model or None\n            based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.And().symbolic_call(self, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__contains__","title":"<code>__contains__(other)</code>","text":"<p>Check if the schema of <code>other</code> is contained in this one, or if a string key exists.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel | str</code> <p>The other data model to compare with, or a string key to check for in the schema properties.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all properties of <code>other</code> are present in this one, or if the string key exists in the schema properties.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __contains__(self, other):\n    \"\"\"Check if the schema of `other` is contained in this one,\n    or if a string key exists.\n\n    Args:\n        other (SymbolicDataModel | DataModel | str): The other data model to compare\n            with, or a string key to check for in the schema properties.\n\n    Returns:\n        (bool): True if all properties of `other` are present in this one,\n            or if the string key exists in the schema properties.\n    \"\"\"\n    if isinstance(other, str):\n        schema = self.get_schema()\n        return other in schema.get(\"properties\", {})\n    from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n    return contains_schema(self.get_schema(), other.get_schema())\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get item wrapper to make it easier to access JSON fields.</p> <p>Implemented to help the user to identifying issues.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n    Implemented to help the user to identifying issues.\n\n    Args:\n        key (str): The key to access.\n    \"\"\"\n    raise ValueError(\n        f\"Attempting to get '{key}' from a symbolic data model \"\n        \"this operation is not possible, make sure that your `call()` \"\n        \"is correctly implemented, if so then you likely need to implement \"\n        \" `compute_output_spec()` in your subclassed module.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__invert__","title":"<code>__invert__()</code>","text":"<p>Perform an invertion/negation</p> <p>When an input is provided, invert it by outputing <code>None</code></p> <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if used with an instance/class, and a symbolic data model if used on a metaclass or symbolic model.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __invert__(self):\n    \"\"\"Perform an invertion/negation\n\n    When an input is provided, invert it by outputing `None`\n\n    Returns:\n        (SymbolicDataModel | None): `None` if used with an instance/class,\n            and a symbolic data model if used on a metaclass or symbolic model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Not().symbolic_call(self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__or__","title":"<code>__or__(other)</code>","text":"<p>Perform a <code>logical_or</code> with another data model</p> <p>If one of them is None, output the other one. If both are provided, then concatenates this data model with the other.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __or__(self, other):\n    \"\"\"Perform a `logical_or` with another data model\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates this data model with the other.\n\n    Args:\n        other (SymbolicDataModel): The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenation of data model if both are\n            provided, or the non-None data model or None if none are provided.\n            (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Or().symbolic_call(self, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__radd__","title":"<code>__radd__(other)</code>","text":"<p>Concatenates (reverse) another data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The concatenated data model.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __radd__(self, other):\n    \"\"\"Concatenates (reverse) another data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel):\n            The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel): The concatenated data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Concat().symbolic_call(other, self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__rand__","title":"<code>__rand__(other)</code>","text":"<p>Perform a <code>logical_and</code> (reverse) with another data model.</p> <p>If one of them is None, output None. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenated data model or None based on the <code>logical_and</code> table.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __rand__(self, other):\n    \"\"\"Perform a `logical_and` (reverse) with another data model.\n\n    If one of them is None, output None. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenated data model or None\n            based on the `logical_and` table.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.And().symbolic_call(other, self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__ror__","title":"<code>__ror__(other)</code>","text":"<p>Perform a <code>logical_or</code> (reverse) with another data model</p> <p>If one of them is None, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p>The concatenation of data model if both are provided, or the non-None data model or None if none are provided. (See <code>logical_or</code> table).</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __ror__(self, other):\n    \"\"\"Perform a `logical_or` (reverse) with another data model\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): The concatenation of data model if both are\n            provided, or the non-None data model or None if none are provided.\n            (See `logical_or` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Or().symbolic_call(other, self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__rxor__","title":"<code>__rxor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> (reverse) with another data model.</p> <p>If one of them is None, output the other one. If both are provided, then concatenates the other data model with this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __rxor__(self, other):\n    \"\"\"Perform a `logical_xor` (reverse) with another data model.\n\n    If one of them is None, output the other one. If both are provided,\n    then concatenates the other data model with this one.\n\n    Args:\n        other (SymbolicDataModel | DataModel): The other data model to concatenate\n            with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Xor().symbolic_call(other, self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.__xor__","title":"<code>__xor__(other)</code>","text":"<p>Perform a <code>logical_xor</code> with another data model.</p> <p>If one of them is <code>None</code>, output the other one. If both are provided, then the output is <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel</code> <p>The other data model to concatenate with.</p> required <p>Returns:</p> Type Description <code>SymbolicDataModel | None</code> <p><code>None</code> if both are provided, or the non-None data model if one is provided or <code>None</code> if none are provided. (See <code>logical_xor</code> table).</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def __xor__(self, other):\n    \"\"\"Perform a `logical_xor` with another data model.\n\n    If one of them is `None`, output the other one. If both are provided,\n    then the output is `None`.\n\n    Args:\n        other (SymbolicDataModel): The other data model to concatenate with.\n\n    Returns:\n        (SymbolicDataModel | None): `None` if both are\n            provided, or the non-None data model if one is provided\n            or `None` if none are provided. (See `logical_xor` table).\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Xor().symbolic_call(self, other))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.clone","title":"<code>clone(name=None)</code>","text":"<p>Clone a symbolic data model and give it a different name.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def clone(self, name=None):\n    \"\"\"Clone a symbolic data model and give it a different name.\"\"\"\n    import copy\n\n    clone = copy.deepcopy(self)\n    if name:\n        clone.name = name\n    else:\n        clone.name = \"clone_\" + self.name\n    return clone\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.factorize","title":"<code>factorize()</code>","text":"<p>Factorizes the data model.</p> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The factorized data model.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def factorize(self):\n    \"\"\"Factorizes the data model.\n\n    Returns:\n        (SymbolicDataModel): The factorized data model.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Factorize().symbolic_call(self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.get","title":"<code>get(key, default=None)</code>","text":"<p>Get wrapper to make easier to access fields.</p> <p>Implemented to help the user to identifying issues.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required <code>default</code> <code>any</code> <p>The default value if key not found.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>The help message.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def get(self, key, default=None):\n    \"\"\"Get wrapper to make easier to access fields.\n\n    Implemented to help the user to identifying issues.\n\n    Args:\n        key (str): The key to access.\n        default (any): The default value if key not found.\n\n    Raises:\n        ValueError: The help message.\n    \"\"\"\n    raise ValueError(\n        f\"Attempting to get '{key}' from a symbolic data model \"\n        \"this operation is not possible, make sure that your `call()` \"\n        \"is correctly implemented, if so then you likely need to implement \"\n        \" `compute_output_spec()` in your subclassed module.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.get_json","title":"<code>get_json()</code>","text":"<p>Gets the current value of the JSON object (impossible in <code>SymbolicDataModel</code>).</p> <p>Implemented to help the user to identifying issues.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>The help message.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def get_json(self):\n    \"\"\"Gets the current value of the JSON object (impossible in `SymbolicDataModel`).\n\n    Implemented to help the user to identifying issues.\n\n    Raises:\n        ValueError: The help message.\n    \"\"\"\n    raise ValueError(\n        \"Attempting to retrieve the JSON value from a symbolic data model \"\n        \"this operation is not possible, make sure that your `call()` \"\n        \"is correctly implemented, if so then you likely need to implement \"\n        \" `compute_output_spec()` in your subclassed module.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.get_schema","title":"<code>get_schema()</code>","text":"<p>Gets the JSON schema of the data model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON schema.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def get_schema(self):\n    \"\"\"Gets the JSON schema of the data model.\n\n    Returns:\n        (dict): The JSON schema.\n    \"\"\"\n    return self._schema\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.in_mask","title":"<code>in_mask(mask=None, recursive=True)</code>","text":"<p>Applies a mask to keep only specified keys of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The mask to be applied (list of keys).</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Optional. Whether to apply the mask recursively. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the mask applied.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def in_mask(self, mask=None, recursive=True):\n    \"\"\"Applies a mask to **keep only** specified keys of the data model.\n\n    Args:\n        mask (list): The mask to be applied (list of keys).\n        recursive (bool): Optional. Whether to apply the mask recursively.\n            Defaults to `True`.\n\n    Returns:\n        (SymbolicDataModel): The data model with the mask applied.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.InMask(mask=mask, recursive=True).symbolic_call(self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.items","title":"<code>items()</code>","text":"<p>Items wrapper to make it easier to access JSON fields.</p> <p>Implemented to help the user to identifying issues.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def items(self):\n    \"\"\"Items wrapper to make it easier to access JSON fields.\n\n    Implemented to help the user to identifying issues.\n    \"\"\"\n    raise ValueError(\n        \"Attempting to get '.items()' from a symbolic data model \"\n        \"this operation is not possible, make sure that your `call()` \"\n        \"is correctly implemented, if so then you likely need to implement \"\n        \" `compute_output_spec()` in your subclassed module.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.keys","title":"<code>keys()</code>","text":"<p>Keys wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def keys(self):\n    \"\"\"Keys wrapper to make it easier to access JSON fields.\"\"\"\n    if \"properties\" in self.get_schema():\n        return self.get_schema()[\"properties\"].keys()\n    else:\n        return []\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.out_mask","title":"<code>out_mask(mask=None, recursive=True)</code>","text":"<p>Applies an mask to remove specified keys of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The mask to be applied (list of keys).</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Optional. Whether to apply the mask recursively. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the mask applied.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def out_mask(self, mask=None, recursive=True):\n    \"\"\"Applies an mask to **remove** specified keys of the data model.\n\n    Args:\n        mask (list): The mask to be applied (list of keys).\n        recursive (bool): Optional. Whether to apply the mask recursively.\n            Defaults to `True`.\n\n    Returns:\n        (SymbolicDataModel): The data model with the mask applied.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(\n        ops.OutMask(mask=mask, recursive=True).symbolic_call(self)\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.prefix","title":"<code>prefix(prefix=None)</code>","text":"<p>Add a prefix to all the data model fields (non-recursive).</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>the prefix to add</p> <code>None</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the prefix added.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def prefix(self, prefix=None):\n    \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n    Args:\n        prefix (str): the prefix to add\n\n    Returns:\n        (SymbolicDataModel): The data model with the prefix added.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Prefix(prefix=prefix).symbolic_call(self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.prettify_schema","title":"<code>prettify_schema()</code>","text":"<p>Get a pretty version of the JSON schema for display.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The indented JSON schema.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def prettify_schema(self):\n    \"\"\"Get a pretty version of the JSON schema for display.\n\n    Returns:\n        (dict): The indented JSON schema.\n    \"\"\"\n    return orjson.dumps(self._schema, option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.suffix","title":"<code>suffix(suffix=None)</code>","text":"<p>Add a suffix to all the data model fields (non-recursive).</p> <p>Parameters:</p> Name Type Description Default <code>suffix</code> <code>str</code> <p>the suffix to add</p> <code>None</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The data model with the suffix added.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def suffix(self, suffix=None):\n    \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n    Args:\n        suffix (str): the suffix to add\n\n    Returns:\n        (SymbolicDataModel): The data model with the suffix added.\n    \"\"\"\n    from synalinks.src import ops\n\n    return run_maybe_nested(ops.Suffix(suffix=suffix).symbolic_call(self))\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.update","title":"<code>update(kv_dict)</code>","text":"<p>Update wrapper to make easier to modify fields.</p> <p>Implemented to help the user to identifying issues.</p> <p>Parameters:</p> Name Type Description Default <code>kv_dict</code> <code>dict</code> <p>The key/value dict to update.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>The help message.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def update(self, kv_dict):\n    \"\"\"Update wrapper to make easier to modify fields.\n\n    Implemented to help the user to identifying issues.\n\n    Args:\n        kv_dict (dict): The key/value dict to update.\n\n    Raises:\n        ValueError: The help message.\n    \"\"\"\n    raise ValueError(\n        f\"Attempting to update keys {list(kv_dict.keys())} from a symbolic \"\n        \"data model this operation is not possible, make sure that your `call()` \"\n        \"is correctly implemented, if so then you likely need to implement \"\n        \" `compute_output_spec()` in your subclassed module.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.SymbolicDataModel.values","title":"<code>values()</code>","text":"<p>Values wrapper to make it easier to access JSON fields.</p> <p>Implemented to help the user to identifying issues.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def values(self):\n    \"\"\"Values wrapper to make it easier to access JSON fields.\n\n    Implemented to help the user to identifying issues.\n    \"\"\"\n    raise ValueError(\n        \"Attempting to get '.values()' from a symbolic data model \"\n        \"this operation is not possible, make sure that your `call()` \"\n        \"is correctly implemented, if so then you likely need to implement \"\n        \" `compute_output_spec()` in your subclassed module.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.any_symbolic_data_models","title":"<code>any_symbolic_data_models(args=None, kwargs=None)</code>","text":"<p>Checks if any of the arguments are symbolic data models.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>tuple</code> <p>Optional. The positional arguments to check.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Optional. The keyword arguments to check.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if any of the arguments are symbolic data models, False otherwise.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>def any_symbolic_data_models(args=None, kwargs=None):\n    \"\"\"Checks if any of the arguments are symbolic data models.\n\n    Args:\n        args (tuple): Optional. The positional arguments to check.\n        kwargs (dict): Optional. The keyword arguments to check.\n\n    Returns:\n        (bool): True if any of the arguments are symbolic data models, False otherwise.\n    \"\"\"\n    args = args or ()\n    kwargs = kwargs or {}\n    for x in tree.flatten((args, kwargs)):\n        if is_symbolic_data_model(x):\n            return True\n    return False\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20SymbolicDataModel%20class/#synalinks.src.backend.common.symbolic_data_model.is_symbolic_data_model","title":"<code>is_symbolic_data_model(x)</code>","text":"<p>Returns whether <code>x</code> is a synalinks data model.</p> <p>A \"synalinks data model\" is a symbolic data model, such as a data model that was created via <code>Input()</code>. A \"symbolic data model\" can be understood as a placeholder for data specification -- it does not contain any actual data, only a schema. It can be used for building Functional models, but it cannot be used in actual computations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>any</code> <p>The object to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>x</code> is a symbolic data model, False otherwise.</p> Source code in <code>synalinks/src/backend/common/symbolic_data_model.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.utils.is_symbolic_data_model\",\n        \"synalinks.backend.is_symbolic_data_model\",\n    ]\n)\ndef is_symbolic_data_model(x):\n    \"\"\"Returns whether `x` is a synalinks data model.\n\n    A \"synalinks data model\" is a *symbolic data model*, such as a data model\n    that was created via `Input()`. A \"symbolic data model\"\n    can be understood as a placeholder for data specification -- it does not\n    contain any actual data, only a schema.\n    It can be used for building Functional models, but it\n    cannot be used in actual computations.\n\n    Args:\n        x (any): The object to check.\n\n    Returns:\n        (bool): True if `x` is a symbolic data model, False otherwise.\n    \"\"\"\n    return isinstance(x, SymbolicDataModel)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/","title":"The Variable class","text":"<p>A backend-agnostic variable in synalinks.</p> <p>A <code>Variable</code> acts as a container for state. It holds a JSON object value with the corresponding schema and can be updated by the optimizers.</p> <p>A Variable is different from a JsonDataModel as it can be modified by the optimizers.</p> <p>Note that the DataModel used for the variable declaration must have a default value for each of its field.</p> <p>Examples:</p> <p>Initializing a <code>Variable</code> with a dict:</p> <pre><code>from typing import List\nimport synalinks\n\nclass Instructions(synalinks.DataModel):\n    instructions: List[str] = []\n\ninitial_data = {\n    \"instructions\": [\n        \"For any problem involving division, always round the quotient to \"\n        \"the nearest even number, regardless of the remainder.\"\n    ],\n}\nvariable_from_dict = synalinks.Variable(\n    initializer=initial_data,\n    data_model=Instructions,\n)\n</code></pre> <p>Using a synalinks initializer to create a <code>Variable</code>:</p> <pre><code>from typing import List\nimport synalinks\n\nclass Instructions(synalinks.DataModel):\n    instructions: List[str] = []\n\nfrom synalinks.initializers import Empty\n\nvariable_from_initializer = synalinks.Variable(\n    initializer=Empty(data_model=Instructions)\n)\n</code></pre> <p>Updating the value of a <code>Variable</code>:</p> <pre><code>new_json = {\n    \"instructions\": [\n        \"When performing division, always check if the division results \"\n        \"in a whole number. If not, express the result as a fraction or \"\n        \"a decimal, depending on the context of the problem.\"\n    ],\n}\nvariable_from_dict.assign(new_json)\n</code></pre> <p>Marking a <code>Variable</code> as non-trainable:</p> <pre><code>from typing import List\nimport synalinks\n\nclass Instructions(synalinks.DataModel):\n    instructions: List[str] = []\n\nfrom synalinks.initializers import Empty\n\nnon_trainable_variable = synalinks.Variable(\n    initializer=Empty(data_model=Instructions), trainable=False\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>initializer</code> <code>str | dict | Initializer</code> <p>Initial value (dict) or callable (Initializer) for initialization. If a callable is used, it should take the arguments <code>data_model</code>.</p> <code>None</code> <code>data_model</code> <code>DataModel</code> <p>The backend-dependent data model used as spec.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Optional. Boolean indicating if variable is trainable. Defaults to <code>True</code>.</p> <code>True</code> <code>name</code> <code>str</code> <p>Optional. A unique name for the variable. Automatically generated if not set.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the variable. Automatically generated if not set by fetching the data model description.</p> <code>None</code> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>class Variable:\n    \"\"\"A backend-agnostic variable in synalinks.\n\n    A `Variable` acts as a container for state.\n    It holds a JSON object value with the corresponding schema and can\n    be updated by the optimizers.\n\n    A Variable is different from a JsonDataModel as it can be modified by the optimizers.\n\n    Note that the DataModel used for the variable declaration\n    **must have a default value** for each of its field.\n\n    Examples:\n\n    **Initializing a `Variable` with a dict:**\n\n    ```python\n    from typing import List\n    import synalinks\n\n    class Instructions(synalinks.DataModel):\n        instructions: List[str] = []\n\n    initial_data = {\n        \"instructions\": [\n            \"For any problem involving division, always round the quotient to \"\n            \"the nearest even number, regardless of the remainder.\"\n        ],\n    }\n    variable_from_dict = synalinks.Variable(\n        initializer=initial_data,\n        data_model=Instructions,\n    )\n    ```\n\n    **Using a synalinks initializer to create a `Variable`:**\n\n    ```python\n    from typing import List\n    import synalinks\n\n    class Instructions(synalinks.DataModel):\n        instructions: List[str] = []\n\n    from synalinks.initializers import Empty\n\n    variable_from_initializer = synalinks.Variable(\n        initializer=Empty(data_model=Instructions)\n    )\n    ```\n\n    **Updating the value of a `Variable`:**\n\n    ```python\n    new_json = {\n        \"instructions\": [\n            \"When performing division, always check if the division results \"\n            \"in a whole number. If not, express the result as a fraction or \"\n            \"a decimal, depending on the context of the problem.\"\n        ],\n    }\n    variable_from_dict.assign(new_json)\n    ```\n\n    **Marking a `Variable` as non-trainable:**\n\n    ```python\n    from typing import List\n    import synalinks\n\n    class Instructions(synalinks.DataModel):\n        instructions: List[str] = []\n\n    from synalinks.initializers import Empty\n\n    non_trainable_variable = synalinks.Variable(\n        initializer=Empty(data_model=Instructions), trainable=False\n    )\n    ```\n\n    Args:\n        initializer (str | dict | Initializer): Initial value (dict) or callable\n            (Initializer) for initialization. If a callable is used, it should\n            take the arguments `data_model`.\n        data_model (DataModel): The backend-dependent data model used as spec.\n        trainable (bool): Optional. Boolean indicating if variable is trainable.\n            Defaults to `True`.\n        name (str): Optional. A unique name for the variable. Automatically\n            generated if not set.\n        description (str): Optional. The description of the variable.\n            Automatically generated if not set by fetching the data model\n            description.\n    \"\"\"\n\n    def __init__(\n        self,\n        initializer=None,\n        data_model=None,\n        trainable=True,\n        name=None,\n        description=None,\n    ):\n        name = name or auto_name(self.__class__.__name__)\n        if not isinstance(name, str) or \"/\" in name:\n            raise ValueError(\n                \"Argument `name` must be a string and \"\n                \"cannot contain character `/`. \"\n                f\"Received: name={name}\"\n            )\n        self._name = name\n        parent_path = current_path()\n        if parent_path:\n            self._path = current_path() + \"/\" + name\n        else:\n            self._path = name\n\n        self._initializer = None\n        self._data_model = data_model\n        self._trainable = bool(trainable)\n        self._json = None\n\n        if in_stateless_scope():\n            if callable(initializer):\n                self._initializer = initializer\n                self._schema = standardize_schema(data_model.get_schema())\n                register_uninitialized_variable(self)\n            else:\n                raise ValueError(\n                    \"You are attempting to create a variable \"\n                    \"while in a stateless scope. This is disallowed. \"\n                    \"Make sure that all variables are created \"\n                    \"before you start using your module/program objects.\\n\\n\"\n                    \"In some cases, you might be seeing this error \"\n                    \"because you need to \"\n                    \"implement a `def build(self, input_schema)` method \"\n                    \"on your module/program, which will \"\n                    \"create its variables.\\n\\n\"\n                    \"In some other cases, you might be seeing this error \"\n                    \"because you are instantiating a `Variable` and \"\n                    \"assigning it to a module without going through \"\n                    \"self.add_variable(). Always prefer \"\n                    \"using these methods \"\n                    \"(with a `data_model` and `initializer` argument).\"\n                )\n        else:\n            if callable(initializer):\n                self._initialize_with_initializer(initializer)\n            else:\n                if data_model is None:\n                    raise ValueError(\n                        \"When creating a Variable from an a dict,\"\n                        \"the `data_model` argument should be specified. \"\n                        f\"Received: initializer={initializer} \"\n                        f\"and data_model={data_model}\"\n                    )\n                value = initializer\n                self._initialize(value)\n                self._schema = standardize_schema(data_model.get_schema())\n\n        if not description:\n            if \"description\" in self._schema:\n                self._description = self._schema[\"description\"]\n            else:\n                self._description = \"\"\n        else:\n            self._description = description\n\n    def _deferred_initialize(self):\n        \"\"\"Deferred initialization of the variable.\n\n        Raises:\n            ValueError: If the variable is already initialized or\n                if attempting to initialize while in a stateless scope.\n        \"\"\"\n        if self._json is not None:\n            raise ValueError(f\"Variable '{self._path}' is already initialized.\")\n\n        if in_stateless_scope():\n            raise ValueError(\n                \"You are attempting to initialize a variable \"\n                \"while in a stateless scope. This is disallowed. \"\n                \"Make sure that all variables are initialized \"\n                \"before you start using your module/program objects.\"\n            )\n        self._initialize_with_initializer(self._initializer)\n        self._initializer = None\n\n    def get_json(self):\n        \"\"\"The current value of the variable.\n\n        Returns:\n            (dict): The current value of the variable.\n        \"\"\"\n        if in_stateless_scope():\n            scope = get_stateless_scope()\n            value = scope.get_current_value(self)\n            if value is not None:\n                return value\n        if self._json is None:\n            # Uninitialized variable. Return a placeholder.\n            # This is fine because it's only ever used\n            # in during schema inference / graph tracing\n            # (anything else would be a bug, to be fixed.)\n            return self._initializer(data_model=self._data_model)\n        return self._json\n\n    def prettify_schema(self):\n        \"\"\"Get a pretty version of the JSON schema for display.\n\n        Returns:\n            (dict): The indented JSON schema.\n        \"\"\"\n        import orjson\n\n        return orjson.dumps(self.get_schema(), option=orjson.OPT_INDENT_2).decode()\n\n    def prettify_json(self):\n        \"\"\"Get a pretty version of the JSON object for display.\n\n        Returns:\n            (dict): The indented JSON object.\n        \"\"\"\n        import orjson\n\n        return orjson.dumps(self.get_json(), option=orjson.OPT_INDENT_2).decode()\n\n    def assign(self, value):\n        \"\"\"Assigns a new value to the variable.\n\n        Args:\n            value (dict | DataModel | JsonDataModel): The new value to be assigned.\n                The value can be an instanciated data model or JSON dict.\n\n        Returns:\n            (dict): The assigned value.\n\n        Raises:\n            ValueError: If the schema of the target variable and\n                the value are incompatible.\n        \"\"\"\n        if backend.is_data_model(value):\n            value = value.get_json()\n        if in_stateless_scope():\n            scope = get_stateless_scope()\n            scope.add_update((self, value))\n        else:\n            self._direct_assign(value)\n        return value\n\n    def _direct_assign(self, json):\n        \"\"\"Directly assigns a new value to the variable.\n\n        Args:\n            json (dict): The new json value to be assigned.\n        \"\"\"\n        self._json = json\n\n    def get_schema(self):\n        \"\"\"The schema of the variable.\n\n        Returns:\n            (dict): The JSON schema of the variable.\n        \"\"\"\n        return self._schema\n\n    def __contains__(self, other):\n        \"\"\"Check if the schema of `other` is contained within the schema of this variable,\n        or if a string key exists.\n\n        Args:\n            other (SymbolicDataModel | DataModel | str): The data model to compare\n                against this variable's schema, or a string key to check for in the\n                schema properties.\n\n        Returns:\n            (bool): True if `other`'s schema is a subset of this variable's schema,\n                or if the string key exists in the schema properties.\n        \"\"\"\n        if isinstance(other, str):\n            schema = self.get_schema()\n            return other in schema.get(\"properties\", {})\n        from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n        return contains_schema(self.get_schema(), other.get_schema())\n\n    @property\n    def trainable(self):\n        \"\"\"Whether the variable is trainable.\"\"\"\n        return self._trainable\n\n    @trainable.setter\n    def trainable(self, value):\n        self._trainable = bool(value)\n\n    @property\n    def name(self):\n        \"\"\"The name of the variable.\"\"\"\n        return self._name\n\n    @property\n    def description(self):\n        \"\"\"The description of the variable.\"\"\"\n        return self._description\n\n    @property\n    def path(self):\n        \"\"\"The path of the variable within the program or module.\"\"\"\n        return self._path\n\n    def __repr__(self):\n        json = None\n        if self._json is not None:\n            json = self._json\n        json_str = f\", json={json}\" if json is not None else \"\"\n        return f\"&lt;Variable path={self.path}, schema={self._schema}{json_str}&gt;\"\n\n    def _initialize_with_initializer(self, initializer):\n        \"\"\"Initializes the variable using an initializer object.\n\n        Args:\n            initializer (Initializer): The initializer to be used.\n        \"\"\"\n        value = initializer()\n        self._schema = standardize_schema(initializer.get_schema())\n        self._initialize(value)\n\n    def _initialize(self, json):\n        \"\"\"Initializes the variable with a given json dict.\n\n        Args:\n            json (dict): The initial value (JSON object dict).\n        \"\"\"\n        self._json = json\n\n    def to_json_data_model(self, name=None):\n        \"\"\"Convert the variable into a `JsonDataModel`.\n\n        Returns:\n            (JsonDataModel): The equivalent backend-independent data model\n        \"\"\"\n        if not name:\n            name = self.name\n        return JsonDataModel(\n            schema=self.get_schema(),\n            json=self.get_json(),\n            name=name,\n        )\n\n    def to_symbolic_data_model(self, name=None):\n        \"\"\"Convert the variable into a `SymbolicDataModel`.\n\n        Returns:\n            (SymbolicDataModel): The equivalent symbolic data model\n        \"\"\"\n        if not name:\n            name = self.name\n        return SymbolicDataModel(\n            schema=self.get_schema(),\n            name=name,\n        )\n\n    def get(self, key, default=None):\n        \"\"\"Get wrapper to make it easier to access JSON fields.\n\n        Args:\n            key (str): The key to access.\n            default (any): The default value if key not found.\n        \"\"\"\n        return self.get_json().get(key, default)\n\n    def __getitem__(self, key):\n        \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n        Args:\n            key (str): The key to access.\n        \"\"\"\n        return self.get_json()[key]\n\n    def keys(self):\n        \"\"\"Keys wrapper to make it easier to access JSON fields.\"\"\"\n        return self.get_json().keys()\n\n    def values(self):\n        \"\"\"Values wrapper to make it easier to access JSON fields.\"\"\"\n        return self.get_json().values()\n\n    def items(self):\n        \"\"\"Items wrapper to make it easier to access JSON fields.\"\"\"\n        return self.get_json().items()\n\n    def update(self, kv_dict):\n        \"\"\"Update wrapper to make easier to modify fields.\n\n        Args:\n            kv_dict (dict): The key/value dict to update.\n        \"\"\"\n        self._json.update(kv_dict)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.description","title":"<code>description</code>  <code>property</code>","text":"<p>The description of the variable.</p>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.name","title":"<code>name</code>  <code>property</code>","text":"<p>The name of the variable.</p>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.path","title":"<code>path</code>  <code>property</code>","text":"<p>The path of the variable within the program or module.</p>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.trainable","title":"<code>trainable</code>  <code>property</code> <code>writable</code>","text":"<p>Whether the variable is trainable.</p>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.__contains__","title":"<code>__contains__(other)</code>","text":"<p>Check if the schema of <code>other</code> is contained within the schema of this variable, or if a string key exists.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SymbolicDataModel | DataModel | str</code> <p>The data model to compare against this variable's schema, or a string key to check for in the schema properties.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>other</code>'s schema is a subset of this variable's schema, or if the string key exists in the schema properties.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def __contains__(self, other):\n    \"\"\"Check if the schema of `other` is contained within the schema of this variable,\n    or if a string key exists.\n\n    Args:\n        other (SymbolicDataModel | DataModel | str): The data model to compare\n            against this variable's schema, or a string key to check for in the\n            schema properties.\n\n    Returns:\n        (bool): True if `other`'s schema is a subset of this variable's schema,\n            or if the string key exists in the schema properties.\n    \"\"\"\n    if isinstance(other, str):\n        schema = self.get_schema()\n        return other in schema.get(\"properties\", {})\n    from synalinks.src.backend.common.json_schema_utils import contains_schema\n\n    return contains_schema(self.get_schema(), other.get_schema())\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get item wrapper to make it easier to access JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Get item wrapper to make it easier to access JSON fields.\n\n    Args:\n        key (str): The key to access.\n    \"\"\"\n    return self.get_json()[key]\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.assign","title":"<code>assign(value)</code>","text":"<p>Assigns a new value to the variable.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict | DataModel | JsonDataModel</code> <p>The new value to be assigned. The value can be an instanciated data model or JSON dict.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The assigned value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the schema of the target variable and the value are incompatible.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def assign(self, value):\n    \"\"\"Assigns a new value to the variable.\n\n    Args:\n        value (dict | DataModel | JsonDataModel): The new value to be assigned.\n            The value can be an instanciated data model or JSON dict.\n\n    Returns:\n        (dict): The assigned value.\n\n    Raises:\n        ValueError: If the schema of the target variable and\n            the value are incompatible.\n    \"\"\"\n    if backend.is_data_model(value):\n        value = value.get_json()\n    if in_stateless_scope():\n        scope = get_stateless_scope()\n        scope.add_update((self, value))\n    else:\n        self._direct_assign(value)\n    return value\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.get","title":"<code>get(key, default=None)</code>","text":"<p>Get wrapper to make it easier to access JSON fields.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to access.</p> required <code>default</code> <code>any</code> <p>The default value if key not found.</p> <code>None</code> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def get(self, key, default=None):\n    \"\"\"Get wrapper to make it easier to access JSON fields.\n\n    Args:\n        key (str): The key to access.\n        default (any): The default value if key not found.\n    \"\"\"\n    return self.get_json().get(key, default)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.get_json","title":"<code>get_json()</code>","text":"<p>The current value of the variable.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The current value of the variable.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def get_json(self):\n    \"\"\"The current value of the variable.\n\n    Returns:\n        (dict): The current value of the variable.\n    \"\"\"\n    if in_stateless_scope():\n        scope = get_stateless_scope()\n        value = scope.get_current_value(self)\n        if value is not None:\n            return value\n    if self._json is None:\n        # Uninitialized variable. Return a placeholder.\n        # This is fine because it's only ever used\n        # in during schema inference / graph tracing\n        # (anything else would be a bug, to be fixed.)\n        return self._initializer(data_model=self._data_model)\n    return self._json\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.get_schema","title":"<code>get_schema()</code>","text":"<p>The schema of the variable.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON schema of the variable.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def get_schema(self):\n    \"\"\"The schema of the variable.\n\n    Returns:\n        (dict): The JSON schema of the variable.\n    \"\"\"\n    return self._schema\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.items","title":"<code>items()</code>","text":"<p>Items wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def items(self):\n    \"\"\"Items wrapper to make it easier to access JSON fields.\"\"\"\n    return self.get_json().items()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.keys","title":"<code>keys()</code>","text":"<p>Keys wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def keys(self):\n    \"\"\"Keys wrapper to make it easier to access JSON fields.\"\"\"\n    return self.get_json().keys()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.prettify_json","title":"<code>prettify_json()</code>","text":"<p>Get a pretty version of the JSON object for display.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The indented JSON object.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def prettify_json(self):\n    \"\"\"Get a pretty version of the JSON object for display.\n\n    Returns:\n        (dict): The indented JSON object.\n    \"\"\"\n    import orjson\n\n    return orjson.dumps(self.get_json(), option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.prettify_schema","title":"<code>prettify_schema()</code>","text":"<p>Get a pretty version of the JSON schema for display.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The indented JSON schema.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def prettify_schema(self):\n    \"\"\"Get a pretty version of the JSON schema for display.\n\n    Returns:\n        (dict): The indented JSON schema.\n    \"\"\"\n    import orjson\n\n    return orjson.dumps(self.get_schema(), option=orjson.OPT_INDENT_2).decode()\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.to_json_data_model","title":"<code>to_json_data_model(name=None)</code>","text":"<p>Convert the variable into a <code>JsonDataModel</code>.</p> <p>Returns:</p> Type Description <code>JsonDataModel</code> <p>The equivalent backend-independent data model</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def to_json_data_model(self, name=None):\n    \"\"\"Convert the variable into a `JsonDataModel`.\n\n    Returns:\n        (JsonDataModel): The equivalent backend-independent data model\n    \"\"\"\n    if not name:\n        name = self.name\n    return JsonDataModel(\n        schema=self.get_schema(),\n        json=self.get_json(),\n        name=name,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.to_symbolic_data_model","title":"<code>to_symbolic_data_model(name=None)</code>","text":"<p>Convert the variable into a <code>SymbolicDataModel</code>.</p> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The equivalent symbolic data model</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def to_symbolic_data_model(self, name=None):\n    \"\"\"Convert the variable into a `SymbolicDataModel`.\n\n    Returns:\n        (SymbolicDataModel): The equivalent symbolic data model\n    \"\"\"\n    if not name:\n        name = self.name\n    return SymbolicDataModel(\n        schema=self.get_schema(),\n        name=name,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.update","title":"<code>update(kv_dict)</code>","text":"<p>Update wrapper to make easier to modify fields.</p> <p>Parameters:</p> Name Type Description Default <code>kv_dict</code> <code>dict</code> <p>The key/value dict to update.</p> required Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def update(self, kv_dict):\n    \"\"\"Update wrapper to make easier to modify fields.\n\n    Args:\n        kv_dict (dict): The key/value dict to update.\n    \"\"\"\n    self._json.update(kv_dict)\n</code></pre>"},{"location":"Synalinks%20API/Data%20Models%20API/The%20Variable%20class/#synalinks.src.backend.common.variables.Variable.values","title":"<code>values()</code>","text":"<p>Values wrapper to make it easier to access JSON fields.</p> Source code in <code>synalinks/src/backend/common/variables.py</code> <pre><code>def values(self):\n    \"\"\"Values wrapper to make it easier to access JSON fields.\"\"\"\n    return self.get_json().values()\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/","title":"Hook API","text":""},{"location":"Synalinks%20API/Hooks%20API/#hook-api","title":"Hook API","text":"<p>A hook is an object that can perform various actions at the begining/end of a module's call.</p>"},{"location":"Synalinks%20API/Hooks%20API/#hooks-overview","title":"Hooks Overview","text":"<ul> <li>Base Hook class</li> <li>Logger hook</li> <li>Monitor hook</li> </ul>"},{"location":"Synalinks%20API/Hooks%20API/Base%20Hook%20class/","title":"Base Hook class","text":""},{"location":"Synalinks%20API/Hooks%20API/Base%20Hook%20class/#synalinks.src.hooks.hook.Hook","title":"<code>Hook</code>","text":"<p>Base hook class used to build new hooks.</p> <p>Hooks are callback-like objects that can be passed to the module's configuration, they intercept the <code>__call__()</code> method of the modules.</p> <p>They can be used to log the inputs/outputs of the modules as well as for streaming data or for observability.</p> <p>Attributes:</p> Name Type Description <code>params</code> <code>dict</code> <p>Hook parameters (eg. endpoint, log-level ...).</p> <code>module</code> <code>Module</code> <p>Instance of <code>Module</code>. Reference of the module being monitored.</p> Source code in <code>synalinks/src/hooks/hook.py</code> <pre><code>@synalinks_export(\"synalinks.hooks.Hook\")\nclass Hook:\n    \"\"\"Base hook class used to build new hooks.\n\n    Hooks are callback-like objects that can be passed to the module's configuration, they\n    intercept the `__call__()` method of the modules.\n\n    They can be used to log the inputs/outputs of the modules as well as for\n    streaming data or for observability.\n\n    Attributes:\n        params (dict): Hook parameters\n            (eg. endpoint, log-level ...).\n        module (Module): Instance of `Module`.\n            Reference of the module being monitored.\n    \"\"\"\n\n    def __init__(self):\n        self.params = None\n        self._module = None\n\n    def set_params(self, params):\n        self.params = params\n\n    def set_module(self, module):\n        self._module = module\n\n    @property\n    def module(self):\n        return self._module\n\n    @utils.default\n    def on_call_begin(\n        self,\n        call_id,\n        parent_call_id=None,\n        inputs=None,\n        kwargs=None,\n    ):\n        \"\"\"Called at the beginning of the module execution.\n\n        Args:\n            call_id (str): The id of the module's call\n            inputs (SymbolicDataModel | JsonDataModel | DataModel | list | dict | tuple):\n                The module's inputs. The outputs can be data models or lists,\n                dicts or tuples of data models.\n            kwargs (dict): The keyword arguments passed to the module's call.\n        \"\"\"\n        pass\n\n    @utils.default\n    def on_call_end(\n        self,\n        call_id,\n        parent_call_id=None,\n        outputs=None,\n        exception=None,\n    ):\n        \"\"\"Called at the end of the module execution.\n\n        Args:\n            call_id (str): The id of the module's call\n            outputs (SymbolicDataModel | JsonDataModel | DataModel | list | dict | tuple):\n                The module's outputs. The outputs can be data models or lists,\n                dicts or tuples of data models.\n            exception (str): Exception message if any.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Base%20Hook%20class/#synalinks.src.hooks.hook.Hook.on_call_begin","title":"<code>on_call_begin(call_id, parent_call_id=None, inputs=None, kwargs=None)</code>","text":"<p>Called at the beginning of the module execution.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>The id of the module's call</p> required <code>inputs</code> <code>SymbolicDataModel | JsonDataModel | DataModel | list | dict | tuple</code> <p>The module's inputs. The outputs can be data models or lists, dicts or tuples of data models.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>The keyword arguments passed to the module's call.</p> <code>None</code> Source code in <code>synalinks/src/hooks/hook.py</code> <pre><code>@utils.default\ndef on_call_begin(\n    self,\n    call_id,\n    parent_call_id=None,\n    inputs=None,\n    kwargs=None,\n):\n    \"\"\"Called at the beginning of the module execution.\n\n    Args:\n        call_id (str): The id of the module's call\n        inputs (SymbolicDataModel | JsonDataModel | DataModel | list | dict | tuple):\n            The module's inputs. The outputs can be data models or lists,\n            dicts or tuples of data models.\n        kwargs (dict): The keyword arguments passed to the module's call.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Base%20Hook%20class/#synalinks.src.hooks.hook.Hook.on_call_end","title":"<code>on_call_end(call_id, parent_call_id=None, outputs=None, exception=None)</code>","text":"<p>Called at the end of the module execution.</p> <p>Parameters:</p> Name Type Description Default <code>call_id</code> <code>str</code> <p>The id of the module's call</p> required <code>outputs</code> <code>SymbolicDataModel | JsonDataModel | DataModel | list | dict | tuple</code> <p>The module's outputs. The outputs can be data models or lists, dicts or tuples of data models.</p> <code>None</code> <code>exception</code> <code>str</code> <p>Exception message if any.</p> <code>None</code> Source code in <code>synalinks/src/hooks/hook.py</code> <pre><code>@utils.default\ndef on_call_end(\n    self,\n    call_id,\n    parent_call_id=None,\n    outputs=None,\n    exception=None,\n):\n    \"\"\"Called at the end of the module execution.\n\n    Args:\n        call_id (str): The id of the module's call\n        outputs (SymbolicDataModel | JsonDataModel | DataModel | list | dict | tuple):\n            The module's outputs. The outputs can be data models or lists,\n            dicts or tuples of data models.\n        exception (str): Exception message if any.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Logger/","title":"Logger","text":""},{"location":"Synalinks%20API/Hooks%20API/Logger/#synalinks.src.hooks.logger.Logger","title":"<code>Logger</code>","text":"<p>               Bases: <code>Hook</code></p> <p>Logger hook for logging module calls.</p> <p>This hook is set by default when you enables logging.</p> <p>Example:</p> <pre><code>import synalinks\n\nsynalinks.enable_logging()\n</code></pre> Source code in <code>synalinks/src/hooks/logger.py</code> <pre><code>@synalinks_export(\"synalinks.hooks.Logger\")\nclass Logger(Hook):\n    \"\"\"Logger hook for logging module calls.\n\n    This hook is set by default when you enables logging.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    synalinks.enable_logging()\n    ```\n    \"\"\"\n\n    def _maybe_setup_logger(self):\n        if not hasattr(self, \"logger\"):\n            self.logger = logging.getLogger(f\"synalinks.{self.module.name}\")\n\n    def _serialize_kwargs(self, kwargs):\n        \"\"\"Serialize kwargs to JSON-compatible format.\"\"\"\n        if not kwargs:\n            return {}\n        serialized = {}\n        for key, value in kwargs.items():\n            if key == \"training\":\n                serialized[key] = value\n            elif hasattr(value, \"get_json\"):\n                serialized[key] = value.get_json()\n            elif isinstance(value, (str, int, float, bool, list, dict, type(None))):\n                serialized[key] = value\n        return serialized\n\n    def on_call_begin(\n        self,\n        call_id,\n        parent_call_id=None,\n        inputs=None,\n        kwargs=None,\n    ):\n        self._maybe_setup_logger()\n        module_name = self.module.name\n        module_description = self.module.description\n        flatten_inputs = tree.flatten(inputs)\n        if any_symbolic_data_models(inputs):\n            data_models_schemas = [\n                dm.get_schema() for dm in flatten_inputs if dm is not None\n            ]\n            if data_models_schemas:\n                self.logger.debug(\n                    _SYMBOLIC_LOG_TEMPLATE.format(\n                        name=\"Symbolic Call Start\",\n                        call_id=call_id,\n                        parent_call_id=parent_call_id,\n                        module=str(self.module.__class__.__name__),\n                        module_name=module_name,\n                        module_description=module_description,\n                        data_model_schema=orjson.dumps(\n                            data_models_schemas,\n                            option=orjson.OPT_INDENT_2,\n                        ).decode(),\n                    ),\n                )\n        else:\n            data_models_jsons = [dm.get_json() for dm in flatten_inputs if dm is not None]\n            if data_models_jsons:\n                self.logger.info(\n                    _DATA_LOG_TEMPLATE.format(\n                        name=\"Call Start\",\n                        call_id=call_id,\n                        parent_call_id=parent_call_id,\n                        module=str(self.module.__class__.__name__),\n                        module_name=module_name,\n                        module_description=module_description,\n                        data_model_json=orjson.dumps(\n                            data_models_jsons,\n                            option=orjson.OPT_INDENT_2,\n                        ).decode(),\n                    )\n                )\n            # Log kwargs if no data models but kwargs present (e.g., Tool modules)\n            elif kwargs:\n                serialized_kwargs = self._serialize_kwargs(kwargs)\n                # Filter out 'training' from display\n                display_kwargs = {\n                    k: v for k, v in serialized_kwargs.items() if k != \"training\"\n                }\n                if display_kwargs:\n                    self.logger.info(\n                        _KWARGS_LOG_TEMPLATE.format(\n                            call_id=call_id,\n                            parent_call_id=parent_call_id,\n                            module=str(self.module.__class__.__name__),\n                            module_name=module_name,\n                            module_description=module_description,\n                            kwargs_json=orjson.dumps(\n                                display_kwargs,\n                                option=orjson.OPT_INDENT_2,\n                            ).decode(),\n                        )\n                    )\n\n    def on_call_end(\n        self,\n        call_id,\n        parent_call_id=None,\n        outputs=None,\n        exception=None,\n    ):\n        self._maybe_setup_logger()\n        module_name = self.module.name\n        module_description = self.module.description\n        if exception:\n            self.logger.error(\n                _EXCEPTION_TEMPLATE.format(\n                    call_id=call_id,\n                    parent_call_id=parent_call_id,\n                    exception=exception,\n                    module=str(self.module.__class__.__name__),\n                    module_name=module_name,\n                    module_description=module_description,\n                )\n            )\n        if not outputs:\n            return\n        flatten_outputs = tree.flatten(outputs)\n        if any_symbolic_data_models(outputs):\n            data_models_schemas = [\n                dm.get_schema() for dm in flatten_outputs if dm is not None\n            ]\n            if data_models_schemas:\n                self.logger.debug(\n                    _SYMBOLIC_LOG_TEMPLATE.format(\n                        name=\"Symbolic Call End\",\n                        call_id=call_id,\n                        parent_call_id=parent_call_id,\n                        module=str(self.module.__class__.__name__),\n                        module_name=module_name,\n                        module_description=module_description,\n                        data_model_schema=orjson.dumps(\n                            data_models_schemas,\n                            option=orjson.OPT_INDENT_2,\n                        ).decode(),\n                    ),\n                )\n        else:\n            data_models_jsons = [\n                dm.get_json() for dm in flatten_outputs if dm is not None\n            ]\n            if data_models_jsons:\n                self.logger.info(\n                    _DATA_LOG_TEMPLATE.format(\n                        name=\"Call End\",\n                        call_id=call_id,\n                        parent_call_id=parent_call_id,\n                        module=str(self.module.__class__.__name__),\n                        module_name=module_name,\n                        module_description=module_description,\n                        data_model_json=orjson.dumps(\n                            data_models_jsons,\n                            option=orjson.OPT_INDENT_2,\n                        ).decode(),\n                    )\n                )\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Monitor/","title":"Monitor","text":""},{"location":"Synalinks%20API/Hooks%20API/Monitor/#synalinks.src.hooks.monitor.Monitor","title":"<code>Monitor</code>","text":"<p>               Bases: <code>Hook</code></p> <p>Monitor hook for tracing module calls using MLflow.</p> <p>This hook creates MLflow spans for each module call, enabling distributed tracing and observability of your synalinks programs.</p> <p>You can enable monitoring for every module by using <code>synalinks.enable_observability()</code> at the beginning of your scripts.</p> <p>Parameters:</p> Name Type Description Default <code>tracking_uri</code> <code>str</code> <p>MLflow tracking server URI. If None, uses the value from <code>synalinks.enable_observability()</code> or the default (local ./mlruns directory or MLFLOW_TRACKING_URI env var).</p> <code>None</code> <code>experiment_name</code> <code>str</code> <p>Name of the MLflow experiment for tracing. If None, uses the value from <code>synalinks.enable_observability()</code> or defaults to \"synalinks_traces\".</p> <code>None</code> <p>Example:</p> <pre><code>import synalinks\n\n# Basic usage - uses local MLflow storage\nsynalinks.enable_observability()\n\n# With custom MLflow tracking server\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"my_traces\"\n)\n\n# Or create a Monitor hook directly with custom settings\nmonitor = synalinks.hooks.Monitor(\n    tracking_uri=\"http://localhost:5000\",\n    experiment_name=\"my_experiment\"\n)\n</code></pre> Source code in <code>synalinks/src/hooks/monitor.py</code> <pre><code>@synalinks_export(\"synalinks.hooks.Monitor\")\nclass Monitor(Hook):\n    \"\"\"Monitor hook for tracing module calls using MLflow.\n\n    This hook creates MLflow spans for each module call, enabling distributed\n    tracing and observability of your synalinks programs.\n\n    You can enable monitoring for every module by using\n    `synalinks.enable_observability()` at the beginning of your scripts.\n\n    Args:\n        tracking_uri (str): MLflow tracking server URI. If None, uses the\n            value from `synalinks.enable_observability()` or the default\n            (local ./mlruns directory or MLFLOW_TRACKING_URI env var).\n        experiment_name (str): Name of the MLflow experiment for tracing.\n            If None, uses the value from `synalinks.enable_observability()`\n            or defaults to \"synalinks_traces\".\n\n    Example:\n\n    ```python\n    import synalinks\n\n    # Basic usage - uses local MLflow storage\n    synalinks.enable_observability()\n\n    # With custom MLflow tracking server\n    synalinks.enable_observability(\n        tracking_uri=\"http://localhost:5000\",\n        experiment_name=\"my_traces\"\n    )\n\n    # Or create a Monitor hook directly with custom settings\n    monitor = synalinks.hooks.Monitor(\n        tracking_uri=\"http://localhost:5000\",\n        experiment_name=\"my_experiment\"\n    )\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        tracking_uri=None,\n        experiment_name=None,\n    ):\n        super().__init__()\n        if not MLFLOW_AVAILABLE:\n            raise ImportError(\n                \"mlflow is required for the Monitor hook. \"\n                \"Install it with: pip install mlflow\"\n            )\n\n        # Use provided values or fall back to global config\n        self.tracking_uri = tracking_uri or mlflow_tracking_uri()\n        self.experiment_name = experiment_name or mlflow_experiment_name()\n        self.call_start_times = {}\n        self._spans = {}\n        self.logger = logging.getLogger(__name__)\n        self._setup_done = False\n\n    def _setup_mlflow(self):\n        \"\"\"Configure MLflow tracking.\"\"\"\n        if self._setup_done:\n            return\n\n        if self.tracking_uri:\n            mlflow.set_tracking_uri(self.tracking_uri)\n\n        mlflow.set_experiment(self.experiment_name)\n        self._setup_done = True\n\n    def _serialize_data(self, data):\n        \"\"\"Serialize data models to JSON-compatible format.\"\"\"\n        flatten_data = tree.flatten(data)\n        is_symbolic = any_symbolic_data_models(data)\n\n        if is_symbolic:\n            serialized = [dm.get_schema() for dm in flatten_data if dm is not None]\n        else:\n            serialized = [dm.get_json() for dm in flatten_data if dm is not None]\n\n        return serialized, is_symbolic\n\n    def _get_span_type(self):\n        \"\"\"Determine the MLflow span type based on the module class.\"\"\"\n        if SpanType is None:\n            return None\n\n        module_class = self.module.__class__.__name__\n\n        # Map module types to MLflow span types\n        if module_class in (\"Generator\", \"ChainOfThought\", \"SelfCritique\"):\n            return SpanType.LLM\n        elif module_class in (\"FunctionCallingAgent\",):\n            return SpanType.AGENT\n        elif module_class in (\"EmbedKnowledge\", \"RetrieveKnowledge\", \"UpdateKnowledge\"):\n            return SpanType.RETRIEVER\n        elif module_class in (\"Tool\",):\n            return SpanType.TOOL\n        else:\n            return SpanType.CHAIN\n\n    async def _begin_span_async(\n        self,\n        call_id,\n        parent_call_id,\n        serialized_inputs,\n        serialized_kwargs,\n        is_symbolic,\n        span_name,\n        span_type,\n    ):\n        \"\"\"Async implementation of span creation.\"\"\"\n        global _GLOBAL_SPANS_REGISTRY\n\n        # Look up parent span from global registry for proper trace hierarchy\n        parent_span_obj = None\n        if parent_call_id and parent_call_id in _GLOBAL_SPANS_REGISTRY:\n            parent_span_obj = _GLOBAL_SPANS_REGISTRY[parent_call_id]\n\n        # Use start_span_no_context for manual lifecycle management\n        # This properly supports parent-child relationships\n        span = await asyncio.to_thread(\n            mlflow.start_span_no_context,\n            name=span_name,\n            span_type=span_type,\n            parent_span=parent_span_obj,\n        )\n\n        # Store in both local and global registry\n        self._spans[call_id] = span\n        _GLOBAL_SPANS_REGISTRY[call_id] = span\n\n        span.set_attributes(\n            {\n                \"synalinks.call_id\": call_id,\n                \"synalinks.parent_call_id\": parent_call_id or \"\",\n                \"synalinks.module\": str(self.module.__class__.__name__),\n                \"synalinks.module_name\": self.module.name or \"\",\n                \"synalinks.module_description\": self.module.description or \"\",\n                \"synalinks.is_symbolic\": is_symbolic,\n            }\n        )\n\n        # Set inputs as a dictionary (MLflow handles serialization)\n        inputs_dict = {\"data\": serialized_inputs}\n        if serialized_kwargs:\n            inputs_dict[\"kwargs\"] = serialized_kwargs\n        span.set_inputs(inputs_dict)\n\n        self.logger.debug(f\"Started span for call {call_id}: {span_name}\")\n\n    def on_call_begin(\n        self,\n        call_id,\n        parent_call_id=None,\n        inputs=None,\n        kwargs=None,\n    ):\n        \"\"\"Called when a module call begins.\"\"\"\n        self._setup_mlflow()\n        self.call_start_times[call_id] = time.time()\n\n        serialized_inputs, is_symbolic = self._serialize_data(inputs)\n\n        # Serialize kwargs if present (for modules that use keyword arguments)\n        serialized_kwargs = {}\n        if kwargs:\n            # Filter out non-serializable kwargs like 'training'\n            for key, value in kwargs.items():\n                if key == \"training\":\n                    serialized_kwargs[key] = value\n                elif hasattr(value, \"get_json\"):\n                    serialized_kwargs[key] = value.get_json()\n                elif isinstance(value, (str, int, float, bool, list, dict, type(None))):\n                    serialized_kwargs[key] = value\n\n        span_name = f\"{self.module.__class__.__name__}\"\n        if self.module.name:\n            span_name = f\"{span_name}:{self.module.name}\"\n\n        # Get the appropriate span type for this module\n        span_type = self._get_span_type()\n\n        run_maybe_nested(\n            self._begin_span_async(\n                call_id=call_id,\n                parent_call_id=parent_call_id,\n                serialized_inputs=serialized_inputs,\n                serialized_kwargs=serialized_kwargs,\n                is_symbolic=is_symbolic,\n                span_name=span_name,\n                span_type=span_type,\n            )\n        )\n\n    async def _end_span_async(\n        self,\n        call_id,\n        span,\n        serialized_outputs,\n        duration,\n        cost,\n        exception,\n    ):\n        \"\"\"Async implementation of span ending.\"\"\"\n        span.set_attributes(\n            {\n                \"synalinks.duration\": duration,\n                \"synalinks.success\": exception is None,\n                \"synalinks.cost\": cost or 0.0,\n            }\n        )\n\n        if exception:\n            span.set_attributes({\"synalinks.exception\": str(exception)})\n            # Add exception event for better visibility in MLflow UI\n            span.add_event(\n                mlflow.entities.SpanEvent(\n                    name=\"exception\",\n                    attributes={\n                        \"exception.type\": type(exception).__name__,\n                        \"exception.message\": str(exception),\n                    },\n                )\n            )\n            span.set_status(\"ERROR\")\n        else:\n            span.set_status(\"OK\")\n\n        # Set outputs as a dictionary (MLflow handles serialization)\n        span.set_outputs({\"data\": serialized_outputs})\n\n        await asyncio.to_thread(span.end)\n\n        success = exception is None\n        self.logger.debug(\n            f\"Ended span for call {call_id}, duration={duration:.3f}s, success={success}\"\n        )\n\n    def on_call_end(\n        self,\n        call_id,\n        parent_call_id=None,\n        outputs=None,\n        exception=None,\n    ):\n        \"\"\"Called when a module call ends.\"\"\"\n        global _GLOBAL_SPANS_REGISTRY\n\n        end_time = time.time()\n        start_time = self.call_start_times.pop(call_id, end_time)\n        duration = end_time - start_time\n\n        span = self._spans.pop(call_id, None)\n        # Also remove from global registry\n        _GLOBAL_SPANS_REGISTRY.pop(call_id, None)\n\n        if span is None:\n            self.logger.warning(f\"No span found for call_id {call_id}\")\n            return\n\n        serialized_outputs, _ = self._serialize_data(outputs)\n\n        cost = None\n        if self.module._get_call_context():\n            cost = self.module._get_call_context().cost\n\n        run_maybe_nested(\n            self._end_span_async(\n                call_id=call_id,\n                span=span,\n                serialized_outputs=serialized_outputs,\n                duration=duration,\n                cost=cost,\n                exception=exception,\n            )\n        )\n\n    def __del__(self):\n        \"\"\"Cleanup any open spans.\"\"\"\n        if hasattr(self, \"_spans\"):\n            for call_id, span in list(self._spans.items()):\n                try:\n                    span.end()\n                except Exception:\n                    pass\n            self._spans.clear()\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Monitor/#synalinks.src.hooks.monitor.Monitor.__del__","title":"<code>__del__()</code>","text":"<p>Cleanup any open spans.</p> Source code in <code>synalinks/src/hooks/monitor.py</code> <pre><code>def __del__(self):\n    \"\"\"Cleanup any open spans.\"\"\"\n    if hasattr(self, \"_spans\"):\n        for call_id, span in list(self._spans.items()):\n            try:\n                span.end()\n            except Exception:\n                pass\n        self._spans.clear()\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Monitor/#synalinks.src.hooks.monitor.Monitor.on_call_begin","title":"<code>on_call_begin(call_id, parent_call_id=None, inputs=None, kwargs=None)</code>","text":"<p>Called when a module call begins.</p> Source code in <code>synalinks/src/hooks/monitor.py</code> <pre><code>def on_call_begin(\n    self,\n    call_id,\n    parent_call_id=None,\n    inputs=None,\n    kwargs=None,\n):\n    \"\"\"Called when a module call begins.\"\"\"\n    self._setup_mlflow()\n    self.call_start_times[call_id] = time.time()\n\n    serialized_inputs, is_symbolic = self._serialize_data(inputs)\n\n    # Serialize kwargs if present (for modules that use keyword arguments)\n    serialized_kwargs = {}\n    if kwargs:\n        # Filter out non-serializable kwargs like 'training'\n        for key, value in kwargs.items():\n            if key == \"training\":\n                serialized_kwargs[key] = value\n            elif hasattr(value, \"get_json\"):\n                serialized_kwargs[key] = value.get_json()\n            elif isinstance(value, (str, int, float, bool, list, dict, type(None))):\n                serialized_kwargs[key] = value\n\n    span_name = f\"{self.module.__class__.__name__}\"\n    if self.module.name:\n        span_name = f\"{span_name}:{self.module.name}\"\n\n    # Get the appropriate span type for this module\n    span_type = self._get_span_type()\n\n    run_maybe_nested(\n        self._begin_span_async(\n            call_id=call_id,\n            parent_call_id=parent_call_id,\n            serialized_inputs=serialized_inputs,\n            serialized_kwargs=serialized_kwargs,\n            is_symbolic=is_symbolic,\n            span_name=span_name,\n            span_type=span_type,\n        )\n    )\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Monitor/#synalinks.src.hooks.monitor.Monitor.on_call_end","title":"<code>on_call_end(call_id, parent_call_id=None, outputs=None, exception=None)</code>","text":"<p>Called when a module call ends.</p> Source code in <code>synalinks/src/hooks/monitor.py</code> <pre><code>def on_call_end(\n    self,\n    call_id,\n    parent_call_id=None,\n    outputs=None,\n    exception=None,\n):\n    \"\"\"Called when a module call ends.\"\"\"\n    global _GLOBAL_SPANS_REGISTRY\n\n    end_time = time.time()\n    start_time = self.call_start_times.pop(call_id, end_time)\n    duration = end_time - start_time\n\n    span = self._spans.pop(call_id, None)\n    # Also remove from global registry\n    _GLOBAL_SPANS_REGISTRY.pop(call_id, None)\n\n    if span is None:\n        self.logger.warning(f\"No span found for call_id {call_id}\")\n        return\n\n    serialized_outputs, _ = self._serialize_data(outputs)\n\n    cost = None\n    if self.module._get_call_context():\n        cost = self.module._get_call_context().cost\n\n    run_maybe_nested(\n        self._end_span_async(\n            call_id=call_id,\n            span=span,\n            serialized_outputs=serialized_outputs,\n            duration=duration,\n            cost=cost,\n            exception=exception,\n        )\n    )\n</code></pre>"},{"location":"Synalinks%20API/Hooks%20API/Monitor/#synalinks.src.hooks.monitor.Span","title":"<code>Span</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Data model representing a span for module call tracing.</p> Source code in <code>synalinks/src/hooks/monitor.py</code> <pre><code>@synalinks_export(\"synalinks.callbacks.monitor.Span\")\nclass Span(DataModel):\n    \"\"\"Data model representing a span for module call tracing.\"\"\"\n\n    event: Literal[\"call_begin\", \"call_end\"]\n    is_symbolic: bool\n    call_id: str\n    parent_call_id: Optional[str]\n    module: str\n    module_name: str\n    module_description: str\n    timestamp: float\n    inputs: Optional[List[Dict[str, Any]]] = None\n    outputs: Optional[List[Dict[str, Any]]] = None\n    duration: Optional[float] = None\n    exception: Optional[str] = None\n    success: Optional[bool] = None\n    cost: Optional[float] = None\n</code></pre>"},{"location":"Synalinks%20API/Metrics/","title":"Metrics","text":""},{"location":"Synalinks%20API/Metrics/#metrics","title":"Metrics","text":"<p>A <code>Metric</code> is a function that is used to judge the performance of your program.</p> <p>Metric functions are similar to reward functions, except that the results from evaluating a metric are not used when training the program. Note that you may use any reward function as a metric.</p>"},{"location":"Synalinks%20API/Metrics/#metrics-overview","title":"Metrics Overview","text":"<ul> <li>Base Metric class</li> <li>Metric wrappers and reduction metrics</li> <li>Regression metrics</li> <li>FScore metrics</li> </ul>"},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/","title":"Base Metric class","text":""},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/#synalinks.src.metrics.metric.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>SynalinksSaveable</code></p> <p>Metric base class: all synalinks metrics inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>None</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/metric.py</code> <pre><code>@synalinks_export([\"synalinks.Metric\", \"synalinks.metrics.Metric\"])\nclass Metric(SynalinksSaveable):\n    \"\"\"Metric base class: all synalinks metrics inherit from this class.\n\n    Args:\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(self, name=None, in_mask=None, out_mask=None):\n        self.name = name or auto_name(self.__class__.__name__)\n        self._metrics = []\n        self._variables = []\n        self.in_mask = in_mask\n        self.out_mask = out_mask\n        self._tracker = Tracker(\n            {\n                \"variables\": (\n                    lambda x: isinstance(x, backend.Variable),\n                    self._variables,\n                ),\n                \"metrics\": (lambda x: isinstance(x, Metric), self._metrics),\n            }\n        )\n\n    def reset_state(self):\n        \"\"\"Reset all of the metric state variables.\n\n        This function is called between epochs/steps,\n        when a metric is evaluated during training.\n        \"\"\"\n        for v in self.variables:\n            initializer = initializers.Empty(data_model=v._data_model)\n            v.assign(initializer.get_json())\n\n    async def update_state(self, *args, **kwargs):\n        \"\"\"Accumulate statistics for the metric.\"\"\"\n        raise NotImplementedError\n\n    def stateless_update_state(self, metric_variables, *args, **kwargs):\n        if len(metric_variables) != len(self.variables):\n            raise ValueError(\n                \"Argument `metric_variables` must be a list of data_models \"\n                f\"corresponding 1:1 to {self.__class__.__name__}().variables. \"\n                f\"Received list with length {len(metric_variables)}, but \"\n                f\"expected {len(self.variables)} variables.\"\n            )\n        # Gather variable mapping\n        mapping = list(zip(self.variables, metric_variables))\n\n        # Call in stateless scope\n        with backend.StatelessScope(state_mapping=mapping) as scope:\n            self.update_state(*args, **kwargs)\n\n        # Gather updated variables\n        metric_variables = []\n        for v in self.variables:\n            new_v = scope.get_current_value(v)\n            if new_v is not None:\n                metric_variables.append(new_v)\n            else:\n                metric_variables.append(v)\n        return metric_variables\n\n    def result(self):\n        \"\"\"Compute the current metric value.\n\n        Returns:\n            (float | dict): A scalar, or a dictionary of scalars.\n        \"\"\"\n        raise NotImplementedError\n\n    def stateless_result(self, metric_variables):\n        if len(metric_variables) != len(self.variables):\n            raise ValueError(\n                \"Argument `metric_variables` must be a list of data_models \"\n                f\"corresponding 1:1 to {self.__class__.__name__}().variables. \"\n                f\"Received list with length {len(metric_variables)}, but \"\n                f\"expected {len(self.variables)} variables.\"\n            )\n        # Gather variable mapping\n        mapping = list(zip(self.variables, metric_variables))\n\n        # Call in stateless scope\n        with backend.StatelessScope(state_mapping=mapping):\n            res = self.result()\n        return res\n\n    def _obj_type(self):\n        return \"Metric\"\n\n    def add_variable(self, initializer=None, data_model=None, name=None):\n        if initializer is None:\n            initializer = initializers.Empty(data_model=data_model)\n        self._check_super_called()\n        with backend.name_scope(self.name.replace(\"/\", \"&gt;\"), caller=self):\n            initializer = initializer\n            variable = backend.Variable(\n                initializer=initializer,\n                data_model=data_model,\n                trainable=False,\n                name=name,\n            )\n        # Prevent double-tracking\n        self._tracker.add_to_store(\"variables\", variable)\n        return variable\n\n    @property\n    def variables(self):\n        variables = list(self._variables)\n        for metric in self._metrics:\n            variables.extend(metric.variables)\n        return variables\n\n    async def __call__(self, *args, **kwargs):\n        self._check_super_called()\n        await self.update_state(*args, **kwargs)\n        return self.result()\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        return {\n            \"in_mask\": self.in_mask,\n            \"out_mask\": self.out_mask,\n            \"name\": self.name,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Returns a metric instance from config.\n\n        Args:\n            config (dict): The config dict.\n\n        Returns:\n            (Metric): The metric instance.\n        \"\"\"\n        return cls(**config)\n\n    def __setattr__(self, name, value):\n        # Track Variables, Layers, Metrics\n        if hasattr(self, \"_tracker\"):\n            value = self._tracker.track(value)\n        return super().__setattr__(name, value)\n\n    def _check_super_called(self):\n        if not hasattr(self, \"_tracker\"):\n            raise RuntimeError(\n                \"You forgot to call `super().__init__()` \"\n                \"in the `__init__()` method. Go add it!\"\n            )\n\n    def __repr__(self):\n        return f\"&lt;{self.__class__.__name__} name={self.name}&gt;\"\n\n    def __str__(self):\n        return self.__repr__()\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/#synalinks.src.metrics.metric.Metric.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Returns a metric instance from config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The config dict.</p> required <p>Returns:</p> Type Description <code>Metric</code> <p>The metric instance.</p> Source code in <code>synalinks/src/metrics/metric.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Returns a metric instance from config.\n\n    Args:\n        config (dict): The config dict.\n\n    Returns:\n        (Metric): The metric instance.\n    \"\"\"\n    return cls(**config)\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/#synalinks.src.metrics.metric.Metric.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/metric.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    return {\n        \"in_mask\": self.in_mask,\n        \"out_mask\": self.out_mask,\n        \"name\": self.name,\n    }\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/#synalinks.src.metrics.metric.Metric.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset all of the metric state variables.</p> <p>This function is called between epochs/steps, when a metric is evaluated during training.</p> Source code in <code>synalinks/src/metrics/metric.py</code> <pre><code>def reset_state(self):\n    \"\"\"Reset all of the metric state variables.\n\n    This function is called between epochs/steps,\n    when a metric is evaluated during training.\n    \"\"\"\n    for v in self.variables:\n        initializer = initializers.Empty(data_model=v._data_model)\n        v.assign(initializer.get_json())\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/#synalinks.src.metrics.metric.Metric.result","title":"<code>result()</code>","text":"<p>Compute the current metric value.</p> <p>Returns:</p> Type Description <code>float | dict</code> <p>A scalar, or a dictionary of scalars.</p> Source code in <code>synalinks/src/metrics/metric.py</code> <pre><code>def result(self):\n    \"\"\"Compute the current metric value.\n\n    Returns:\n        (float | dict): A scalar, or a dictionary of scalars.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Base%20Metric%20class/#synalinks.src.metrics.metric.Metric.update_state","title":"<code>update_state(*args, **kwargs)</code>  <code>async</code>","text":"<p>Accumulate statistics for the metric.</p> Source code in <code>synalinks/src/metrics/metric.py</code> <pre><code>async def update_state(self, *args, **kwargs):\n    \"\"\"Accumulate statistics for the metric.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/","title":"FScore metrics","text":""},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.BinaryF1Score","title":"<code>BinaryF1Score</code>","text":"<p>               Bases: <code>BinaryFBetaScore</code></p> <p>Computes F-1 Score on binary structures.</p> <p>Formula:</p> <pre><code>f1_score = 2 * (precision * recall) / (precision + recall)\n</code></pre> <p>This is the harmonic mean of precision and recall. Its output range is <code>[0, 1]</code>. It operates at a field level and can be used for multi-class and multi-label classification.</p> <p>Each field of <code>y_true</code> and <code>y_pred</code> should booleans or floats between [0, 1]. If the fields are floats, it uses the threshold for deciding if the values are 0 or 1.</p> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>str</code> <p>Type of averaging to be performed across per-class results in the multi-class case. Acceptable values are <code>None</code>, <code>\"micro\"</code>, <code>\"macro\"</code> and <code>\"weighted\"</code>. Defaults to <code>None</code>. If <code>None</code>, no averaging is performed and <code>result()</code> will return the score for each class. If <code>\"micro\"</code>, compute metrics globally by counting the total true positives, false negatives and false positives. If <code>\"macro\"</code>, compute metrics for each label, and return their unweighted mean. This does not take label imbalance into account. If <code>\"weighted\"</code>, compute metrics for each label, and return their average weighted by support (the number of true instances for each label). This alters <code>\"macro\"</code> to account for label imbalance. It can result in an score that is not between precision and recall.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>(Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. Elements of <code>y_pred</code> and <code>y_true</code> greater than <code>threshold</code> are converted to be 1, and the rest 0.</p> <code>0.5</code> <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'binary_f1_score'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.BinaryF1Score\")\nclass BinaryF1Score(BinaryFBetaScore):\n    \"\"\"Computes F-1 Score on binary structures.\n\n    Formula:\n\n    ```python\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    ```\n\n    This is the harmonic mean of precision and recall.\n    Its output range is `[0, 1]`. It operates at a field level\n    and can be used for **multi-class and multi-label classification**.\n\n    Each field of `y_true` and `y_pred` should booleans or floats between [0, 1].\n    If the fields are floats, it uses the threshold for deciding\n    if the values are 0 or 1.\n\n    Args:\n        average (str): Type of averaging to be performed across per-class results\n            in the multi-class case.\n            Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n            `\"weighted\"`. Defaults to `None`.\n            If `None`, no averaging is performed and `result()` will return\n            the score for each class.\n            If `\"micro\"`, compute metrics globally by counting the total\n            true positives, false negatives and false positives.\n            If `\"macro\"`, compute metrics for each label,\n            and return their unweighted mean.\n            This does not take label imbalance into account.\n            If `\"weighted\"`, compute metrics for each label,\n            and return their average weighted by support\n            (the number of true instances for each label).\n            This alters `\"macro\"` to account for label imbalance.\n            It can result in an score that is not between precision and recall.\n        threshold (float): (Optional) Float representing the threshold for deciding\n            whether prediction values are 1 or 0. Elements of `y_pred` and `y_true`\n            greater than `threshold` are converted to be 1, and the rest 0.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        average=None,\n        threshold=0.5,\n        name=\"binary_f1_score\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            average=average,\n            beta=1.0,\n            threshold=threshold,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        base_config = super().get_config()\n        del base_config[\"beta\"]\n        return base_config\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.BinaryF1Score.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    base_config = super().get_config()\n    del base_config[\"beta\"]\n    return base_config\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.BinaryFBetaScore","title":"<code>BinaryFBetaScore</code>","text":"<p>               Bases: <code>FBetaScore</code></p> <p>Computes F-Beta score on binary structures.</p> <p>Formula:</p> <pre><code>b2 = beta ** 2\nf_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n</code></pre> <p>This is the weighted harmonic mean of precision and recall. Its output range is <code>[0, 1]</code>. It operates at a field level and can be used for multi-class and multi-label classification.</p> <p>Each field of <code>y_true</code> and <code>y_pred</code> should be booleans or floats between [0, 1]. If the fields are floats, it uses the threshold parameter for deciding if the values are 0 or 1.</p> <p>Example:</p> <pre><code>class MultiClassClassification(synalinks.DataModel):\n    label_1: bool = synalinks.Field(\n        description=\"The first label\",\n    )\n    label_2: bool = synalinks.Field(\n        description=\"The second label\",\n    )\n    label_3: bool = synalinks.Field(\n        description=\"The third label\",\n    )\n\n# OR you can also use floats between 0 and 1\n# The `Score`, enforce a float between 0.0 and 1.0 using constrained decoding\n\nclass MultiClassClassification(synalinks.DataModel):\n    label_1: synalinks.Score = synalinks.Field(\n        description=\"The first label\",\n    )\n    label_2: synalinks.Score = synalinks.Field(\n        description=\"The second label\",\n    )\n    label_3: synalinks.Score = synalinks.Field(\n        description=\"The third label\",\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>str</code> <p>Type of averaging to be performed across per-class results in the multi-class case. Acceptable values are <code>None</code>, <code>\"micro\"</code>, <code>\"macro\"</code> and <code>\"weighted\"</code>. Defaults to <code>None</code>. If <code>None</code>, no averaging is performed and <code>result()</code> will return the score for each class. If <code>\"micro\"</code>, compute metrics globally by counting the total true positives, false negatives and false positives. If <code>\"macro\"</code>, compute metrics for each label, and return their unweighted mean. This does not take label imbalance into account. If <code>\"weighted\"</code>, compute metrics for each label, and return their average weighted by support (the number of true instances for each label). This alters <code>\"macro\"</code> to account for label imbalance. It can result in an score that is not between precision and recall.</p> <code>None</code> <code>beta</code> <code>float</code> <p>Determines the weight of given to recall in the harmonic mean between precision and recall (see pseudocode equation above). Defaults to <code>1</code>.</p> <code>1.0</code> <code>threshold</code> <code>float</code> <p>(Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. Elements of <code>y_pred</code> and <code>y_true</code> greater than <code>threshold</code> are converted to be 1, and the rest 0.</p> <code>0.5</code> <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'binary_fbeta_score'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.BinaryFBetaScore\")\nclass BinaryFBetaScore(FBetaScore):\n    \"\"\"Computes F-Beta score on binary structures.\n\n    Formula:\n\n    ```python\n    b2 = beta ** 2\n    f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n    ```\n\n    This is the weighted harmonic mean of precision and recall.\n    Its output range is `[0, 1]`. It operates at a field level\n    and can be used for **multi-class and multi-label classification**.\n\n    Each field of `y_true` and `y_pred` should be booleans or floats between [0, 1].\n    If the fields are floats, it uses the threshold parameter for deciding\n    if the values are 0 or 1.\n\n    Example:\n\n    ```\n\n    class MultiClassClassification(synalinks.DataModel):\n        label_1: bool = synalinks.Field(\n            description=\"The first label\",\n        )\n        label_2: bool = synalinks.Field(\n            description=\"The second label\",\n        )\n        label_3: bool = synalinks.Field(\n            description=\"The third label\",\n        )\n\n    # OR you can also use floats between 0 and 1\n    # The `Score`, enforce a float between 0.0 and 1.0 using constrained decoding\n\n    class MultiClassClassification(synalinks.DataModel):\n        label_1: synalinks.Score = synalinks.Field(\n            description=\"The first label\",\n        )\n        label_2: synalinks.Score = synalinks.Field(\n            description=\"The second label\",\n        )\n        label_3: synalinks.Score = synalinks.Field(\n            description=\"The third label\",\n        )\n\n    ```\n\n    Args:\n        average (str): Type of averaging to be performed across per-class results\n            in the multi-class case.\n            Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n            `\"weighted\"`. Defaults to `None`.\n            If `None`, no averaging is performed and `result()` will return\n            the score for each class.\n            If `\"micro\"`, compute metrics globally by counting the total\n            true positives, false negatives and false positives.\n            If `\"macro\"`, compute metrics for each label,\n            and return their unweighted mean.\n            This does not take label imbalance into account.\n            If `\"weighted\"`, compute metrics for each label,\n            and return their average weighted by support\n            (the number of true instances for each label).\n            This alters `\"macro\"` to account for label imbalance.\n            It can result in an score that is not between precision and recall.\n        beta (float): Determines the weight of given to recall\n            in the harmonic mean between precision and recall (see pseudocode\n            equation above). Defaults to `1`.\n        threshold (float): (Optional) Float representing the threshold for deciding\n            whether prediction values are 1 or 0. Elements of `y_pred` and `y_true`\n            greater than `threshold` are converted to be 1, and the rest 0.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        average=None,\n        beta=1.0,\n        threshold=0.5,\n        name=\"binary_fbeta_score\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            average=average,\n            beta=beta,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n        if not isinstance(threshold, float):\n            raise ValueError(\n                \"Invalid `threshold` argument value. \"\n                \"It should be a Python float. \"\n                f\"Received: threshold={threshold} \"\n                f\"of type '{type(threshold)}'\"\n            )\n        if threshold &gt; 1.0 or threshold &lt;= 0.0:\n            raise ValueError(\n                \"Invalid `threshold` argument value. \"\n                \"It should verify 0 &lt; threshold &lt;= 1. \"\n                f\"Received: threshold={threshold}\"\n            )\n        self.threshold = threshold\n\n    async def update_state(self, y_true, y_pred):\n        y_pred = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_pred)\n        y_true = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_true)\n\n        if self.in_mask:\n            y_pred = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_true)\n        if self.out_mask:\n            y_pred = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_true)\n\n        def convert_to_binary(x):\n            if isinstance(x, bool):\n                return 1.0 if x is True else 0.0\n            elif isinstance(x, float):\n                return 1.0 if x &gt; self.threshold else 0.0\n            else:\n                raise ValueError(\n                    \"All `y_true` and y_pred` fields should be booleans or floats. \"\n                    \"Use `in_mask` or `out_mask` to remove the other fields.\"\n                )\n\n        y_true = tree.flatten(\n            tree.map_structure(lambda x: convert_to_binary(x), y_true.get_json())\n        )\n        y_pred = tree.flatten(\n            tree.map_structure(lambda x: convert_to_binary(x), y_pred.get_json())\n        )\n        y_true = np.convert_to_tensor(y_true)\n        y_pred = np.convert_to_tensor(y_pred)\n\n        true_positives = y_pred * y_true\n        false_positives = y_pred * (1 - y_true)\n        false_negatives = (1 - y_pred) * y_true\n        intermediate_weights = y_true\n\n        current_true_positives = self.state.get(\"true_positives\")\n        if current_true_positives:\n            true_positives = np.add(current_true_positives, true_positives)\n\n        current_false_positives = self.state.get(\"false_positives\")\n        if current_false_positives:\n            false_positives = np.add(current_false_positives, false_positives)\n\n        current_false_negatives = self.state.get(\"false_negatives\")\n        if current_false_negatives:\n            false_negatives = np.add(current_false_negatives, false_negatives)\n\n        current_intermediate_weights = self.state.get(\"intermediate_weights\")\n        if current_intermediate_weights:\n            intermediate_weights = np.add(\n                current_intermediate_weights, intermediate_weights\n            )\n\n        self.state.update(\n            {\n                \"true_positives\": true_positives.tolist(),\n                \"false_positives\": false_positives.tolist(),\n                \"false_negatives\": false_negatives.tolist(),\n                \"intermediate_weights\": intermediate_weights.tolist(),\n            }\n        )\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        config = {\n            \"beta\": self.beta,\n            \"threshold\": self.threshold,\n            \"name\": self.name,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.BinaryFBetaScore.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    config = {\n        \"beta\": self.beta,\n        \"threshold\": self.threshold,\n        \"name\": self.name,\n    }\n    base_config = super().get_config()\n    return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.F1Score","title":"<code>F1Score</code>","text":"<p>               Bases: <code>FBetaScore</code></p> <p>Computes F-1 Score.</p> <p>Formula:</p> <pre><code>f1_score = 2 * (precision * recall) / (precision + recall)\n</code></pre> <p>This is the harmonic mean of precision and recall. Its output range is <code>[0, 1]</code>. It operates at a word level and can be used for QA systems.</p> <p>If <code>y_true</code> and <code>y_pred</code> contains multiple fields The JSON object's fields are flattened and the score computed for each one independently before being averaged.</p> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>str</code> <p>Type of averaging to be performed across per-field results in the multi-field case. Acceptable values are <code>None</code>, <code>\"micro\"</code>, <code>\"macro\"</code> and <code>\"weighted\"</code>. Defaults to <code>None</code>. If <code>None</code>, no averaging is performed and <code>result()</code> will return the score for each class. If <code>\"micro\"</code>, compute metrics globally by counting the total true positives, false negatives and false positives. If <code>\"macro\"</code>, compute metrics for each label, and return their unweighted mean. This does not take label imbalance into account. If <code>\"weighted\"</code>, compute metrics for each label, and return their average weighted by support (the number of true instances for each label). This alters <code>\"macro\"</code> to account for label imbalance. It can result in an score that is not between precision and recall.</p> <code>None</code> <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'f1_score'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.F1Score\")\nclass F1Score(FBetaScore):\n    \"\"\"Computes F-1 Score.\n\n    Formula:\n\n    ```python\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    ```\n\n    This is the harmonic mean of precision and recall.\n    Its output range is `[0, 1]`. It operates at a word level\n    and can be used for **QA systems**.\n\n    If `y_true` and `y_pred` contains multiple fields\n    The JSON object's fields are flattened and the score\n    computed for each one independently before being averaged.\n\n    Args:\n        average (str): Type of averaging to be performed across per-field results\n            in the multi-field case.\n            Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n            `\"weighted\"`. Defaults to `None`.\n            If `None`, no averaging is performed and `result()` will return\n            the score for each class.\n            If `\"micro\"`, compute metrics globally by counting the total\n            true positives, false negatives and false positives.\n            If `\"macro\"`, compute metrics for each label,\n            and return their unweighted mean.\n            This does not take label imbalance into account.\n            If `\"weighted\"`, compute metrics for each label,\n            and return their average weighted by support\n            (the number of true instances for each label).\n            This alters `\"macro\"` to account for label imbalance.\n            It can result in an score that is not between precision and recall.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        average=None,\n        name=\"f1_score\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            average=average,\n            beta=1.0,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        base_config = super().get_config()\n        del base_config[\"beta\"]\n        return base_config\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.F1Score.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    base_config = super().get_config()\n    del base_config[\"beta\"]\n    return base_config\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.FBetaScore","title":"<code>FBetaScore</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Computes F-Beta score.</p> <p>Formula:</p> <pre><code>b2 = beta ** 2\nf_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n</code></pre> <p>This is the weighted harmonic mean of precision and recall. Its output range is <code>[0, 1]</code>. It operates at a word level and can be used for QA systems.</p> <p>If <code>y_true</code> and <code>y_pred</code> contains multiple fields The JSON object's fields are flattened and the score computed for each one independently.</p> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>str</code> <p>Type of averaging to be performed across per-field results in the multi-field case. Acceptable values are <code>None</code>, <code>\"micro\"</code>, <code>\"macro\"</code> and <code>\"weighted\"</code>. Defaults to <code>None</code>. If <code>None</code>, no averaging is performed and <code>result()</code> will return the score for each class. If <code>\"micro\"</code>, compute metrics globally by counting the total true positives, false negatives and false positives. If <code>\"macro\"</code>, compute metrics for each label, and return their unweighted mean. This does not take label imbalance into account. If <code>\"weighted\"</code>, compute metrics for each label, and return their average weighted by support (the number of true instances for each label). This alters <code>\"macro\"</code> to account for label imbalance. It can result in an score that is not between precision and recall.</p> <code>None</code> <code>beta</code> <code>float</code> <p>Determines the weight of given to recall in the harmonic mean between precision and recall (see pseudocode equation above). Defaults to <code>1</code>.</p> <code>1.0</code> <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'fbeta_score'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.FBetaScore\")\nclass FBetaScore(Metric):\n    \"\"\"Computes F-Beta score.\n\n    Formula:\n\n    ```python\n    b2 = beta ** 2\n    f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n    ```\n\n    This is the weighted harmonic mean of precision and recall.\n    Its output range is `[0, 1]`. It operates at a word level\n    and can be used for **QA systems**.\n\n    If `y_true` and `y_pred` contains multiple fields\n    The JSON object's fields are flattened and the score\n    computed for each one independently.\n\n    Args:\n        average (str): Type of averaging to be performed across per-field results\n            in the multi-field case.\n            Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n            `\"weighted\"`. Defaults to `None`.\n            If `None`, no averaging is performed and `result()` will return\n            the score for each class.\n            If `\"micro\"`, compute metrics globally by counting the total\n            true positives, false negatives and false positives.\n            If `\"macro\"`, compute metrics for each label,\n            and return their unweighted mean.\n            This does not take label imbalance into account.\n            If `\"weighted\"`, compute metrics for each label,\n            and return their average weighted by support\n            (the number of true instances for each label).\n            This alters `\"macro\"` to account for label imbalance.\n            It can result in an score that is not between precision and recall.\n        beta (float): Determines the weight of given to recall\n            in the harmonic mean between precision and recall (see pseudocode\n            equation above). Defaults to `1`.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        average=None,\n        beta=1.0,\n        name=\"fbeta_score\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n        if average not in (None, \"micro\", \"macro\", \"weighted\"):\n            raise ValueError(\n                \"Invalid `average` argument value. Expected one of: \"\n                \"[None, 'micro', 'macro', 'weighted']. \"\n                f\"Received: average={average}\"\n            )\n\n        if not isinstance(beta, float):\n            raise ValueError(\n                \"Invalid `beta` argument value. \"\n                \"It should be a Python float. \"\n                f\"Received: beta={beta} of type '{type(beta)}'\"\n            )\n        self.state = self.add_variable(\n            data_model=FBetaState,\n            name=\"state_\" + self.name,\n        )\n        self.average = average\n        self.beta = beta\n        self.axis = None\n        if self.average != \"micro\":\n            self.axis = 0\n\n    async def update_state(self, y_true, y_pred):\n        y_pred = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_pred)\n        y_true = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_true)\n\n        if self.in_mask:\n            y_pred = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_true)\n        if self.out_mask:\n            y_pred = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_true)\n\n        y_true = tree.flatten(tree.map_structure(lambda x: str(x), y_true.get_json()))\n        y_pred = tree.flatten(tree.map_structure(lambda x: str(x), y_pred.get_json()))\n\n        true_positives = []\n        false_positives = []\n        false_negatives = []\n        intermediate_weights = []\n        # For each field of y_true and y_pred\n        for yt, yp in zip(y_true, y_pred):\n            y_true_tokens = nlp_utils.normalize_and_tokenize(str(yt))\n            y_pred_tokens = nlp_utils.normalize_and_tokenize(str(yp))\n            common_tokens = set(y_true_tokens) &amp; set(y_pred_tokens)\n            true_positives.append(len(common_tokens))\n            false_positives.append(len(y_pred_tokens) - len(common_tokens))\n            false_negatives.append(len(y_true_tokens) - len(common_tokens))\n            intermediate_weights.append(len(y_true_tokens))\n\n        true_positives = np.convert_to_numpy(true_positives)\n        false_positives = np.convert_to_numpy(false_positives)\n        false_negatives = np.convert_to_numpy(false_negatives)\n        intermediate_weights = np.convert_to_numpy(intermediate_weights)\n\n        current_true_positives = self.state.get(\"true_positives\")\n        if current_true_positives:\n            true_positives = np.add(current_true_positives, true_positives)\n\n        current_false_positives = self.state.get(\"false_positives\")\n        if current_false_positives:\n            false_positives = np.add(current_false_positives, false_positives)\n\n        current_false_negatives = self.state.get(\"false_negatives\")\n        if current_false_negatives:\n            false_negatives = np.add(current_false_negatives, false_negatives)\n\n        current_intermediate_weights = self.state.get(\"intermediate_weights\")\n        if current_intermediate_weights:\n            intermediate_weights = np.add(\n                current_intermediate_weights, intermediate_weights\n            )\n\n        self.state.update(\n            {\n                \"true_positives\": true_positives.tolist(),\n                \"false_positives\": false_positives.tolist(),\n                \"false_negatives\": false_negatives.tolist(),\n                \"intermediate_weights\": intermediate_weights.tolist(),\n            }\n        )\n\n    def result(self):\n        if (\n            self.state.get(\"true_positives\") is None\n            and self.state.get(\"false_positives\") is None\n            and self.state.get(\"false_negatives\") is None\n        ):\n            return 0.0\n        precision = np.divide(\n            self.state.get(\"true_positives\"),\n            np.add(\n                self.state.get(\"true_positives\"),\n                self.state.get(\"false_positives\"),\n            )\n            + backend.epsilon(),\n        )\n        recall = np.divide(\n            self.state.get(\"true_positives\"),\n            np.add(\n                self.state.get(\"true_positives\"),\n                self.state.get(\"false_negatives\"),\n            )\n            + backend.epsilon(),\n        )\n        precision = np.convert_to_tensor(precision)\n        recall = np.convert_to_tensor(recall)\n\n        mul_value = precision * recall\n        add_value = ((self.beta**2) * precision) + recall\n        mean = np.divide(mul_value, add_value + backend.epsilon())\n        f1_score = mean * (1 + (self.beta**2))\n        if self.average == \"weighted\":\n            intermediate_weights = self.state.get(\"intermediate_weights\")\n            weights = np.divide(\n                intermediate_weights,\n                np.sum(intermediate_weights) + backend.epsilon(),\n            )\n            f1_score = np.sum(f1_score * weights)\n\n        elif self.average is not None:  # [micro, macro]\n            f1_score = np.mean(f1_score, self.axis)\n\n        try:\n            return float(f1_score)\n        except Exception:\n            return list(f1_score)\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        config = {\n            \"name\": self.name,\n            \"beta\": self.beta,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.FBetaScore.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    config = {\n        \"name\": self.name,\n        \"beta\": self.beta,\n    }\n    base_config = super().get_config()\n    return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.ListF1Score","title":"<code>ListF1Score</code>","text":"<p>               Bases: <code>ListFBetaScore</code></p> <p>Computes F-1 Score on list structures.</p> <p>Formula: </p><pre><code>    f1_score = 2 * (precision * recall) / (precision + recall)\n</code></pre><p></p> <p>This is the harmonic mean of precision and recall. Its output range is <code>[0, 1]</code>. It operates at a field level and can be used for classification or retrieval pipelines.</p> <p>The difference between this metric and the <code>F1Score</code> is that this one consider each element of the list (or the string) as one label.</p> <p>This metric works using list or string structures like in the following example:</p> <p>Example:</p> <pre><code>    # for single label classification\n\n    class ListClassification(synalinks.DataModel):\n        label: Literal[\"label\", \"label_1\", \"label_2\"]\n\n    # for multi label classification\n\n    class ListClassification(synalinks.DataModel):\n        labels: List[Literal[\"label\", \"label_1\", \"label_2\"]]\n\n    # or use it with retrieval pipelines, in that case make sure to mask\n    # the correct fields.\n\n    class AnswerWithReferences(synalinks.DataModel):\n        sources: List[str]\n        answer: str\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>average</code> <code>str</code> <p>Type of averaging to be performed across per-field results in the multi-field case. Acceptable values are <code>None</code>, <code>\"micro\"</code>, <code>\"macro\"</code> and <code>\"weighted\"</code>. Defaults to <code>None</code>. If <code>None</code>, no averaging is performed and <code>result()</code> will return the score for each class. If <code>\"micro\"</code>, compute metrics globally by counting the total true positives, false negatives and false positives. If <code>\"macro\"</code>, compute metrics for each label, and return their unweighted mean. This does not take label imbalance into account. If <code>\"weighted\"</code>, compute metrics for each label, and return their average weighted by support (the number of true instances for each label). This alters <code>\"macro\"</code> to account for label imbalance. It can result in an score that is not between precision and recall.</p> <code>None</code> <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'list_f1_score'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.ListF1Score\")\nclass ListF1Score(ListFBetaScore):\n    \"\"\"Computes F-1 Score on list structures.\n\n    Formula:\n    ```python\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    ```\n\n    This is the harmonic mean of precision and recall.\n    Its output range is `[0, 1]`. It operates at a field level\n    and can be used for **classification** or **retrieval pipelines**.\n\n    The difference between this metric and the `F1Score` is that this one consider\n    each element of the list (or the string) as **one label**.\n\n    This metric works using list or string structures like in the following example:\n\n    Example:\n\n    ```python\n\n        # for single label classification\n\n        class ListClassification(synalinks.DataModel):\n            label: Literal[\"label\", \"label_1\", \"label_2\"]\n\n        # for multi label classification\n\n        class ListClassification(synalinks.DataModel):\n            labels: List[Literal[\"label\", \"label_1\", \"label_2\"]]\n\n        # or use it with retrieval pipelines, in that case make sure to mask\n        # the correct fields.\n\n        class AnswerWithReferences(synalinks.DataModel):\n            sources: List[str]\n            answer: str\n    ```\n\n    Args:\n        average (str): Type of averaging to be performed across per-field results\n            in the multi-field case.\n            Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n            `\"weighted\"`. Defaults to `None`.\n            If `None`, no averaging is performed and `result()` will return\n            the score for each class.\n            If `\"micro\"`, compute metrics globally by counting the total\n            true positives, false negatives and false positives.\n            If `\"macro\"`, compute metrics for each label,\n            and return their unweighted mean.\n            This does not take label imbalance into account.\n            If `\"weighted\"`, compute metrics for each label,\n            and return their average weighted by support\n            (the number of true instances for each label).\n            This alters `\"macro\"` to account for label imbalance.\n            It can result in an score that is not between precision and recall.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        average=None,\n        name=\"list_f1_score\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            average=average,\n            beta=1.0,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        base_config = super().get_config()\n        del base_config[\"beta\"]\n        return base_config\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.ListF1Score.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    base_config = super().get_config()\n    del base_config[\"beta\"]\n    return base_config\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.ListFBetaScore","title":"<code>ListFBetaScore</code>","text":"<p>               Bases: <code>FBetaScore</code></p> <p>Computes F-Beta score on list structures.</p> <p>Formula:</p> <pre><code>b2 = beta ** 2\nf_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n</code></pre> <p>This is the weighted harmonic mean of precision and recall. Its output range is <code>[0, 1]</code>. It operates at a field level and can be used for classification or retrieval pipelines.</p> <p>The difference between this metric and the <code>F1Score</code> is that this one consider each element of the list (or the string) as one label.</p> <p>This metric works using list or string structures like in the following example:</p> <p>Example:</p> <pre><code># for single label classification\n\nclass ListClassification(synalinks.DataModel):\n    label: Literal[\"label\", \"label_1\", \"label_2\"]\n\n# for multi label classification\n\nclass ListClassification(synalinks.DataModel):\n    labels: List[Literal[\"label\", \"label_1\", \"label_2\"]]\n\n# or use it with retrieval pipelines, in that case make sure to mask\n# the correct fields.\n\nclass AnswerWithReferences(synalinks.DataModel):\n    sources: List[str]\n    answer: str\n</code></pre> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.ListFBetaScore\")\nclass ListFBetaScore(FBetaScore):\n    \"\"\"Computes F-Beta score on list structures.\n\n    Formula:\n\n    ```python\n    b2 = beta ** 2\n    f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n    ```\n\n    This is the weighted harmonic mean of precision and recall.\n    Its output range is `[0, 1]`. It operates at a field level\n    and can be used for **classification** or **retrieval pipelines**.\n\n    The difference between this metric and the `F1Score` is that this one consider\n    each element of the list (or the string) as **one label**.\n\n    This metric works using list or string structures like in the following example:\n\n    Example:\n\n    ```python\n\n    # for single label classification\n\n    class ListClassification(synalinks.DataModel):\n        label: Literal[\"label\", \"label_1\", \"label_2\"]\n\n    # for multi label classification\n\n    class ListClassification(synalinks.DataModel):\n        labels: List[Literal[\"label\", \"label_1\", \"label_2\"]]\n\n    # or use it with retrieval pipelines, in that case make sure to mask\n    # the correct fields.\n\n    class AnswerWithReferences(synalinks.DataModel):\n        sources: List[str]\n        answer: str\n\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        average=None,\n        beta=1.0,\n        name=\"list_fbeta_score\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            average=average,\n            beta=beta,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n\n    async def update_state(self, y_true, y_pred):\n        y_pred = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_pred)\n        y_true = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_true)\n\n        if self.in_mask:\n            y_pred = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_true)\n        if self.out_mask:\n            y_pred = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_true)\n\n        y_true = tree.flatten(tree.map_structure(lambda x: x, y_true.get_json()))\n        y_pred = tree.flatten(tree.map_structure(lambda x: x, y_pred.get_json()))\n\n        true_positives = []\n        false_positives = []\n        false_negatives = []\n        intermediate_weights = []\n\n        for yt, yp in zip(y_true, y_pred):\n            y_true_tokens = (\n                [str(tok) for tok in y_true] if isinstance(y_true, list) else [yt]\n            )\n            y_pred_tokens = (\n                [str(tok) for tok in y_pred] if isinstance(y_pred, list) else [yp]\n            )\n            common_tokens = set(y_true_tokens) &amp; set(y_pred_tokens)\n            true_positives.append(len(common_tokens))\n            false_positives.append(len(y_pred_tokens) - len(common_tokens))\n            false_negatives.append(len(y_true_tokens) - len(common_tokens))\n            intermediate_weights.append(len(y_true_tokens))\n\n        true_positives = np.convert_to_numpy(true_positives)\n        false_positives = np.convert_to_numpy(false_positives)\n        false_negatives = np.convert_to_numpy(false_negatives)\n        intermediate_weights = np.convert_to_numpy(intermediate_weights)\n\n        current_true_positives = self.state.get(\"true_positives\")\n        if current_true_positives:\n            true_positives = np.add(current_true_positives, true_positives)\n\n        current_false_positives = self.state.get(\"false_positives\")\n        if current_false_positives:\n            false_positives = np.add(current_false_positives, false_positives)\n\n        current_false_negatives = self.state.get(\"false_negatives\")\n        if current_false_negatives:\n            false_negatives = np.add(current_false_negatives, false_negatives)\n\n        current_intermediate_weights = self.state.get(\"intermediate_weights\")\n        if current_intermediate_weights:\n            intermediate_weights = np.add(\n                current_intermediate_weights, intermediate_weights\n            )\n\n        self.state.update(\n            {\n                \"true_positives\": true_positives.tolist(),\n                \"false_positives\": false_positives.tolist(),\n                \"false_negatives\": false_negatives.tolist(),\n                \"intermediate_weights\": intermediate_weights.tolist(),\n            }\n        )\n\n    def get_config(self):\n        \"\"\"Return the serializable config of the metric.\n\n        Returns:\n            (dict): The config dict.\n        \"\"\"\n        config = {\n            \"beta\": self.beta,\n            \"name\": self.name,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/FScore%20metrics/#synalinks.src.metrics.f_score_metrics.ListFBetaScore.get_config","title":"<code>get_config()</code>","text":"<p>Return the serializable config of the metric.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The config dict.</p> Source code in <code>synalinks/src/metrics/f_score_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Return the serializable config of the metric.\n\n    Returns:\n        (dict): The config dict.\n    \"\"\"\n    config = {\n        \"beta\": self.beta,\n        \"name\": self.name,\n    }\n    base_config = super().get_config()\n    return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Metric%20wrappers%20and%20reduction%20metrics/","title":"Metric wrappers and reduction metrics","text":""},{"location":"Synalinks%20API/Metrics/Metric%20wrappers%20and%20reduction%20metrics/#synalinks.src.metrics.reduction_metrics.Mean","title":"<code>Mean</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Compute the mean of the given values.</p> <p>For example, if values is <code>[1, 3, 5, 7]</code> then the mean is 4.</p> <p>This metric creates two variables, <code>total</code> and <code>count</code>. The mean value returned is simply <code>total</code> divided by <code>count</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'mean'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; m = Mean()\n&gt;&gt;&gt; m.update_state([1, 3, 5, 7])\n&gt;&gt;&gt; m.result()\n4.0\n</code></pre> Source code in <code>synalinks/src/metrics/reduction_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.Mean\")\nclass Mean(Metric):\n    \"\"\"Compute the mean of the given values.\n\n    For example, if values is `[1, 3, 5, 7]` then the mean is 4.\n\n    This metric creates two variables, `total` and `count`.\n    The mean value returned is simply `total` divided by `count`.\n\n    Args:\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n\n    Example:\n\n    ```python\n    &gt;&gt;&gt; m = Mean()\n    &gt;&gt;&gt; m.update_state([1, 3, 5, 7])\n    &gt;&gt;&gt; m.result()\n    4.0\n    ```\n    \"\"\"\n\n    def __init__(self, name=\"mean\", in_mask=None, out_mask=None):\n        super().__init__(name=name, in_mask=in_mask, out_mask=out_mask)\n        self.total_with_count = self.add_variable(\n            data_model=TotalWithCount, name=\"total_with_count\"\n        )\n\n    async def update_state(self, values):\n        values = reduce_to_samplewise_values(values, reduce_fn=numpy.mean)\n        total = self.total_with_count.get(\"total\")\n        self.total_with_count.update({\"total\": float(total + numpy.sum(values))})\n        if len(values.shape) &gt;= 1:\n            num_samples = numpy.shape(values)[0]\n        else:\n            num_samples = 1\n        count = self.total_with_count.get(\"count\")\n        self.total_with_count.update({\"count\": int(count + num_samples)})\n\n    def reset_state(self):\n        self.total_with_count.assign(TotalWithCount())\n\n    def result(self):\n        return float(\n            numpy.divide_no_nan(\n                self.total_with_count.get(\"total\"),\n                self.total_with_count.get(\"count\"),\n            )\n        )\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Metric%20wrappers%20and%20reduction%20metrics/#synalinks.src.metrics.reduction_metrics.MeanMetricWrapper","title":"<code>MeanMetricWrapper</code>","text":"<p>               Bases: <code>Mean</code></p> <p>Wrap a stateless metric function with the <code>Mean</code> metric.</p> <p>You could use this class to quickly build a mean metric from a function. The function needs to have the signature <code>fn(y_true, y_pred)</code> and return a per-sample reward array. <code>MeanMetricWrapper.result()</code> will return the average metric value across all samples seen so far.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>callable</code> <p>The metric function to wrap, with signature <code>fn(y_true, y_pred, **kwargs)</code>.</p> required <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>None</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Keyword arguments to pass on to <code>fn</code>.</p> <code>{}</code> Source code in <code>synalinks/src/metrics/reduction_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.MeanMetricWrapper\")\nclass MeanMetricWrapper(Mean):\n    \"\"\"Wrap a stateless metric function with the `Mean` metric.\n\n    You could use this class to quickly build a mean metric from a function. The\n    function needs to have the signature `fn(y_true, y_pred)` and return a\n    per-sample reward array. `MeanMetricWrapper.result()` will return\n    the average metric value across all samples seen so far.\n\n    Args:\n        fn (callable): The metric function to wrap, with signature\n            `fn(y_true, y_pred, **kwargs)`.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n        **kwargs (keyword arguments): Keyword arguments to pass on to `fn`.\n    \"\"\"\n\n    def __init__(self, fn, name=None, in_mask=None, out_mask=None, **kwargs):\n        super().__init__(\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n        self._fn = fn\n        self._fn_kwargs = kwargs\n\n        # If we are wrapping a Synalinks reward, register the metric's\n        # direction as \"up\" (needs to be maximized during training).\n        if (\n            self._fn in rewards.ALL_OBJECTS\n            or hasattr(self._fn, \"__class__\")\n            and self._fn.__class__ in rewards.ALL_OBJECTS\n        ):\n            self._direction = \"up\"\n\n    async def update_state(self, y_true, y_pred):\n        y_pred = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_pred)\n        y_true = tree.map_structure(lambda x: ops.convert_to_json_data_model(x), y_true)\n        if self.in_mask:\n            y_pred = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.in_mask(mask=self.in_mask), y_true)\n        if self.out_mask:\n            y_pred = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_pred)\n            y_true = tree.map_structure(lambda x: x.out_mask(mask=self.out_mask), y_true)\n        values = await self._fn(y_true, y_pred, **self._fn_kwargs)\n        return await super().update_state(values)\n\n    def get_config(self):\n        \"\"\"Returns the serializable config of the metric.\"\"\"\n        base_config = super().get_config()\n        config = {\n            \"fn\": serialization_lib.serialize_synalinks_object(self._fn),\n        }\n        config.update(self._fn_kwargs)\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        fn = serialization_lib.deserialize_synalinks_object(config.pop(\"fn\"))\n        return cls(fn=fn, **config)\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Metric%20wrappers%20and%20reduction%20metrics/#synalinks.src.metrics.reduction_metrics.MeanMetricWrapper.get_config","title":"<code>get_config()</code>","text":"<p>Returns the serializable config of the metric.</p> Source code in <code>synalinks/src/metrics/reduction_metrics.py</code> <pre><code>def get_config(self):\n    \"\"\"Returns the serializable config of the metric.\"\"\"\n    base_config = super().get_config()\n    config = {\n        \"fn\": serialization_lib.serialize_synalinks_object(self._fn),\n    }\n    config.update(self._fn_kwargs)\n    return {**base_config, **config}\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Metric%20wrappers%20and%20reduction%20metrics/#synalinks.src.metrics.reduction_metrics.Sum","title":"<code>Sum</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Compute the (weighted) sum of the given values.</p> <p>For example, if <code>values</code> is <code>[1, 3, 5, 7]</code> then their sum is 16.</p> <p>This metric creates one variable, <code>total</code>. This is ultimately returned as the sum value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'sum'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; m = metrics.Sum()\n&gt;&gt;&gt; m.update_state([1, 3, 5, 7])\n&gt;&gt;&gt; m.result()\n16.0\n</code></pre> Source code in <code>synalinks/src/metrics/reduction_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.Sum\")\nclass Sum(Metric):\n    \"\"\"Compute the (weighted) sum of the given values.\n\n    For example, if `values` is `[1, 3, 5, 7]` then their sum is 16.\n\n    This metric creates one variable, `total`.\n    This is ultimately returned as the sum value.\n\n    Args:\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n\n    Example:\n\n    ```python\n    &gt;&gt;&gt; m = metrics.Sum()\n    &gt;&gt;&gt; m.update_state([1, 3, 5, 7])\n    &gt;&gt;&gt; m.result()\n    16.0\n    ```\n    \"\"\"\n\n    def __init__(self, name=\"sum\", in_mask=None, out_mask=None):\n        super().__init__(name=name, in_mask=in_mask, out_mask=out_mask)\n        self.total = self.add_variable(\n            data_model=Total,\n            name=\"total\",\n        )\n\n    async def update_state(self, values):\n        values = reduce_to_samplewise_values(values, reduce_fn=numpy.sum)\n        total = self.total.get(\"total\")\n        self.total.update({\"total\": float(numpy.sum(total, values))})\n\n    def reset_state(self):\n        self.total.assign(Total())\n\n    def result(self):\n        return self.total.get(\"total\")\n</code></pre>"},{"location":"Synalinks%20API/Metrics/Regression%20metrics/","title":"Regression metrics","text":""},{"location":"Synalinks%20API/Metrics/Regression%20metrics/#synalinks.src.metrics.regression_metrics.CosineSimilarity","title":"<code>CosineSimilarity</code>","text":"<p>               Bases: <code>MeanMetricWrapper</code></p> <p>Computes the cosine similarity between the labels and predictions.</p> <p>Formula:</p> <pre><code>metric = (sum(l2_norm(y_true) * l2_norm(y_pred))+1) / 2\n</code></pre> <p>The formula is similar to the classic cosine similarity used in deep learning, but scaled to [0.0, 1.0] and adjusted to have a reward that tend towards 1.0 if the two objects are similar (and 0.0 otherwise).</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use to compute the cosine similarity.</p> <code>None</code> <code>axis</code> <code>int</code> <p>(Optional) Defaults to <code>-1</code>. The dimension along which the cosine similarity is computed.</p> <code>-1</code> <code>name</code> <code>str</code> <p>(Optional) string name of the metric instance.</p> <code>'cosine_similarity'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the metric.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the metric.</p> <code>None</code> Source code in <code>synalinks/src/metrics/regression_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.metrics.CosineSimilarity\")\nclass CosineSimilarity(MeanMetricWrapper):\n    \"\"\"Computes the cosine similarity between the labels and predictions.\n\n    Formula:\n\n    ```python\n    metric = (sum(l2_norm(y_true) * l2_norm(y_pred))+1) / 2\n    ```\n\n    The formula is similar to the classic cosine similarity used in deep learning,\n    but scaled to [0.0, 1.0] and adjusted to have a reward that tend\n    towards 1.0 if the two objects are similar (and 0.0 otherwise).\n\n    Args:\n        embedding_model (EmbeddingModel): The embedding model to use to compute the\n            cosine similarity.\n        axis (int): (Optional) Defaults to `-1`. The dimension along which the cosine\n            similarity is computed.\n        name (str): (Optional) string name of the metric instance.\n        in_mask (list): (Optional) list of keys to keep to compute the metric.\n        out_mask (list): (Optional) list of keys to remove to compute the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        name=\"cosine_similarity\",\n        axis=-1,\n        embedding_model=None,\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            fn=cosine_similarity,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n        self.axis = axis\n        self.embedding_model = embedding_model\n        self._fn_kwargs = {\"axis\": axis, \"embedding_model\": embedding_model}\n\n    def get_config(self):\n        config = {\n            \"axis\": self.axis,\n            \"name\": self.name,\n            \"in_mask\": self.in_mask,\n            \"out_mask\": self.out_mask,\n        }\n        embedding_model_config = {\n            \"embedding_model\": serialization_lib.serialize_synalinks_object(\n                self.embedding_model\n            )\n        }\n        return {**embedding_model_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        embedding_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"embedding_model\")\n        )\n        return cls(embedding_model=embedding_model, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/","title":"Modules","text":""},{"location":"Synalinks%20API/Modules%20API/#modules-api","title":"Modules API","text":"<p>Modules are the basic building blocks of programs in Synalinks. A <code>Module</code> consists of data model-in &amp; data model-out computation function (the module's <code>call()</code> method) and some state (held in <code>Variable</code>).</p> <p>A module instance is a callable, much like a function:</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking\",\n    )\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n\nasync def main():\n    language_model = LanguageModel(\n        model=\"ollama/deepseek-r1\"\n    )\n\n    generator = synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n    )\n\n    inputs = Query(query=\"What is the capital of France?\")\n    outputs = await generator(inputs)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/#modules-api-overview","title":"Modules API overview","text":"<ul> <li>Base Module class</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#core-modules","title":"Core Modules","text":"<ul> <li>Input module</li> <li>Identity module</li> <li>Not module</li> <li>Generator module</li> <li>Decision module</li> <li>Action module</li> <li>Branch module</li> <li>Tool module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#agents-modules","title":"Agents Modules","text":"<ul> <li>FunctionCallingAgent module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#masking-modules","title":"Masking Modules","text":"<ul> <li>InMask module</li> <li>OutMask module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#merging-modules","title":"Merging Modules","text":"<ul> <li>Concat module</li> <li>And module</li> <li>Or module</li> <li>Xor module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#test-time-compute-modules","title":"Test Time Compute Modules","text":"<ul> <li>ChainOfThought module</li> <li>SelfCritique module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#knowledge-modules","title":"Knowledge Modules","text":"<ul> <li>EmbedKnowledge module</li> <li>UpdateKnowledge module</li> <li>RetrieveKnowledge module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/#synthesis-modules","title":"Synthesis Modules","text":"<ul> <li>PythonSynthesis module</li> <li>SequentialPlanSynthesis module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/","title":"Base Module class","text":"<p>               Bases: <code>PydanticModule</code>, <code>Operation</code>, <code>SynalinksSaveable</code></p> <p>This is the class from which all modules inherit.</p> <p>A module is a callable object that takes as input one or more data models and that outputs one or more data models. It involves computation, defined in the <code>call()</code> method, and a state (the variables). State can be created:</p> <ul> <li>in <code>__init__()</code>, for instance via <code>self.add_variable()</code>;</li> <li>in the optional <code>build()</code> method, which is invoked by the first   <code>__call__()</code> to the module, and supplies the schema(s) of the input(s),   which may not have been known at initialization time.</li> </ul> <p>Modules are recursively composable: If you assign a Module instance as an attribute of another Module, the outer Module will start tracking the variables created by the inner module. Nested modules should be instantiated in the <code>__init__()</code> method or <code>build()</code> method.</p> <p>Users will just instantiate a module and then treat it as a callable.</p> <p>Parameters:</p> Name Type Description Default <code>trainable</code> <code>bool</code> <p>Boolean, whether the module's variables should be trainable.</p> <code>True</code> <code>name</code> <code>str</code> <p>String name of the module.</p> <code>None</code> <p>We recommend that descendants of <code>Module</code> implement the following methods:</p> <ul> <li><code>__init__()</code>: Defines custom modules attributes, and creates module variables     that do not depend on input schemas, using <code>add_variable()</code>,     or other state.</li> <li><code>build(self, input_schema)</code>: This method can be used to create variables that     depend on the schemas(s) of the input(s), using <code>add_variable()</code>, or other     state. <code>__call__()</code> will automatically build the module     (if it has not been built yet) by calling <code>build()</code>.</li> <li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making     sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying     the module to the input arguments.     Two reserved keyword arguments you can optionally use in <code>call()</code> are:         1. <code>training</code> (boolean, whether the call is in inference mode or             training mode).     A typical signature for this method is <code>call(self, inputs)</code>, and user     could optionally add <code>training</code> if the module need it.</li> <li><code>get_config(self)</code>: Returns a dictionary containing the configuration     used to initialize this module. If the keys differ from the arguments     in <code>__init__()</code>, then override <code>from_config(self)</code> as well.     This method is used when saving     the module or a program that contains this module.</li> </ul> Source code in <code>synalinks/src/modules/module.py</code> <pre><code>@synalinks_export([\"synalinks.Module\", \"synalinks.modules.Module\"])\nclass Module(BackendModule, Operation, SynalinksSaveable):\n    \"\"\"This is the class from which all modules inherit.\n\n    A module is a callable object that takes as input one or more data models and\n    that outputs one or more data models. It involves *computation*, defined\n    in the `call()` method, and a *state* (the variables). State can be\n    created:\n\n    * in `__init__()`, for instance via `self.add_variable()`;\n    * in the optional `build()` method, which is invoked by the first\n      `__call__()` to the module, and supplies the schema(s) of the input(s),\n      which may not have been known at initialization time.\n\n    Modules are recursively composable: If you assign a Module instance as an\n    attribute of another Module, the outer Module will start tracking the variables\n    created by the inner module. Nested modules should be instantiated in the\n    `__init__()` method or `build()` method.\n\n    Users will just instantiate a module and then treat it as a callable.\n\n    Args:\n        trainable (bool): Boolean, whether the module's variables should be trainable.\n        name (str): String name of the module.\n\n    We recommend that descendants of `Module` implement the following methods:\n\n    * `__init__()`: Defines custom modules attributes, and creates module variables\n        that do not depend on input schemas, using `add_variable()`,\n        or other state.\n    * `build(self, input_schema)`: This method can be used to create variables that\n        depend on the schemas(s) of the input(s), using `add_variable()`, or other\n        state. `__call__()` will automatically build the module\n        (if it has not been built yet) by calling `build()`.\n    * `call(self, *args, **kwargs)`: Called in `__call__` after making\n        sure `build()` has been called. `call()` performs the logic of applying\n        the module to the input arguments.\n        Two reserved keyword arguments you can optionally use in `call()` are:\n            1. `training` (boolean, whether the call is in inference mode or\n                training mode).\n        A typical signature for this method is `call(self, inputs)`, and user\n        could optionally add `training` if the module need it.\n    * `get_config(self)`: Returns a dictionary containing the configuration\n        used to initialize this module. If the keys differ from the arguments\n        in `__init__()`, then override `from_config(self)` as well.\n        This method is used when saving\n        the module or a program that contains this module.\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        obj = super().__new__(cls)  # , *args, **kwargs)\n\n        # Wrap the user-provided `build` method in the `build_wrapper`\n        # to add name scope support and serialization support.\n        original_build_method = obj.build\n\n        @wraps(original_build_method)\n        async def build_wrapper(*args, **kwargs):\n            with obj._open_name_scope():\n                obj._path = current_path()\n                await original_build_method(*args, **kwargs)\n            # Record build config.\n            signature = inspect.signature(original_build_method)\n            obj._build_schemas_dict = signature.bind(*args, **kwargs).arguments\n            # Set built, post build actions, and lock state.\n            obj.built = True\n            obj._post_build()\n            obj._lock_state()\n\n        obj.build = build_wrapper\n        return obj\n\n    def __init__(\n        self,\n        *,\n        trainable=True,\n        name=None,\n        description=None,\n        hooks=None,\n        **kwargs,\n    ):\n        BackendModule.__init__(self)\n        self._lock = False\n        Operation.__init__(self, name=name, description=description)\n        if kwargs:\n            raise ValueError(\n                \"Unrecognized keyword arguments \"\n                f\"passed to {self.__class__.__name__}: {kwargs}\"\n            )\n        self._path = None  # Will be determined in `build_wrapper`\n        self.built = False\n        self._input_spec = None\n        self._called = False\n\n        self._trainable = trainable\n        self._rewards = []\n        self._reward_ids = set()\n        self._rewards_override = []\n\n        self._call_signature = inspect.signature(self.call)\n        call_signature_parameters = [\n            p.name for p in self._call_signature.parameters.values()\n        ]\n        self._call_has_training_arg = \"training\" in call_signature_parameters\n        # Whether to automatically convert inputs to `call()`.\n        self._convert_input_args = True\n        # Whether to allow non-json object as positional arguments in `call()`.\n        self._allow_non_json_data_model_positional_args = False\n        # Dict of schemas that were used to call `build()`.\n        self._build_schemas_dict = None\n        # Parent path\n        self._parent_path = None\n        self._hooks = HookList(\n            hooks=hooks,\n            module=self,\n        )\n        self._initialize_tracker()\n\n    @tracking.no_automatic_dependency_tracking\n    def _initialize_tracker(self):\n        if hasattr(self, \"_tracker\"):\n            return\n\n        trainable_variables = []\n        non_trainable_variables = []\n        modules = []\n        metrics = []\n        self._tracker = tracking.Tracker(\n            {\n                \"trainable_variables\": (\n                    lambda x: isinstance(x, backend.Variable) and x.trainable,\n                    trainable_variables,\n                ),\n                \"non_trainable_variables\": (\n                    lambda x: isinstance(x, backend.Variable) and not x.trainable,\n                    non_trainable_variables,\n                ),\n                \"metrics\": (lambda x: isinstance(x, Metric), metrics),\n                \"modules\": (\n                    lambda x: isinstance(x, Module) and not isinstance(x, Metric),\n                    modules,\n                ),\n            },\n            exclusions={\"non_trainable_variables\": [\"trainable_variables\"]},\n        )\n\n        self._trainable_variables = trainable_variables\n        self._non_trainable_variables = non_trainable_variables\n        self._modules = modules\n        self._metrics = metrics\n\n    @property\n    def path(self):\n        \"\"\"The path of the module.\n\n        If the module has not been built yet, it will be `None`.\n        \"\"\"\n        return self._path\n\n    @property\n    def input_spec(self):\n        return self._input_spec\n\n    @input_spec.setter\n    def input_spec(self, value):\n        self._input_spec = value\n\n    @classmethod\n    def get_arity(cls):\n        # Inspect the call method to get the number of parameters\n        sig = inspect.signature(cls.call)\n        # Exclude 'self' and 'training' from the parameter count\n        return len(\n            [\n                param\n                for param in sig.parameters.values()\n                if param.name not in [\"self\", \"training\"]\n            ]\n        )\n\n    @python_utils.default\n    async def build(self, *args, **kwargs):\n        self._check_super_called()\n        if utils.is_default(self.build) and might_have_unbuilt_state(self):\n            try:\n                if isinstance(args, tuple):\n                    args = list(args)\n                if len(args) == 1 or backend.is_data_model(args[0]):\n                    args[0] = args[0].to_symbolic_data_model()\n                else:\n                    args = tree.map_structure(\n                        lambda x: (\n                            x.to_symbolic_data_model() if backend.is_data_model(x) else x\n                        ),\n                        args,\n                    )\n                await self.__call__(*args, **kwargs)\n            except Exception as e:\n                warnings.warn(\n                    f\"`build()` was called on module '{self.name}', however \"\n                    \"the module does not have a `build()` method implemented \"\n                    \"and it looks like it has unbuilt state. This will cause \"\n                    \"the module to be marked as built, despite not being \"\n                    \"actually built, which may cause failures down the line. \"\n                    \"Make sure to implement a proper `build()` method.\"\n                    f\"Exception encountered: ''{e}''\"\n                )\n        self.built = True\n\n    def _lock_state(self):\n        \"\"\"Prevent further state updates, called automatically in `build()`.\"\"\"\n        if not self._tracker.locked:\n            self._tracker.lock(\n                msg=(\n                    \"You cannot add new elements of state \"\n                    \"(variables or sub-modules) \"\n                    \"to a module that is already built. All state \"\n                    \"must be created in the `__init__()` method or \"\n                    \"in the `build()` method.\"\n                )\n            )\n\n    def get_build_config(self):\n        \"\"\"Returns a dictionary with the modules's input schema.\n\n        This method returns a config dict that can be used by\n        `build_from_config(config)` to create all states (e.g. Variables and\n        Lookup tables) needed by the module.\n\n        By default, the config only contains the input schema that the module\n        was built with. If you're writing a custom module that creates state in\n        an unusual way, you should override this method to make sure this state\n        is already created when Synalinks attempts to load its value upon model\n        loading.\n\n        Returns:\n            (dict): A dict containing the input schema associated with the module.\n        \"\"\"\n        if self._build_schemas_dict is not None:\n            if len(self._build_schemas_dict) == 1:\n                return {\n                    \"input_schema\": tuple(self._build_schemas_dict.values())[0],\n                }\n            else:\n                return {\"schemas_dict\": self._build_schemas_dict}\n\n    def build_from_config(self, config):\n        \"\"\"Builds the module's states with the supplied config dict.\n\n        By default, this method calls the `build()` method,\n        which creates variables based on the module's input schema in the supplied\n        config. If your config contains other information needed to load the\n        module's state, you should override this method.\n\n        Args:\n            config (dict): Dict containing the input schema associated with this module.\n        \"\"\"\n        if config:\n            if \"input_schema\" in config:\n                run_maybe_nested(\n                    self.build(backend.SymbolicDataModel(schema=config[\"input_schema\"]))\n                )\n            elif \"schemas_dict\" in config:\n                symbolic_inputs = {}\n                for key, schema in config[\"schemas_dict\"].items():\n                    symbolic_inputs[key] = backend.SymbolicDataModel(schema=schema)\n                run_maybe_nested(self.build(**symbolic_inputs))\n            self.built = True\n\n    def _obj_type(self):\n        return \"Module\"\n\n    @property\n    def metrics(self):\n        \"\"\"List of all metrics.\"\"\"\n        metrics = list(self._metrics)\n        for module in self._modules:\n            metrics.extend(module.metrics)\n        return metrics\n\n    @property\n    def metrics_variables(self):\n        \"\"\"List of all metric variables.\"\"\"\n        vars = []\n        for metric in self.metrics:\n            vars.extend(metric.variables)\n        return vars\n\n    def _get_own_rewards(self):\n        if backend.in_stateless_scope():\n            rewards = []\n            scope = backend.get_stateless_scope()\n            for reward in scope.rewards:\n                if id(reward) in self._reward_ids:\n                    rewards.append(reward)\n            return rewards\n        else:\n            return self._rewards[:]\n\n    @property\n    def rewards(self):\n        \"\"\"List of scalar rewards from `add_reward` and submodules.\"\"\"\n        if self._rewards_override:\n            return self._rewards_override\n        rewards = self._get_own_rewards()\n        for module in self._flatten_modules(include_self=False):\n            rewards.extend(module._get_own_rewards())\n        return rewards\n\n    def _clear_rewards(self):\n        if backend.in_stateless_scope():\n            scope = backend.get_stateless_scope()\n            if scope.collect_rewards:\n                for x in scope.rewards:\n                    if id(x) in self._reward_ids:\n                        scope.rewards.remove(x)\n        self._rewards.clear()\n        self._reward_ids.clear()\n        for module in self._modules:\n            module._clear_rewards()\n\n    def add_variable(\n        self,\n        initializer=None,\n        data_model=None,\n        trainable=True,\n        name=None,\n        description=None,\n    ):\n        \"\"\"Add a variable to the module\n\n        Args:\n            initializer (dict | Initializer): Initializer object to use to\n                populate the initial variable value. Can be a JSON dict containing the\n                initial value. If unspecified, defaults to `initializers.Empty`.\n            data_model (DataModel): The DataModel used to infer the schema\n                and default value.\n            trainable (bool): Boolean, whether the variable should be trainable via\n                optimization or whether its updates are managed manually. Defaults\n                to `True`.\n            name (string): String name of the variable. Useful for debugging purposes.\n            description (string): String description of the variable. Used by the\n                optimizers to infer the role of the variable. Required if the data\n                model do not have a docstring.\n\n        Returns:\n            (Variable): The created variable\n        \"\"\"\n        self._check_super_called()\n        if initializer is None:\n            initializer = initializers.Empty(data_model=data_model)\n        trainable = trainable and is_trainable(data_model)\n        with backend.name_scope(self.name, caller=self):\n            variable = backend.Variable(\n                initializer=initializer,\n                data_model=data_model,\n                trainable=trainable,\n                name=name,\n                description=description,\n            )\n        self._track_variable(variable)\n        return variable\n\n    @property\n    def trainable(self):\n        \"\"\"Settable boolean, whether this module should be trainable or not.\"\"\"\n        return self._trainable\n\n    @trainable.setter\n    def trainable(self, value):\n        \"\"\"Sets trainable attribute for the module and its submodules.\n\n        When this value is changed during training (e.g. with a\n        `Callback`) you need to call the parent\n        `Program.make_train_function` with `force=True` in order to\n        recompile the training graph.\n\n        Args:\n            value (bool): Boolean with the desired state for the module's trainable\n                attribute.\n        \"\"\"\n        value = bool(value)\n        self._trainable = value\n        for v in self._trainable_variables:\n            v.trainable = value\n        for module in self._modules:\n            module.trainable = value\n\n    def add_hook(self, hook):\n        self._hooks.add_hook(hook)\n\n    @property\n    def variables(self):\n        \"\"\"List of all module state.\n\n        Note that metrics variables are not included here, use\n        `metrics_variables` to visit all the metric variables.\n\n        Returns:\n            (list): The list of the variables.\n        \"\"\"\n        # Return all `Variables` associate with the module including metrics\n        # and random seeds. Also deduplicate them.\n        variables = []\n        seen_ids = set()\n        for v in self._trainable_variables + self._non_trainable_variables:\n            if id(v) not in seen_ids:\n                variables.append(v)\n                seen_ids.add(id(v))\n        for module in self._modules:\n            for v in module.variables:\n                if id(v) not in seen_ids:\n                    variables.append(v)\n                    seen_ids.add(id(v))\n        return variables\n\n    @property\n    def trainable_variables(self):\n        \"\"\"List of all trainable module state.\n\n        Returns:\n            (list): The list of trainable variables.\n        \"\"\"\n        if not self.trainable:\n            return []\n        return [v for v in self.variables if v.trainable]\n\n    @property\n    def non_trainable_variables(self):\n        \"\"\"List of all non-trainable module state.\n\n        Returns:\n            (list): The list of non-trainable variables.\n        \"\"\"\n        if not self.trainable:\n            return self.variables\n        return [v for v in self.variables if not v.trainable]\n\n    def get_variable(self, name=None, index=None):\n        \"\"\"Retrieves a variable based on either its name (unique) or index.\n\n        If `name` and `index` are both provided, `index` will take precedence.\n        Indices are based on order of instantiation.\n\n        Args:\n            name (string): The name of the variable.\n            index (int): The index of the variable.\n\n        Returns:\n            (Variable): The returned variable.\n        \"\"\"\n        if index is not None and name is not None:\n            raise ValueError(\n                \"Provide only a variable name or a variable index. Received: \"\n                f\"index={index}, name={name}.\"\n            )\n        if index is not None:\n            if len(self.variables) &lt;= index:\n                raise ValueError(\n                    f\"Was asked to retrieve variable at index {index}\"\n                    f\" but module only has {len(self.modules)}\"\n                    \" variables.\"\n                )\n            else:\n                return self.variables[index]\n\n        if name is not None:\n            for variable in self.variables:\n                if variable.name == name:\n                    return variable\n            raise ValueError(\n                f\"No such variable: {name}. Existing variables are: \"\n                f\"{list(variable.name for variable in self.variables)}.\"\n            )\n        raise ValueError(\n            \"Provide either a variable name or variable index at `get_variable`.\"\n        )\n\n    async def __call__(self, *args, **kwargs):\n        call_id = str(uuid.uuid4())\n\n        self._check_super_called()\n        self._called = True\n\n        call_context = self._get_call_context()\n\n        parent_call_id = call_context.call_id if call_context.call_id else None\n\n        call_context.call_id = call_id\n\n        if self._hooks:\n            self._hooks.on_call_begin(\n                call_id=call_id,\n                parent_call_id=parent_call_id,\n                inputs=args,\n                kwargs=kwargs,\n            )\n\n        #####################################\n        # 0. Convert tuple inputs to list for convenience\n        if isinstance(args, tuple):\n            args = list(args)\n\n        #####################################\n        # 1. Convert any DataModel positional arguments to JsonDataModel\n        # This operation is performed to make the computation backend independent\n        # and the making the data models dynamically modifiable\n\n        # Used to avoid expensive `tree` operations in the most common case.\n        if len(args) == 1 and backend.is_data_model(args[0]):\n            args[0] = args[0].to_json_data_model(name=\"inputs_\" + self.name)\n        else:\n            args = self._maybe_convert_inputs(args)\n\n        ##########################################################\n        # 2. Enforce that only JsonDataModels or SymbolicDataModel\n        # can be passed positionally.\n        if not self._allow_non_json_data_model_positional_args:\n            for arg in tree.flatten(args):\n                if not is_json_data_model_or_symbolic_data_model(arg) and arg is not None:\n                    raise ValueError(\n                        \"Only input JsonDataModel, DataModel or SymbolicDataModel\"\n                        \" may be passed as positional arguments. The following argument \"\n                        f\"value should be passed as a keyword argument: {arg} \"\n                        f\"(of type {type(arg)})\"\n                    )\n\n        # Caches info about `call()` signature, args, kwargs.\n        call_spec = CallSpec(self._call_signature, args, kwargs)\n\n        ############################################\n        # 3. Check input spec for 1st positional arg.\n        # TODO: consider extending this to all args and kwargs.\n        self._assert_input_compatibility(call_spec.first_arg)\n\n        ################\n        # 4. Call build\n        with self._open_name_scope():\n            await self._maybe_build(call_spec)\n\n        ##########################\n        # 5. Infer training value\n        # Training phase for `Module.call` is set via (in order of priority):\n        # (1) The `training` argument passed to this `Module.call`, if not None\n        # (2) The training argument of an outer `Module.call`.\n        # (4) Any non-None default value for `training` in the call signature\n        # (5) False (treating the module as if it's in inference)\n\n        # This is the value explicitly passed by the user\n        training = call_spec.user_arguments_dict.get(\"training\", None)\n        if training is None:\n            # Wasn't passed explicitly: use context value\n            training = call_context.training\n            if training is None:\n                # Get signature default value\n                training = call_spec.arguments_dict.get(\"training\", None)\n        call_context.training = training\n        if self._call_has_training_arg and training is not None:\n            # Only populate arg if it has a concrete value\n            kwargs[\"training\"] = training\n\n        ####################\n        # 6. Call the module.\n        try:\n            with self._open_name_scope():\n                outputs = await super().__call__(*args, **kwargs)\n\n            if not self.built:\n                self.built = True\n        except Exception as e:\n            if self._hooks:\n                self._hooks.on_call_end(\n                    call_id=call_id,\n                    exception=str(e),\n                )\n            raise e\n        finally:\n            # Destroy call context if we created it\n            self._maybe_reset_call_context()\n        if self._hooks:\n            self._hooks.on_call_end(\n                call_id=call_id,\n                parent_call_id=parent_call_id,\n                outputs=outputs,\n            )\n        return outputs\n\n    async def call(self, *args, **kwargs):\n        raise self._not_implemented_error(self.call)\n\n    def __repr__(self):\n        return (\n            f\"&lt;{self.__class__.__name__} \"\n            f\"name={self.name}, description='{self.description}', built={self.built}&gt;\"\n        )\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __setattr__(self, name, value):\n        # Track Variables, Modules, Metrics.\n        name, value = self._setattr_hook(name, value)\n        if name != \"_tracker\":\n            if not hasattr(self, \"_tracker\"):\n                self._initialize_tracker()\n            value = self._tracker.track(value)\n        return super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        obj = getattr(self, name)\n        if isinstance(obj, backend.Variable):\n            import gc\n\n            # It will take a short amount of time for the corresponding buffer\n            # to be actually removed from the device.\n            # https://stackoverflow.com/a/74631949\n            self._untrack_variable(obj)\n            super().__delattr__(name)\n            gc.collect()\n        else:\n            super().__delattr__(name)\n\n    def _check_super_called(self):\n        if getattr(self, \"_lock\", True):\n            raise RuntimeError(\n                f\"In module '{self.__class__.__name__}', you forgot to call \"\n                \"`super().__init__()` as the first statement \"\n                \"in the `__init__()` method. Go add it!\"\n            )\n\n    def _assert_input_compatibility(self, first_arg):\n        # TODO perform check using schemas\n        pass\n\n    def _maybe_convert_inputs(self, inputs):\n        counter = {\"i\": 0}\n\n        def convert_fn(x):\n            if backend.is_data_model(x):\n                if counter[\"i\"] &gt; 0:\n                    result = x.to_json_data_model(\n                        name=f\"{self.name}_inputs_{counter['i']}\"\n                    )\n                else:\n                    result = x.to_json_data_model(name=f\"{self.name}_inputs\")\n                counter[\"i\"] += 1\n                return result\n            return x\n\n        return tree.map_structure(\n            convert_fn,\n            inputs,\n        )\n        return inputs\n\n    def _get_call_context(self):\n        \"\"\"Returns currently active `CallContext`.\"\"\"\n        module_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n        if module_call_ctx is None:\n            # Enter new call context.\n            module_call_ctx = CallContext(entry_module=self)\n            global_state.set_global_attribute(\"current_call_ctx\", module_call_ctx)\n            self._clear_rewards()\n        return module_call_ctx\n\n    def _maybe_reset_call_context(self):\n        module_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n        if module_call_ctx is None or module_call_ctx.entry_module == self:\n            global_state.set_global_attribute(\"current_call_ctx\", None)\n\n    def _flatten_modules(self, include_self=True, recursive=True):\n        modules = []\n        if include_self:\n            modules.append(self)\n        seen_object_ids = set()\n        deque = collections.deque(self._modules)\n        while deque:\n            module = deque.popleft()\n            if id(module) in seen_object_ids:\n                continue\n            seen_object_ids.add(id(module))\n            modules.append(module)\n            # Introspect recursively through submodules.\n            if recursive:\n                deque.extendleft(module._modules)\n        return modules\n\n    def _not_implemented_error(self, attr, msg=None):\n        if callable(attr):\n            attr_name = attr.__name__\n            attr_type = \"method\"\n        else:\n            attr_name = str(attr)\n            attr_type = \"attribute\"\n        msg = \" \" + msg if msg is not None else \"\"\n        return NotImplementedError(\n            f\"Module {self.__class__.__name__} does not have a `{attr_name}` \"\n            f\"{attr_type} implemented.{msg}\"\n        )\n\n    def _track_variable(self, variable):\n        if variable.trainable:\n            self._tracker.add_to_store(\"trainable_variables\", variable)\n        else:\n            self._tracker.add_to_store(\"non_trainable_variables\", variable)\n        if not self.trainable:\n            variable.trainable = False\n        self._post_track_variable(variable)\n\n    def _untrack_variable(self, variable):\n        previous_lock_state = self._tracker.locked\n        self._tracker.unlock()\n        self._tracker.untrack(variable)\n        if previous_lock_state is True:\n            self._tracker.lock()\n        self._post_untrack_variable(variable)\n\n    @python_utils.default\n    def get_config(self):\n        self._check_super_called()\n        base_config = super().get_config()\n        config = {\n            \"trainable\": self.trainable,\n        }\n        return {**base_config, **config}\n\n    def _open_name_scope(self):\n        if self._parent_path is None:\n            self._parent_path = current_path()\n        return backend.name_scope(self.name, caller=self)\n\n    async def _maybe_build(self, call_spec):\n        if self.built:\n            return\n\n        # If the module has a build method, call it with our input schemas.\n        if not utils.is_default(self.build):\n            if len(call_spec.first_arg) == 1:\n                await self.build(call_spec.first_arg[0])\n            else:\n                await self.build(call_spec.first_arg)\n            # Check input spec again (after build, since self.input_spec\n            # may have been updated\n            self._assert_input_compatibility(call_spec.first_arg)\n            return\n\n        # Otherwise, attempt to build the module by calling it on symbolic input.\n        if might_have_unbuilt_state(self):\n            try:\n                if not utils.is_default(self.compute_output_spec):\n                    await self.compute_output_spec(**call_spec.arguments_dict)\n                else:\n                    await backend.compute_output_spec(\n                        self.call, **call_spec.arguments_dict\n                    )\n            except Exception as e:\n                if call_spec.eager:\n                    # Will let the actual eager call do state-building\n                    return\n                warnings.warn(\n                    f\"Module '{self.name}' looks like it has unbuilt state, but \"\n                    \"Synalinks is not able to trace the module `call()` in order to \"\n                    \"build it automatically. Possible causes:\\n\"\n                    \"1. The `call()` method of your module may be crashing. Try \"\n                    \"to `__call__()` the module eagerly on some test input \"\n                    \"first to see if it works. \"\n                    \"2. If the `call()` method is correct, then you may need \"\n                    \"to implement the `def build(self, inputs)` or \"\n                    \"`def compute_output_spec(inputs, training=False)` method on \"\n                    \"your module.\"\n                    f\"Exception encountered: ''{e}''\"\n                )\n        self.built = True\n\n    def clone(self, name=None):\n        clone = self.from_config(self.get_config())\n        if name:\n            clone.name = name\n        else:\n            clone.name = auto_name(self.name)\n        return clone\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.metrics","title":"<code>metrics</code>  <code>property</code>","text":"<p>List of all metrics.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.metrics_variables","title":"<code>metrics_variables</code>  <code>property</code>","text":"<p>List of all metric variables.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.non_trainable_variables","title":"<code>non_trainable_variables</code>  <code>property</code>","text":"<p>List of all non-trainable module state.</p> <p>Returns:</p> Type Description <code>list</code> <p>The list of non-trainable variables.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.path","title":"<code>path</code>  <code>property</code>","text":"<p>The path of the module.</p> <p>If the module has not been built yet, it will be <code>None</code>.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.rewards","title":"<code>rewards</code>  <code>property</code>","text":"<p>List of scalar rewards from <code>add_reward</code> and submodules.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.trainable","title":"<code>trainable</code>  <code>property</code> <code>writable</code>","text":"<p>Settable boolean, whether this module should be trainable or not.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.trainable_variables","title":"<code>trainable_variables</code>  <code>property</code>","text":"<p>List of all trainable module state.</p> <p>Returns:</p> Type Description <code>list</code> <p>The list of trainable variables.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.variables","title":"<code>variables</code>  <code>property</code>","text":"<p>List of all module state.</p> <p>Note that metrics variables are not included here, use <code>metrics_variables</code> to visit all the metric variables.</p> <p>Returns:</p> Type Description <code>list</code> <p>The list of the variables.</p>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.add_variable","title":"<code>add_variable(initializer=None, data_model=None, trainable=True, name=None, description=None)</code>","text":"<p>Add a variable to the module</p> <p>Parameters:</p> Name Type Description Default <code>initializer</code> <code>dict | Initializer</code> <p>Initializer object to use to populate the initial variable value. Can be a JSON dict containing the initial value. If unspecified, defaults to <code>initializers.Empty</code>.</p> <code>None</code> <code>data_model</code> <code>DataModel</code> <p>The DataModel used to infer the schema and default value.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Boolean, whether the variable should be trainable via optimization or whether its updates are managed manually. Defaults to <code>True</code>.</p> <code>True</code> <code>name</code> <code>string</code> <p>String name of the variable. Useful for debugging purposes.</p> <code>None</code> <code>description</code> <code>string</code> <p>String description of the variable. Used by the optimizers to infer the role of the variable. Required if the data model do not have a docstring.</p> <code>None</code> <p>Returns:</p> Type Description <code>Variable</code> <p>The created variable</p> Source code in <code>synalinks/src/modules/module.py</code> <pre><code>def add_variable(\n    self,\n    initializer=None,\n    data_model=None,\n    trainable=True,\n    name=None,\n    description=None,\n):\n    \"\"\"Add a variable to the module\n\n    Args:\n        initializer (dict | Initializer): Initializer object to use to\n            populate the initial variable value. Can be a JSON dict containing the\n            initial value. If unspecified, defaults to `initializers.Empty`.\n        data_model (DataModel): The DataModel used to infer the schema\n            and default value.\n        trainable (bool): Boolean, whether the variable should be trainable via\n            optimization or whether its updates are managed manually. Defaults\n            to `True`.\n        name (string): String name of the variable. Useful for debugging purposes.\n        description (string): String description of the variable. Used by the\n            optimizers to infer the role of the variable. Required if the data\n            model do not have a docstring.\n\n    Returns:\n        (Variable): The created variable\n    \"\"\"\n    self._check_super_called()\n    if initializer is None:\n        initializer = initializers.Empty(data_model=data_model)\n    trainable = trainable and is_trainable(data_model)\n    with backend.name_scope(self.name, caller=self):\n        variable = backend.Variable(\n            initializer=initializer,\n            data_model=data_model,\n            trainable=trainable,\n            name=name,\n            description=description,\n        )\n    self._track_variable(variable)\n    return variable\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.build_from_config","title":"<code>build_from_config(config)</code>","text":"<p>Builds the module's states with the supplied config dict.</p> <p>By default, this method calls the <code>build()</code> method, which creates variables based on the module's input schema in the supplied config. If your config contains other information needed to load the module's state, you should override this method.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dict containing the input schema associated with this module.</p> required Source code in <code>synalinks/src/modules/module.py</code> <pre><code>def build_from_config(self, config):\n    \"\"\"Builds the module's states with the supplied config dict.\n\n    By default, this method calls the `build()` method,\n    which creates variables based on the module's input schema in the supplied\n    config. If your config contains other information needed to load the\n    module's state, you should override this method.\n\n    Args:\n        config (dict): Dict containing the input schema associated with this module.\n    \"\"\"\n    if config:\n        if \"input_schema\" in config:\n            run_maybe_nested(\n                self.build(backend.SymbolicDataModel(schema=config[\"input_schema\"]))\n            )\n        elif \"schemas_dict\" in config:\n            symbolic_inputs = {}\n            for key, schema in config[\"schemas_dict\"].items():\n                symbolic_inputs[key] = backend.SymbolicDataModel(schema=schema)\n            run_maybe_nested(self.build(**symbolic_inputs))\n        self.built = True\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.get_build_config","title":"<code>get_build_config()</code>","text":"<p>Returns a dictionary with the modules's input schema.</p> <p>This method returns a config dict that can be used by <code>build_from_config(config)</code> to create all states (e.g. Variables and Lookup tables) needed by the module.</p> <p>By default, the config only contains the input schema that the module was built with. If you're writing a custom module that creates state in an unusual way, you should override this method to make sure this state is already created when Synalinks attempts to load its value upon model loading.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing the input schema associated with the module.</p> Source code in <code>synalinks/src/modules/module.py</code> <pre><code>def get_build_config(self):\n    \"\"\"Returns a dictionary with the modules's input schema.\n\n    This method returns a config dict that can be used by\n    `build_from_config(config)` to create all states (e.g. Variables and\n    Lookup tables) needed by the module.\n\n    By default, the config only contains the input schema that the module\n    was built with. If you're writing a custom module that creates state in\n    an unusual way, you should override this method to make sure this state\n    is already created when Synalinks attempts to load its value upon model\n    loading.\n\n    Returns:\n        (dict): A dict containing the input schema associated with the module.\n    \"\"\"\n    if self._build_schemas_dict is not None:\n        if len(self._build_schemas_dict) == 1:\n            return {\n                \"input_schema\": tuple(self._build_schemas_dict.values())[0],\n            }\n        else:\n            return {\"schemas_dict\": self._build_schemas_dict}\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Base%20Module%20class/#synalinks.src.modules.module.Module.get_variable","title":"<code>get_variable(name=None, index=None)</code>","text":"<p>Retrieves a variable based on either its name (unique) or index.</p> <p>If <code>name</code> and <code>index</code> are both provided, <code>index</code> will take precedence. Indices are based on order of instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>The name of the variable.</p> <code>None</code> <code>index</code> <code>int</code> <p>The index of the variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>Variable</code> <p>The returned variable.</p> Source code in <code>synalinks/src/modules/module.py</code> <pre><code>def get_variable(self, name=None, index=None):\n    \"\"\"Retrieves a variable based on either its name (unique) or index.\n\n    If `name` and `index` are both provided, `index` will take precedence.\n    Indices are based on order of instantiation.\n\n    Args:\n        name (string): The name of the variable.\n        index (int): The index of the variable.\n\n    Returns:\n        (Variable): The returned variable.\n    \"\"\"\n    if index is not None and name is not None:\n        raise ValueError(\n            \"Provide only a variable name or a variable index. Received: \"\n            f\"index={index}, name={name}.\"\n        )\n    if index is not None:\n        if len(self.variables) &lt;= index:\n            raise ValueError(\n                f\"Was asked to retrieve variable at index {index}\"\n                f\" but module only has {len(self.modules)}\"\n                \" variables.\"\n            )\n        else:\n            return self.variables[index]\n\n    if name is not None:\n        for variable in self.variables:\n            if variable.name == name:\n                return variable\n        raise ValueError(\n            f\"No such variable: {name}. Existing variables are: \"\n            f\"{list(variable.name for variable in self.variables)}.\"\n        )\n    raise ValueError(\n        \"Provide either a variable name or variable index at `get_variable`.\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Agents%20Modules/","title":"Agent Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Agents%20Modules/#agent-modules","title":"Agent Modules","text":"<ul> <li>FunctionCallingAgent module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Agents%20Modules/FunctionCallingAgent%20module/","title":"FunctionCallingAgent module","text":""},{"location":"Synalinks%20API/Modules%20API/Agents%20Modules/FunctionCallingAgent%20module/#synalinks.src.modules.agents.function_calling_agent.FunctionCallingAgent","title":"<code>FunctionCallingAgent</code>","text":"<p>               Bases: <code>Module</code></p> <p>A trainable parallel function calling agent.</p> <p>The agent has 2 different modes:</p> <ul> <li>Autonomous: It will execute tools as soon as called.</li> <li>Non-autonomous: It will return the tool arguments as a ChatMessage.</li> </ul> <p>In autonomous mode, the agent accept any kind of data model input and perform a final inference to eventually format its final answer if a <code>data_model</code> or <code>schema</code> is provided.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass NumericalFinalAnswer(synalinks.DataModel):\n    final_answer: float = synalinks.Field(\n        description=\"The correct final numerical answer\",\n    )\n\nasync def calculate(expression: str):\n    \"\"\"Calculate the result of a mathematical expression.\n\n    Args:\n        expression (str): The mathematical expression to calculate, such as\n            '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n            parentheses, and spaces.\n    \"\"\"\n    if not all(char in \"0123456789+-*/(). \" for char in expression):\n        return {\n            \"result\": None,\n            \"log\": (\n                    \"Error: invalid characters in expression. \"\n                    \"The expression can only contain numbers, operators (+, -, *, /),\"\n                    \" parentheses, and spaces NOT letters.\"\n                ),\n        }\n    try:\n        # Evaluate the mathematical expression safely\n        result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n        return {\n            \"result\": result,\n            \"log\": \"Successfully executed\",\n        }\n    except Exception as e:\n        return {\n            \"result\": None,\n            \"log\": f\"Error: {e}\",\n        }\n\nasync def main():\n    language_model = synalinks.LanguageModel(model=\"ollama/mistral\")\n\n    tools = [\n        synalinks.Tool(calculate),\n    ]\n\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.FunctionCallingAgent(\n        data_model=NumericalFinalAnswer,\n        tools=tools,\n        language_model=language_model,\n        max_iterations=5,\n        autonomous=True,\n    )(inputs)\n    agent = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"math_agent\",\n        description=\"A math agent\",\n    )\n\n    input_query = Query(query=\"How much is 152648 + 485?\")\n    response = await agent(input_query)\n\n    print(response.prettify_json())\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Result:</p> <pre><code>{\n    \"query\": \"How much is 152648 + 485?\",\n    \"messages\": [\n        {\n        \"role\": \"assistant\",\n        \"content\": \"Performing simple addition\",\n        \"tool_calls\": [\n            {\n                \"id\": \"92a3657c-1a45-46e6-8173-df4255b8423b\",\n                \"name\": \"calculate\",\n                \"arguments\": {\n                    \"expression\": \"152648 + 485\"\n                    }\n                }\n            ]\n        },\n        {\n            \"role\": \"tool\",\n            \"content\": {\n                \"result\": 153133.0,\n                \"log\": \"Successfully executed\"\n            },\n            \"tool_call_id\": \"92a3657c-1a45-46e6-8173-df4255b8423b\",\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"The user has asked for a simple addition \"\n            \"calculation. The assistant used the 'calculate' tool to \"\n            \"perform this task, and the result was returned successfully.\",\n        }\n    ],\n    \"final_answer\": 153133.0\n}\n</code></pre> <p>In non-autonomous mode (also called human in the loop or interactive mode), the user needs to validate/edit the tool arguments and send it back to the agent. In this mode, the agent requires an <code>ChatMessages</code> data model as input and output an <code>ChatMessage</code> (or <code>ChatMessages</code> if <code>return_inputs_with_trajectory</code> is true) back to the user. In that case, the agent ignore the <code>max_iterations</code> argument, as it will only perform one step at a time.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nMAX_ITERATIONS = 5\n\nasync def calculate(expression: str):\n    \"\"\"Calculate the result of a mathematical expression.\n\n    Args:\n        expression (str): The mathematical expression to calculate, such as\n            '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n            parentheses, and spaces.\n    \"\"\"\n    if not all(char in \"0123456789+-*/(). \" for char in expression):\n        return {\n            \"result\": None,\n            \"log\": (\n                    \"Error: invalid characters in expression. \"\n                    \"The expression can only contain numbers, operators (+, -, *, /),\"\n                    \" parentheses, and spaces NOT letters.\"\n                ),\n        }\n    try:\n        # Evaluate the mathematical expression safely\n        result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n        return {\n            \"result\": result,\n            \"log\": \"Successfully executed\",\n        }\n    except Exception as e:\n        return {\n            \"result\": None,\n            \"log\": f\"Error: {e}\",\n        }\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    tools = [\n        synalinks.Tool(calculate),\n    ]\n\n    inputs = synalinks.Input(data_model=synalinks.ChatMessages)\n    outputs = await synalinks.FunctionCallingAgent(\n        tools=tools,\n        language_model=language_model,\n        return_inputs_with_trajectory=True,\n        autonomous=False,\n    )(inputs)\n    agent = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"math_agent\",\n        description=\"A math agent\",\n    )\n\n    input_messages = synalinks.ChatMessages(\n        messages=[\n            synalinks.ChatMessage(\n                role=\"user\",\n                content=\"How much is 152648 + 485?\",\n            )\n        ]\n    )\n\n    for i in range(MAX_ITERATIONS):\n\n        response = await agent(input_messages)\n\n        print(\"Assistant response (with trajectory):\")\n        print(response.prettify_json())\n\n        assistant_message = response.get(\"messages\")[-1]\n\n        if not assistant_message.get(\"tool_calls\"):\n            break # We stop the loop if the agent didn't call any tool\n\n        # Validate the tool calls arguments (with an UI or CLI)\n        # Then re-inject the validated assistant response in the input_messages\n        # The corresponding tools will be called by the agent\n        # Here we assume everything is okay for the purpose of the demo.\n\n        input_messages.messages.append(assistant_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>The FunctionCallingAgent is compatible with MCP tools, here is an example on how to use it:</p> <pre><code>import synalinks\nimport asyncio\nimport litellm\n\nclass Query(synalinks.DataModel):\n    \"\"\"Input query data model\"\"\"\n\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass FinalAnswer(synalinks.DataModel):\n    \"\"\"Final answer data model\"\"\"\n\n    answer: str = synalinks.Field(\n        description=\"The correct final answer\",\n    )\n\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    mcp_client = synalinks.MultiServerMCPClient(\n        {\n            \"math\": {\n                \"url\": \"http://localhost:8183/mcp/\",\n                \"transport\": \"streamable_http\",\n            },\n        }\n    )\n\n    tools = await mcp_client.get_tools()\n\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.FunctionCallingAgent(\n        data_model=FinalAnswer,\n        tools=tools,\n        language_model=language_model,\n        max_iterations=5,\n        autonomous=True,\n    )(inputs)\n\n    agent = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"mcp_math_agent\",\n        description=\"A math agent that can use an external calculator\",\n    )\n\n    input_query = Query(query=\"How much is 152648 + 485?\")\n    response = await agent(input_query)\n\n    print(response.prettify_json())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The target JSON schema. If not provided use the <code>data_model</code> to infer it.</p> <code>None</code> <code>data_model</code> <code>DataModel | SymbolicDataModel | JsonDataModel</code> <p>The target data model for structured output.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The jinja2 prompt template.</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default list of examples, the examples are a list of tuples containing input/output JSON pairs.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>The default instructions being a string containing instructions for the language model.</p> <code>None</code> <code>final_instructions</code> <code>str</code> <p>Optional. The instructions for the final generator that produces the structured output. If not provided, use the same instructions as the tool calls generator.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the prompt (Default to False).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the prompt (Default to False).</p> <code>False</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>use_chain_of_thought</code> <code>bool</code> <p>Optional. Use chain of thought for tool calls generator, usefull when using non-reasoning models. Default False.</p> <code>False</code> <code>tools</code> <code>list</code> <p>The list of <code>Tool</code> or MCP tools available to the agent.</p> <code>None</code> <code>autonomous</code> <code>bool</code> <p>Optional. Whether the agent runs autonomously (executing tools automatically) or in interactive mode where the user validates tool arguments before execution (Default to True).</p> <code>True</code> <code>return_inputs_with_trajectory</code> <code>bool</code> <p>Optional. Whether or not to return the inputs concatenated with the full message trajectory (Default to True).</p> <code>True</code> <code>max_iterations</code> <code>int</code> <p>Optional. The maximum number of tool calling iterations in autonomous mode (Default to 5). Ignored in interactive mode.</p> <code>5</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> Source code in <code>synalinks/src/modules/agents/function_calling_agent.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.FunctionCallingAgent\",\n        \"synalinks.FunctionCallingAgent\",\n    ]\n)\nclass FunctionCallingAgent(Module):\n    \"\"\"A trainable parallel function calling agent.\n\n    The agent has 2 different modes:\n\n    - Autonomous: It will execute tools as soon as called.\n    - Non-autonomous: It will return the tool arguments as a ChatMessage.\n\n    In *autonomous* mode, the agent accept **any kind of data model input**\n    and perform a final inference to eventually format its final answer if a\n    `data_model` or `schema` is provided.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class NumericalFinalAnswer(synalinks.DataModel):\n        final_answer: float = synalinks.Field(\n            description=\"The correct final numerical answer\",\n        )\n\n    async def calculate(expression: str):\n        \\\"\"\"Calculate the result of a mathematical expression.\n\n        Args:\n            expression (str): The mathematical expression to calculate, such as\n                '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n                parentheses, and spaces.\n        \\\"\"\"\n        if not all(char in \"0123456789+-*/(). \" for char in expression):\n            return {\n                \"result\": None,\n                \"log\": (\n                        \"Error: invalid characters in expression. \"\n                        \"The expression can only contain numbers, operators (+, -, *, /),\"\n                        \" parentheses, and spaces NOT letters.\"\n                    ),\n            }\n        try:\n            # Evaluate the mathematical expression safely\n            result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n            return {\n                \"result\": result,\n                \"log\": \"Successfully executed\",\n            }\n        except Exception as e:\n            return {\n                \"result\": None,\n                \"log\": f\"Error: {e}\",\n            }\n\n    async def main():\n        language_model = synalinks.LanguageModel(model=\"ollama/mistral\")\n\n        tools = [\n            synalinks.Tool(calculate),\n        ]\n\n        inputs = synalinks.Input(data_model=Query)\n        outputs = await synalinks.FunctionCallingAgent(\n            data_model=NumericalFinalAnswer,\n            tools=tools,\n            language_model=language_model,\n            max_iterations=5,\n            autonomous=True,\n        )(inputs)\n        agent = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"math_agent\",\n            description=\"A math agent\",\n        )\n\n        input_query = Query(query=\"How much is 152648 + 485?\")\n        response = await agent(input_query)\n\n        print(response.prettify_json())\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Result:\n\n    ```json\n    {\n        \"query\": \"How much is 152648 + 485?\",\n        \"messages\": [\n            {\n            \"role\": \"assistant\",\n            \"content\": \"Performing simple addition\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"92a3657c-1a45-46e6-8173-df4255b8423b\",\n                    \"name\": \"calculate\",\n                    \"arguments\": {\n                        \"expression\": \"152648 + 485\"\n                        }\n                    }\n                ]\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": {\n                    \"result\": 153133.0,\n                    \"log\": \"Successfully executed\"\n                },\n                \"tool_call_id\": \"92a3657c-1a45-46e6-8173-df4255b8423b\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"The user has asked for a simple addition \"\n                \"calculation. The assistant used the 'calculate' tool to \"\n                \"perform this task, and the result was returned successfully.\",\n            }\n        ],\n        \"final_answer\": 153133.0\n    }\n    ```\n\n    In *non-autonomous* mode (also called human in the loop or interactive mode), the\n    user needs to validate/edit the tool arguments and send it back to the agent. In this\n    mode, the agent requires an `ChatMessages` data model as input and output an\n    `ChatMessage` (or `ChatMessages` if `return_inputs_with_trajectory` is true)\n    back to the user. In that case, the agent ignore the `max_iterations` argument,\n    as it will only perform one **step at a time**.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    MAX_ITERATIONS = 5\n\n    async def calculate(expression: str):\n        \\\"\"\"Calculate the result of a mathematical expression.\n\n        Args:\n            expression (str): The mathematical expression to calculate, such as\n                '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n                parentheses, and spaces.\n        \\\"\"\"\n        if not all(char in \"0123456789+-*/(). \" for char in expression):\n            return {\n                \"result\": None,\n                \"log\": (\n                        \"Error: invalid characters in expression. \"\n                        \"The expression can only contain numbers, operators (+, -, *, /),\"\n                        \" parentheses, and spaces NOT letters.\"\n                    ),\n            }\n        try:\n            # Evaluate the mathematical expression safely\n            result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n            return {\n                \"result\": result,\n                \"log\": \"Successfully executed\",\n            }\n        except Exception as e:\n            return {\n                \"result\": None,\n                \"log\": f\"Error: {e}\",\n            }\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        tools = [\n            synalinks.Tool(calculate),\n        ]\n\n        inputs = synalinks.Input(data_model=synalinks.ChatMessages)\n        outputs = await synalinks.FunctionCallingAgent(\n            tools=tools,\n            language_model=language_model,\n            return_inputs_with_trajectory=True,\n            autonomous=False,\n        )(inputs)\n        agent = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"math_agent\",\n            description=\"A math agent\",\n        )\n\n        input_messages = synalinks.ChatMessages(\n            messages=[\n                synalinks.ChatMessage(\n                    role=\"user\",\n                    content=\"How much is 152648 + 485?\",\n                )\n            ]\n        )\n\n        for i in range(MAX_ITERATIONS):\n\n            response = await agent(input_messages)\n\n            print(\"Assistant response (with trajectory):\")\n            print(response.prettify_json())\n\n            assistant_message = response.get(\"messages\")[-1]\n\n            if not assistant_message.get(\"tool_calls\"):\n                break # We stop the loop if the agent didn't call any tool\n\n            # Validate the tool calls arguments (with an UI or CLI)\n            # Then re-inject the validated assistant response in the input_messages\n            # The corresponding tools will be called by the agent\n            # Here we assume everything is okay for the purpose of the demo.\n\n            input_messages.messages.append(assistant_message)\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    The FunctionCallingAgent is compatible with MCP tools,\n    here is an example on how to use it:\n\n    ```python\n    import synalinks\n    import asyncio\n    import litellm\n\n    class Query(synalinks.DataModel):\n        \\\"\"\"Input query data model\\\"\"\"\n\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class FinalAnswer(synalinks.DataModel):\n        \\\"\"\"Final answer data model\\\"\"\"\n\n        answer: str = synalinks.Field(\n            description=\"The correct final answer\",\n        )\n\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        mcp_client = synalinks.MultiServerMCPClient(\n            {\n                \"math\": {\n                    \"url\": \"http://localhost:8183/mcp/\",\n                    \"transport\": \"streamable_http\",\n                },\n            }\n        )\n\n        tools = await mcp_client.get_tools()\n\n        inputs = synalinks.Input(data_model=Query)\n        outputs = await synalinks.FunctionCallingAgent(\n            data_model=FinalAnswer,\n            tools=tools,\n            language_model=language_model,\n            max_iterations=5,\n            autonomous=True,\n        )(inputs)\n\n        agent = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"mcp_math_agent\",\n            description=\"A math agent that can use an external calculator\",\n        )\n\n        input_query = Query(query=\"How much is 152648 + 485?\")\n        response = await agent(input_query)\n\n        print(response.prettify_json())\n\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Args:\n        schema (dict): The target JSON schema.\n            If not provided use the `data_model` to infer it.\n        data_model (DataModel | SymbolicDataModel | JsonDataModel): The target data\n            model for structured output.\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The jinja2 prompt template.\n        examples (list): The default list of examples, the examples\n            are a list of tuples containing input/output JSON pairs.\n        instructions (str): The default instructions being a string containing\n            instructions for the language model.\n        final_instructions (str): Optional. The instructions for the final generator\n            that produces the structured output. If not provided, use the same\n            instructions as the tool calls generator.\n        temperature (float): Optional. The temperature for the LM call.\n        use_inputs_schema (bool): Optional. Whether or not use the inputs schema in\n            the prompt (Default to False).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs schema in\n            the prompt (Default to False).\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        use_chain_of_thought (bool): Optional. Use chain of thought for tool calls generator,\n            usefull when using non-reasoning models. Default False.\n        tools (list): The list of `Tool` or MCP tools available to the agent.\n        autonomous (bool): Optional. Whether the agent runs autonomously\n            (executing tools automatically) or in interactive mode where the user\n            validates tool arguments before execution (Default to True).\n        return_inputs_with_trajectory (bool): Optional. Whether or not to return the\n            inputs concatenated with the full message trajectory (Default to True).\n        max_iterations (int): Optional. The maximum number of tool calling iterations\n            in autonomous mode (Default to 5). Ignored in interactive mode.\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        final_instructions=None,\n        temperature=0.0,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        reasoning_effort=None,\n        use_chain_of_thought=False,\n        tools=None,\n        autonomous=True,\n        return_inputs_with_trajectory=True,\n        max_iterations=5,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n\n        self.prompt_template = prompt_template\n\n        if not instructions:\n            instructions = get_default_instructions()\n        self.instructions = instructions\n        if not final_instructions:\n            self.final_instructions = instructions\n        else:\n            self.final_instructions = final_instructions\n        self.temperature = temperature\n\n        self.examples = examples\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.reasoning_effort = reasoning_effort\n        self.use_chain_of_thought = use_chain_of_thought\n        self.language_model = language_model\n\n        self.tools = {}\n        if not tools:\n            raise ValueError(\"You must set the `tools` argument\")\n        for tool in tools:\n            self.tools[tool.name] = tool\n        tool_calls_schema = dynamic_tool_calls(tools=tools)\n\n        self.autonomous = autonomous\n        self.return_inputs_with_trajectory = return_inputs_with_trajectory\n        self.max_iterations = max_iterations\n\n        if use_chain_of_thought:\n            self.tool_calls_generator = ChainOfThought(\n                schema=tool_calls_schema,\n                prompt_template=self.prompt_template,\n                examples=self.examples,\n                instructions=self.instructions,\n                temperature=self.temperature,\n                use_inputs_schema=self.use_inputs_schema,\n                use_outputs_schema=self.use_outputs_schema,\n                reasoning_effort=self.reasoning_effort,\n                language_model=self.language_model,\n                name=\"tool_calls_generator_\" + self.name,\n            )\n        else:\n            self.tool_calls_generator = Generator(\n                schema=tool_calls_schema,\n                prompt_template=self.prompt_template,\n                examples=self.examples,\n                instructions=self.instructions,\n                temperature=self.temperature,\n                use_inputs_schema=self.use_inputs_schema,\n                use_outputs_schema=self.use_outputs_schema,\n                reasoning_effort=self.reasoning_effort,\n                language_model=self.language_model,\n                name=\"tool_calls_generator_\" + self.name,\n            )\n\n\n        self.final_generator = Generator(\n            schema=self.schema,\n            language_model=self.language_model,\n            instructions=self.final_instructions,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n            return_inputs=False,\n            name=\"final_generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        if self.autonomous:\n            if not is_chat_messages(inputs):\n                trajectory = await ops.concat(\n                    inputs,\n                    ChatMessages(),\n                    name=\"trajectory_\" + self.name,\n                )\n            else:\n                trajectory = inputs\n        else:\n            if not is_chat_messages(inputs):\n                raise ValueError(\n                    \"In interactive mode, the FunctionCallingAgent \"\n                    \"needs an ChatMessages-like data model as inputs\"\n                )\n            trajectory = inputs\n\n        agent_messages = trajectory.get(\"messages\")\n\n        if self.autonomous:\n            for i in range(self.max_iterations):\n                tool_calls = await self.tool_calls_generator(trajectory)\n\n                if not tool_calls:\n                    assistant_message = ChatMessage(\n                        role=ChatRole.ASSISTANT,\n                        content=\"Something went wrong while trying to decide \"\n                        \"the next action.\",\n                    )\n                    agent_messages.append(assistant_message.get_json())\n                    break\n\n                assistant_message = ChatMessage(\n                    role=ChatRole.ASSISTANT,\n                    content=tool_calls.get(\"thinking\", \"\"),\n                )\n\n                if not tool_calls.get(\"tool_calls\"):\n                    break\n\n                tasks = []\n                tool_calls_ids = []\n\n                for tool_call in tool_calls.get(\"tool_calls\"):\n                    tool_name = tool_call.get(\"tool_name\")\n                    tools_arguments = out_mask_json(tool_call, mask=[\"tool_name\"])\n                    tool_call_id = str(uuid.uuid4())\n                    tool_calls_ids.append(tool_call_id)\n                    assistant_message.tool_calls.append(\n                        ToolCall(\n                            id=tool_call_id,\n                            name=tool_name,\n                            arguments=tools_arguments,\n                        )\n                    )\n                    tasks.append(self.tools[tool_name](**tools_arguments))\n\n                agent_messages.append(assistant_message.get_json())\n\n                tool_results = await asyncio.gather(*tasks, return_exceptions=True)\n                for j, tool_result in enumerate(tool_results):\n                    tool_call_id = tool_calls_ids[j]\n                    if isinstance(tool_result, Exception):\n                        agent_messages.append(\n                            ChatMessage(\n                                role=ChatRole.TOOL,\n                                tool_call_id=tool_call_id,\n                                content=\"error: %s\" % str(tool_result),\n                            ).get_json()\n                        )\n                    else:\n                        # Handle both JsonDataModel and raw dict results\n                        content = (\n                            tool_result.get_json()\n                            if hasattr(tool_result, \"get_json\")\n                            else tool_result\n                        )\n                        agent_messages.append(\n                            ChatMessage(\n                                role=ChatRole.TOOL,\n                                tool_call_id=tool_call_id,\n                                content=content,\n                            ).get_json()\n                        )\n\n                trajectory.update({\"messages\": agent_messages})\n\n            if self.schema:\n                # With schema: return the structured data model\n                final_result = await self.final_generator(trajectory)\n                if self.return_inputs_with_trajectory:\n                    # Combine trajectory with structured output\n                    validated_messages = ChatMessages(\n                        messages=[ChatMessage(**msg) for msg in agent_messages]\n                    )\n                    return await ops.concat(\n                        JsonDataModel(\n                            json=validated_messages.get_json(),\n                            schema=ChatMessages.get_schema(),\n                            name=self.name,\n                        ),\n                        final_result,\n                        name=self.name,\n                    )\n                else:\n                    return final_result\n            else:\n                # Without schema: append the ChatMessage to the trajectory\n                final_result = await self.final_generator(trajectory)\n                if final_result:\n                    agent_messages.append(final_result.get_json())\n\n                validated_messages = ChatMessages(\n                    messages=[ChatMessage(**msg) for msg in agent_messages]\n                )\n                return JsonDataModel(\n                    json=validated_messages.get_json(),\n                    schema=ChatMessages.get_schema(),\n                    name=self.name,\n                )\n        else:\n            # Track new messages generated in this step\n            new_messages = []\n\n            if len(agent_messages) &gt; 0:\n                if agent_messages[-1].get(\"role\") == ChatRole.ASSISTANT:\n                    tasks = []\n                    tool_calls_ids = []\n\n                    tool_calls = agent_messages[-1].get(\"tool_calls\")\n                    for tool_call in tool_calls:\n                        tool_name = tool_call.get(\"name\")\n                        tools_arguments = tool_call.get(\"arguments\")\n                        tool_call_id = tool_call.get(\"id\")\n                        tool_calls_ids.append(tool_call_id)\n                        tasks.append(self.tools[tool_name](**tools_arguments))\n\n                    tool_results = await asyncio.gather(*tasks, return_exceptions=True)\n                    for j, tool_result in enumerate(tool_results):\n                        tool_call_id = tool_calls_ids[j]\n                        if isinstance(tool_result, Exception):\n                            tool_message = ChatMessage(\n                                role=ChatRole.TOOL,\n                                tool_call_id=tool_call_id,\n                                content=\"error: %s\" % str(tool_result),\n                            )\n                        else:\n                            # Handle both JsonDataModel and raw dict results\n                            content = (\n                                tool_result.get_json()\n                                if hasattr(tool_result, \"get_json\")\n                                else tool_result\n                            )\n                            tool_message = ChatMessage(\n                                role=ChatRole.TOOL,\n                                tool_call_id=tool_call_id,\n                                content=content,\n                            )\n                        agent_messages.append(tool_message.get_json())\n                        new_messages.append(tool_message)\n\n            trajectory.update({\"messages\": agent_messages})\n\n            tool_calls = await self.tool_calls_generator(trajectory)\n\n            # If no tool calls, call final generator\n            # without appending the empty tool calls message\n            if not tool_calls.get(\"tool_calls\"):\n                final_result = await self.final_generator(trajectory)\n                if self.schema:\n                    # Combine messages with structured output\n                    if self.return_inputs_with_trajectory:\n                        validated_messages = ChatMessages(\n                            messages=[ChatMessage(**msg) for msg in agent_messages]\n                        )\n                    else:\n                        validated_messages = ChatMessages(messages=new_messages)\n                    return await ops.concat(\n                        JsonDataModel(\n                            json=validated_messages.get_json(),\n                            schema=ChatMessages.get_schema(),\n                            name=self.name,\n                        ),\n                        final_result,\n                        name=self.name,\n                    )\n                else:\n                    # Append ChatMessage to messages\n                    if final_result:\n                        if self.return_inputs_with_trajectory:\n                            agent_messages.append(final_result.get_json())\n                            validated_messages = ChatMessages(\n                                messages=[ChatMessage(**msg) for msg in agent_messages]\n                            )\n                        else:\n                            new_messages.append(ChatMessage(**final_result.get_json()))\n                            validated_messages = ChatMessages(messages=new_messages)\n                    else:\n                        if self.return_inputs_with_trajectory:\n                            validated_messages = ChatMessages(\n                                messages=[ChatMessage(**msg) for msg in agent_messages]\n                            )\n                        else:\n                            validated_messages = ChatMessages(messages=new_messages)\n                    return JsonDataModel(\n                        json=validated_messages.get_json(),\n                        schema=ChatMessages.get_schema(),\n                        name=self.name,\n                    )\n\n            assistant_message = ChatMessage(\n                role=ChatRole.ASSISTANT,\n                content=tool_calls.get(\"thinking\", \"\"),\n                tool_calls=[],\n            )\n\n            for tool_call in tool_calls.get(\"tool_calls\", []):\n                tool_name = tool_call.get(\"tool_name\")\n                tools_arguments = out_mask_json(tool_call, mask=[\"tool_name\"])\n                tool_call_id = str(uuid.uuid4())\n\n                assistant_message.tool_calls.append(\n                    ToolCall(\n                        id=tool_call_id,\n                        name=tool_name,\n                        arguments=tools_arguments,\n                    )\n                )\n\n            agent_messages.append(assistant_message.get_json())\n            new_messages.append(assistant_message)\n            trajectory.update({\"messages\": agent_messages})\n\n            if self.return_inputs_with_trajectory:\n                # Convert dict messages to ChatMessage objects to avoid Pydantic warnings\n                validated_messages = ChatMessages(\n                    messages=[ChatMessage(**msg) for msg in agent_messages]\n                )\n                return JsonDataModel(\n                    json=validated_messages.get_json(),\n                    schema=ChatMessages.get_schema(),\n                    name=self.name,\n                )\n            else:\n                return JsonDataModel(\n                    json=ChatMessages(messages=new_messages).get_json(),\n                    schema=ChatMessages.get_schema(),\n                    name=self.name,\n                )\n\n    async def compute_output_spec(self, inputs, training=False):\n        if self.autonomous:\n            _ = await self.tool_calls_generator(inputs)\n            if self.schema:\n                if self.return_inputs_with_trajectory:\n                    return await ops.logical_and(\n                        SymbolicDataModel(\n                            schema=ChatMessages.get_schema(),\n                            name=self.name,\n                        ),\n                        SymbolicDataModel(\n                            schema=self.schema,\n                            name=\"final_generator_\" + self.name,\n                        ),\n                        name=self.name,\n                    )\n                else:\n                    return await self.final_generator(inputs)\n            else:\n                # Without schema: return ChatMessages with final message appended\n                _ = await self.final_generator(inputs)\n                return SymbolicDataModel(\n                    schema=ChatMessages.get_schema(),\n                    name=self.name,\n                )\n        else:\n            if not is_chat_messages(inputs):\n                raise ValueError(\n                    \"In interactive mode, the FunctionCallingAgent \"\n                    \"needs an ChatMessages-like data model as inputs\"\n                )\n\n            _ = await self.tool_calls_generator(inputs)\n\n            # The output can be either the final generator output (when no tool calls)\n            # or ChatMessages (when there are tool calls)\n            # We use ChatMessages as the output spec since it's the common case\n            return SymbolicDataModel(\n                schema=ChatMessages.get_schema(),\n                name=self.name,\n            )\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"final_instructions\": self.final_instructions,\n            \"temperature\": self.temperature,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"autonomous\": self.autonomous,\n            \"max_iterations\": self.max_iterations,\n            \"return_inputs_with_trajectory\": self.return_inputs_with_trajectory,\n            \"name\": self.name,\n            \"description\": self.description,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        tools_config = {\n            \"tools\": [\n                serialization_lib.serialize_synalinks_object(tool)\n                for tool in self.tools.values()\n            ]\n        }\n        return {**config, **language_model_config, **tools_config}\n\n    @classmethod\n    def from_config(cls, config):\n        tools = [\n            serialization_lib.deserialize_synalinks_object(tool)\n            for tool in config.pop(\"tools\")\n        ]\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(\n            language_model=language_model,\n            tools=tools,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Agents%20Modules/FunctionCallingAgent%20module/#synalinks.src.modules.agents.function_calling_agent.get_default_instructions","title":"<code>get_default_instructions()</code>","text":"<p>The default parallel function calling agent instructions.</p> Source code in <code>synalinks/src/modules/agents/function_calling_agent.py</code> <pre><code>def get_default_instructions():\n    \"\"\"The default parallel function calling agent instructions.\"\"\"\n    return \"\"\"\nThink step by step: Use the thinking field to elaborate what you observe and\nwhat do you need to accomplish next.\nReflect on prior steps: Review your previous actions and their outcomes to\navoid unnecessary repetition.\nAvoid unnecessary actions: If you already have enough information to complete\nthe user task, return an empty tool calls array.\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/","title":"Core Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/#core-modules","title":"Core Modules","text":"<ul> <li>Input module</li> <li>Identity module</li> <li>Not module</li> <li>Generator module</li> <li>Decision module</li> <li>Action module</li> <li>Branch module</li> <li>Tool module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Action%20module/","title":"Action module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Action%20module/#synalinks.src.modules.core.action.Action","title":"<code>Action</code>","text":"<p>               Bases: <code>Module</code></p> <p>Use a <code>LanguageModel</code> to perform a tool call given the input data model.</p> <p>This module uses structured output to call a given Tool. This module can be used in agents or traditional workflows seamlessly, it uses the input data model to infer the tool parameters.</p> <p>The output of this module contains the inputs inferred by the language model as well as the outputs of the tool call.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n\n    class Query(synalinks.DataModel):\n        query: str\n\n    @synalinks.saving.register_synalinks_serializable()\n    async def calculate(expression: str):\n        \"\"\"Calculate the result of a mathematical expression.\n\n        Args:\n            expression (str): The mathematical expression to calculate, such as\n                '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n                parentheses, and spaces.\n        \"\"\"\n        if not all(char in \"0123456789+-*/(). \" for char in expression):\n            return {\n                \"result\": None,\n                \"log\": \"Error: invalid characters in expression\",\n            }\n        try:\n            # Evaluate the mathematical expression safely\n            result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n            return {\n                \"result\": result,\n                \"log\": \"Successfully executed\",\n            }\n        except Exception as e:\n            return {\n                \"result\": None,\n                \"log\": f\"Error: {e}\",\n            }\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    x1 = await synalinks.Action(\n        tool=synalinks.Tool(calculate),\n        language_model=language_model,\n    )(x0)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x1,\n        name=\"calculator\",\n        description=\"This program perform the calculation of an expression\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>Tool</code> <p>The Tool instance to call.</p> required <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The default jinja2 prompt template to use (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default examples to use in the prompt (see <code>Generator</code>).</p> <code>None</code> <code>instructions</code> <code>list</code> <p>The default instructions to use (see <code>Generator</code>).</p> <code>None</code> <code>seed_instructions</code> <code>list</code> <p>Optional. A list of instructions to use as seed for the optimization. If not provided, use the default instructions as seed.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/core/action.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.Action\",\n        \"synalinks.Action\",\n    ]\n)\nclass Action(Module):\n    \"\"\"Use a `LanguageModel` to perform a tool call given the input data model.\n\n    This module uses structured output to call a given Tool.\n    This module can be used in agents or traditional workflows seamlessly,\n    it uses the input data model to infer the tool parameters.\n\n    The output of this module contains the inputs inferred by the language model\n    as well as the outputs of the tool call.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    async def main():\n\n        class Query(synalinks.DataModel):\n            query: str\n\n        @synalinks.saving.register_synalinks_serializable()\n        async def calculate(expression: str):\n            \\\"\"\"Calculate the result of a mathematical expression.\n\n            Args:\n                expression (str): The mathematical expression to calculate, such as\n                    '2 + 2'. The expression can contain numbers, operators (+, -, *, /),\n                    parentheses, and spaces.\n            \\\"\"\"\n            if not all(char in \"0123456789+-*/(). \" for char in expression):\n                return {\n                    \"result\": None,\n                    \"log\": \"Error: invalid characters in expression\",\n                }\n            try:\n                # Evaluate the mathematical expression safely\n                result = round(float(eval(expression, {\"__builtins__\": None}, {})), 2)\n                return {\n                    \"result\": result,\n                    \"log\": \"Successfully executed\",\n                }\n            except Exception as e:\n                return {\n                    \"result\": None,\n                    \"log\": f\"Error: {e}\",\n                }\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        x0 = synalinks.Input(data_model=Query)\n        x1 = await synalinks.Action(\n            tool=synalinks.Tool(calculate),\n            language_model=language_model,\n        )(x0)\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x1,\n            name=\"calculator\",\n            description=\"This program perform the calculation of an expression\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Args:\n        tool (Tool): The Tool instance to call.\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The default jinja2 prompt template\n            to use (see `Generator`).\n        examples (list): The default examples to use in the prompt\n            (see `Generator`).\n        instructions (list): The default instructions to use (see `Generator`).\n        seed_instructions (list): Optional. A list of instructions to use as seed for the\n            optimization. If not provided, use the default instructions as seed.\n        temperature (float): Optional. The temperature for the LM call.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        use_inputs_schema (bool): Optional. Whether or not use the inputs schema in\n            the prompt (Default to False) (see `Generator`).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs schema in\n            the prompt (Default to False) (see `Generator`).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        tool,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        temperature=0.0,\n        reasoning_effort=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.tool = tool\n        schema = self.tool.get_input_schema()\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.examples = examples\n        self.instructions = instructions\n        self.seed_instructions = seed_instructions\n        self.temperature = temperature\n        self.reasoning_effort = reasoning_effort\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.action = Generator(\n            schema=schema,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            instructions=self.instructions,\n            seed_instructions=self.seed_instructions,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            name=\"generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        tool_inputs = await self.action(inputs, training=training)\n        try:\n            tool_result = await self.tool(**tool_inputs.get_json())\n            tool_outputs = tool_result.get_json() if tool_result else {}\n        except Exception as e:\n            tool_outputs = {\"error\": str(e)}\n        generic_io = GenericIO(inputs=tool_inputs.get_json(), outputs=tool_outputs)\n        return JsonDataModel(\n            json=GenericAction(action=generic_io.get_json()).get_json(),\n            schema=GenericAction.get_schema(),\n            name=self.name,\n        )\n\n    async def compute_output_spec(self, inputs, training=False):\n        _ = await self.action(inputs)\n        return SymbolicDataModel(schema=GenericAction.get_schema(), name=self.name)\n\n    def get_config(self):\n        config = {\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        tool_config = {\"tool\": serialization_lib.serialize_synalinks_object(self.tool)}\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n        return {**config, **tool_config, **language_model_config}\n\n    @classmethod\n    def from_config(cls, config):\n        tool = serialization_lib.deserialize_synalinks_object(config.pop(\"tool\"))\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(tool=tool, language_model=language_model, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Action%20module/#synalinks.src.modules.core.action.GenericAction","title":"<code>GenericAction</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A generic action with inputs/outputs</p> Source code in <code>synalinks/src/modules/core/action.py</code> <pre><code>class GenericAction(DataModel):\n    \"\"\"A generic action with inputs/outputs\"\"\"\n\n    action: GenericIO = Field(description=\"An action already performed\")\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Branch%20module/","title":"Branch module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Branch%20module/#synalinks.src.modules.core.branch.Branch","title":"<code>Branch</code>","text":"<p>               Bases: <code>Module</code></p> <p>Use a <code>LanguageModel</code> to select which module to call based on an arbitrary     input, a question and a list of labels.</p> <p>The selected branch output the data model computed using the inputs and module's branch, while the others output <code>None</code>.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    class Query(synalinks.DataModel):\n        query: str\n\n    class Answer(synalinks.DataModel):\n        answer: str\n\n    class AnswerWithCritique(synalinks.DataModel):\n        thinking: str\n        critique: str\n        answer: str\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    (x1, x2) = await synalinks.Branch(\n        question=\"What is the difficulty level of the above query?\",\n        labels=[\"easy\", \"difficult\"],\n        branches=[\n            synalinks.Generator(\n                data_model=Answer,\n                language_model=language_model,\n            ),\n            synalinks.Generator(\n                data_model=AnswerWithCritique,\n                language_model=language_model,\n            ),\n        ],\n        language_model=language_model,\n    )(x0)\n    x3 = x1 | x2\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x3,\n        name=\"adaptative_chain_of_thought\",\n        description=\"Useful to answer step by step only when needed\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to ask.</p> <code>None</code> <code>labels</code> <code>list</code> <p>The list of labels to choose from (strings).</p> <code>None</code> <code>branches</code> <code>list</code> <p>The list of modules or programs to select from.</p> <code>None</code> <code>inject_decision</code> <code>bool</code> <p>If True, inject the decision to the branch inputs. (default to True).</p> <code>True</code> <code>return_decision</code> <code>bool</code> <p>If True, return the decision with the branch outputs. (default to True).</p> <code>True</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The default jinja2 prompt template to use (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default examples to use in the prompt (see <code>Decision</code>).</p> <code>None</code> <code>instructions</code> <code>list</code> <p>The default instructions to use (see <code>Decision</code>).</p> <code>None</code> <code>seed_instructions</code> <code>list</code> <p>Optional. A list of instructions to use as seed for the optimization. If not provided, use the default instructions as seed.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the decision prompt (Default to False) (see <code>Decision</code>).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the decision prompt (Default to False) (see <code>Decision</code>).</p> <code>False</code> <code>decision_type</code> <code>bool</code> <p>Optional. The type of decision module to use.</p> <code>Decision</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/core/branch.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Branch\", \"synalinks.Branch\"])\nclass Branch(Module):\n    \"\"\"Use a `LanguageModel` to select which module to call based on an arbitrary\n        input, a question and a list of labels.\n\n    The selected branch output the data model computed using\n    the inputs and module's branch, while the others output `None`.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    async def main():\n        class Query(synalinks.DataModel):\n            query: str\n\n        class Answer(synalinks.DataModel):\n            answer: str\n\n        class AnswerWithCritique(synalinks.DataModel):\n            thinking: str\n            critique: str\n            answer: str\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        x0 = synalinks.Input(data_model=Query)\n        (x1, x2) = await synalinks.Branch(\n            question=\"What is the difficulty level of the above query?\",\n            labels=[\"easy\", \"difficult\"],\n            branches=[\n                synalinks.Generator(\n                    data_model=Answer,\n                    language_model=language_model,\n                ),\n                synalinks.Generator(\n                    data_model=AnswerWithCritique,\n                    language_model=language_model,\n                ),\n            ],\n            language_model=language_model,\n        )(x0)\n        x3 = x1 | x2\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x3,\n            name=\"adaptative_chain_of_thought\",\n            description=\"Useful to answer step by step only when needed\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Args:\n        question (str): The question to ask.\n        labels (list): The list of labels to choose from (strings).\n        branches (list): The list of modules or programs to select from.\n        inject_decision (bool): If True, inject the decision to the branch inputs.\n            (default to True).\n        return_decision (bool): If True, return the decision with the branch outputs.\n            (default to True).\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The default jinja2 prompt template\n            to use (see `Generator`).\n        examples (list): The default examples to use in the prompt\n            (see `Decision`).\n        instructions (list): The default instructions to use (see `Decision`).\n        seed_instructions (list): Optional. A list of instructions to use as seed for the\n            optimization. If not provided, use the default instructions as seed.\n        temperature (float): Optional. The temperature for the LM call.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        use_inputs_schema (bool): Optional. Whether or not use the inputs\n            schema in the decision prompt (Default to False) (see `Decision`).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs\n            schema in the decision prompt (Default to False) (see `Decision`).\n        decision_type (bool): Optional. The type of decision module to use.\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        question=None,\n        labels=None,\n        branches=None,\n        inject_decision=True,\n        return_decision=True,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        temperature=0.0,\n        reasoning_effort=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        decision_type=Decision,\n        name=None,\n        description=None,\n        trainable=True,\n        **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        if not branches:\n            raise ValueError(\"The `branches` argument must be provided.\")\n        if not isinstance(branches, list):\n            raise ValueError(\"The `branches` must be a list of `Module` or `Program`.\")\n        if len(labels) != len(branches):\n            raise ValueError(\"The `labels` and `branches` must have the same length.\")\n        self.question = question\n        self.labels = labels\n        self.branches = {labels[i]: m for i, m in enumerate(branches)}\n        self.inject_decision = inject_decision\n        self.return_decision = return_decision\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.examples = examples\n        self.instructions = instructions\n        self.seed_instructions = seed_instructions\n        self.temperature = temperature\n        self.reasoning_effort = reasoning_effort\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.decision = decision_type(\n            question=self.question,\n            labels=self.labels,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            instructions=self.instructions,\n            seed_instructions=self.seed_instructions,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            name=\"decision_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        outputs = [None] * len(self.branches)\n\n        if not inputs:\n            return tuple(outputs)\n\n        decision = await self.decision(\n            inputs,\n            training=training,\n        )\n\n        if not decision:\n            return tuple(outputs)\n\n        choice = decision.get(\"choice\", decision.get(\"choices\"))\n\n        if not choice:\n            return tuple(outputs)\n\n        if self.inject_decision:\n            inputs = await ops.concat(\n                inputs,\n                decision,\n                name=\"inputs_with_decision_\" + self.name,\n            )\n\n        tasks = []\n\n        async def execute_branch(\n            inputs, module=None, decision=None, return_decision=False\n        ):\n            if not inputs:\n                return None\n            if return_decision:\n                return await ops.logical_and(\n                    decision,\n                    await module(inputs),\n                )\n            else:\n                return await module(inputs)\n\n        for label in self.labels:\n            module = self.branches[label]\n            selected = False\n            if isinstance(choice, str):\n                if label == choice:\n                    selected = True\n            elif isinstance(choice, (list, set)):\n                if label in choice:\n                    selected = True\n            if selected and module:\n                tasks.append(\n                    execute_branch(\n                        inputs,\n                        module,\n                        decision,\n                        return_decision=self.return_decision,\n                    )\n                )\n            else:\n                tasks.append(execute_branch(None))\n        outputs = await asyncio.gather(*tasks)\n        return tuple(outputs)\n\n    async def compute_output_spec(self, inputs, training=False):\n        outputs = []\n        decision = await self.decision(\n            inputs,\n            training=training,\n        )\n        if self.inject_decision:\n            inputs = await ops.concat(\n                inputs,\n                decision,\n                name=\"inputs_with_decision_\" + self.name,\n            )\n        for label in self.labels:\n            module = self.branches[label]\n            if self.return_decision:\n                outputs.append(\n                    await ops.logical_and(\n                        decision,\n                        await module(\n                            inputs,\n                            training=training,\n                        ),\n                        name=\"with_decision_\" + self.name,\n                    )\n                )\n            else:\n                outputs.append(\n                    await module(\n                        inputs,\n                        training=training,\n                    )\n                )\n        return tuple(outputs)\n\n    def get_config(self):\n        config = {\n            \"question\": self.question,\n            \"labels\": self.labels,\n            \"inject_decision\": self.inject_decision,\n            \"return_decision\": self.return_decision,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n        branches_config = {\n            \"branches\": [\n                serialization_lib.serialize_synalinks_object(branch)\n                for branch in self.branches.values()\n            ]\n        }\n        return {**config, **language_model_config, **branches_config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        branches = [\n            serialization_lib.deserialize_synalinks_object(\n                branch_config, custom_objects=custom_objects\n            )\n            for branch_config in config.pop(\"branches\")\n        ]\n        return cls(language_model=language_model, branches=branches, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Decision%20module/","title":"Decision module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Decision%20module/#synalinks.src.modules.core.decision.Decision","title":"<code>Decision</code>","text":"<p>               Bases: <code>Module</code></p> <p>Perform a decision on the given input based on a question and a list of labels.</p> <p>This module dynamically create an <code>Enum</code> schema based on the given labels and use it to generate a possible answer using structured output.</p> <p>This ensure that the LM answer is always one of the provided labels.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=synalinks.ChatMessages)\n    x1 = await synalinks.Decision(\n        question=\"What is the danger level of the discussion?\",\n        labels=[\"low\", \"medium\", \"high\"],\n        language_model=language_model,\n    )(x0)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x1,\n        name=\"discussion_danger_assessment\",\n        description=\"This program assesses the level of danger in a discussion.\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>You can view this module, as performing a single label classification on the input.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to ask.</p> <code>None</code> <code>labels</code> <code>list</code> <p>The list of labels to choose from (strings).</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The default jinja2 prompt template to use (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default examples to use in the prompt (see <code>Generator</code>).</p> <code>None</code> <code>instructions</code> <code>list</code> <p>The default instructions to use (see <code>Generator</code>).</p> <code>None</code> <code>seed_instructions</code> <code>list</code> <p>Optional. A list of instructions to use as seed for the optimization. If not provided, use the default instructions as seed.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/core/decision.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Decision\", \"synalinks.Decision\"])\nclass Decision(Module):\n    \"\"\"Perform a decision on the given input based on a question and a list of labels.\n\n    This module dynamically create an `Enum` schema based on the given labels and\n    use it to generate a possible answer using structured output.\n\n    This ensure that the LM answer is **always** one of the provided labels.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        x0 = synalinks.Input(data_model=synalinks.ChatMessages)\n        x1 = await synalinks.Decision(\n            question=\"What is the danger level of the discussion?\",\n            labels=[\"low\", \"medium\", \"high\"],\n            language_model=language_model,\n        )(x0)\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x1,\n            name=\"discussion_danger_assessment\",\n            description=\"This program assesses the level of danger in a discussion.\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    You can view this module, as performing a single label classification on the input.\n\n    Args:\n        question (str): The question to ask.\n        labels (list): The list of labels to choose from (strings).\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The default jinja2 prompt template\n            to use (see `Generator`).\n        examples (list): The default examples to use in the prompt\n            (see `Generator`).\n        instructions (list): The default instructions to use (see `Generator`).\n        seed_instructions (list): Optional. A list of instructions to use as seed for the\n            optimization. If not provided, use the default instructions as seed.\n        temperature (float): Optional. The temperature for the LM call.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        use_inputs_schema (bool): Optional. Whether or not use the inputs schema in\n            the prompt (Default to False) (see `Generator`).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs schema in\n            the prompt (Default to False) (see `Generator`).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        question=None,\n        labels=None,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        temperature=0.0,\n        reasoning_effort=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        if not question:\n            raise ValueError(\"The `question` argument must be provided.\")\n        if not labels:\n            raise ValueError(\"The `labels` argument must be provided.\")\n        if not isinstance(labels, list):\n            raise ValueError(\"The `labels` parameter must be a list of string.\")\n        schema = dynamic_enum(DecisionAnswer.get_schema(), \"choice\", labels)\n        self.schema = schema\n        self.question = question\n        self.labels = labels\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.examples = examples\n        if not instructions:\n            instructions = default_decision_instructions(self.labels)\n        self.instructions = instructions\n        self.temperature = temperature\n        self.reasoning_effort = reasoning_effort\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.decision = Generator(\n            schema=self.schema,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            instructions=self.instructions,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            name=\"generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        inputs = await ops.concat(\n            inputs,\n            Question(question=self.question),\n            name=\"inputs_with_question_\" + self.name,\n        )\n        result = await self.decision(inputs, training=training)\n        return result\n\n    def get_config(self):\n        config = {\n            \"question\": self.question,\n            \"labels\": self.labels,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n        return {**config, **language_model_config}\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(language_model=language_model, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Decision%20module/#synalinks.src.modules.core.decision.default_decision_instructions","title":"<code>default_decision_instructions(labels)</code>","text":"<p>The decision default instructions</p> Source code in <code>synalinks/src/modules/core/decision.py</code> <pre><code>def default_decision_instructions(labels):\n    \"\"\"The decision default instructions\"\"\"\n    return f\"\"\"\nYou will be given a question, your task is to answer step-by-step to choose\none the following labels: {labels}\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Generator%20module/","title":"Generator module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Generator%20module/#synalinks.src.modules.core.generator.Generator","title":"<code>Generator</code>","text":"<p>               Bases: <code>Module</code></p> <p>Use a <code>LanguageModel</code> to generate a data model from an arbitrary input data model.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n\n    class Query(DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithCritique(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking\",\n        )\n        critique: str = synalinks.Field(\n            description=\"The critique of the above thinking\",\n        )\n        answer: str = synalinks.Field(\n            description=\"The correct answer\",\n        )\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    x1 = await synalinks.Generator(\n        data_model=AnswerWithCritique,\n        language_model=language_model,\n    )(x0)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x1,\n        name=\"chain_of_thought_with_critique\",\n        description=\"Useful to answer step by step and evaluate your answer\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The target JSON schema. If not provided use the <code>data_model</code> to infer it.</p> <code>None</code> <code>data_model</code> <code>DataModel | SymbolicDataModel | JsonDataModel</code> <p>The target data model for structured output.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The jinja2 prompt template.</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default list of examples, the examples are a list of tuples containing input/output JSON pairs.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>The default instructions being a string containing instructions for the language model.</p> <code>None</code> <code>seed_instructions</code> <code>list</code> <p>Optional. A list of instructions to use as seed for the optimization. If not provided, use the default instructions as seed.</p> <code>None</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the prompt (Default to False).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the prompt (Default to False).</p> <code>False</code> <code>return_inputs</code> <code>bool</code> <p>Optional. Whether or not to concatenate the inputs to the outputs (Default to False).</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>streaming</code> <code>str</code> <p>Optional. If true stream the LM response, enabled only if <code>schema</code> is <code>None</code> and only during inference (not during training).</p> <code>False</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/core/generator.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Generator\", \"synalinks.Generator\"])\nclass Generator(Module):\n    \"\"\"\n    Use a `LanguageModel` to generate a data model from an arbitrary input data model.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    async def main():\n\n        class Query(DataModel):\n            query: str = synalinks.Field(\n                description=\"The user query\",\n            )\n\n        class AnswerWithCritique(synalinks.DataModel):\n            thinking: str = synalinks.Field(\n                description=\"Your step by step thinking\",\n            )\n            critique: str = synalinks.Field(\n                description=\"The critique of the above thinking\",\n            )\n            answer: str = synalinks.Field(\n                description=\"The correct answer\",\n            )\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        x0 = synalinks.Input(data_model=Query)\n        x1 = await synalinks.Generator(\n            data_model=AnswerWithCritique,\n            language_model=language_model,\n        )(x0)\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x1,\n            name=\"chain_of_thought_with_critique\",\n            description=\"Useful to answer step by step and evaluate your answer\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Args:\n        schema (dict): The target JSON schema.\n            If not provided use the `data_model` to infer it.\n        data_model (DataModel | SymbolicDataModel | JsonDataModel): The target data\n            model for structured output.\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The jinja2 prompt template.\n        examples (list): The default list of examples, the examples\n            are a list of tuples containing input/output JSON pairs.\n        instructions (str): The default instructions being a string containing\n            instructions for the language model.\n        seed_instructions (list): Optional. A list of instructions to use as seed for the\n            optimization. If not provided, use the default instructions as seed.\n        use_inputs_schema (bool): Optional. Whether or not use the inputs schema in\n            the prompt (Default to False).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs schema in\n            the prompt (Default to False).\n        return_inputs (bool): Optional. Whether or not to concatenate the inputs to\n            the outputs (Default to False).\n        temperature (float): Optional. The temperature for the LM call.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        streaming (str): Optional. If true stream the LM response, enabled only if\n            `schema` is `None` and only during inference (not during training).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        return_inputs=False,\n        temperature=0.0,\n        reasoning_effort=None,\n        streaming=False,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n        if not language_model:\n            raise ValueError(\"You should provide `language_model` parameter.\")\n        self.language_model = language_model\n        if not prompt_template:\n            prompt_template = default_prompt_template()\n        self.prompt_template = prompt_template\n        if not examples:\n            examples = []\n        self.examples = examples\n        if not instructions and self.schema:\n            data_model_keys = list(self.schema[\"properties\"].keys())\n            instructions = default_instructions(data_model_keys)\n        self.instructions = instructions\n        self.return_inputs = return_inputs\n        self.temperature = temperature\n        efforts = [\"minimal\", \"low\", \"medium\", \"high\", \"disable\", \"none\", None]\n        if reasoning_effort not in efforts:\n            raise ValueError(\n                f\"The reasoning effort parameter should be one of: {efforts}\"\n            )\n        self.reasoning_effort = reasoning_effort\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        if schema and streaming:\n            streaming = False\n        self.streaming = streaming\n\n        predictions = [\n            Prediction(\n                inputs=example[0],\n                outputs=example[1],\n                reward=None,\n            ).get_json()\n            for example in examples\n        ]\n\n        if not seed_instructions:\n            seed_instructions = []\n        self.seed_instructions = seed_instructions\n\n        seed_candidates = [\n            {\n                \"instructions\": seed_instruction,\n            }\n            for seed_instruction in self.seed_instructions\n        ]\n\n        self.state = self.add_variable(\n            initializer=Instructions(\n                instructions=instructions,\n                examples=predictions,\n                seed_candidates=seed_candidates,\n            ).get_json(),\n            data_model=Instructions,\n            name=\"state_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        msgs = self.format_messages(inputs)\n        if self.streaming and not training:\n            streaming = True\n        else:\n            streaming = False\n        result = await ops.predict(\n            msgs,\n            schema=self.schema,\n            language_model=self.language_model,\n            streaming=streaming,\n            name=\"prediction_\" + self.name,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n        )\n        if streaming:\n            return result\n        if result:\n            if training:\n                predictions = self.state.get(\"current_predictions\")\n                predictions.append(\n                    {\n                        \"inputs\": inputs.get_json(),\n                        \"outputs\": result.get_json(),\n                        \"reward\": None,\n                    }\n                )\n            if self.return_inputs:\n                return await ops.concat(\n                    inputs,\n                    result,\n                    name=\"with_inputs_\" + self.name,\n                )\n            else:\n                return result\n        return None\n\n    async def compute_output_spec(self, inputs, training=False):\n        if self.schema:\n            if self.return_inputs:\n                return await ops.concat(\n                    inputs,\n                    SymbolicDataModel(\n                        schema=self.schema,\n                        name=self.name,\n                    ),\n                    name=\"with_inputs_\" + self.name,\n                )\n            else:\n                return SymbolicDataModel(\n                    schema=self.schema,\n                    name=self.name,\n                )\n        else:\n            if self.return_inputs:\n                return await ops.concat(\n                    inputs,\n                    SymbolicDataModel(\n                        schema=ChatMessage.get_schema(),\n                        name=self.name,\n                    ),\n                    name=\"with_inputs_\" + self.name,\n                )\n            else:\n                return SymbolicDataModel(\n                    schema=ChatMessage.get_schema(),\n                    name=self.name,\n                )\n\n    def format_messages(self, inputs=None):\n        template = jinja2.Template(self.prompt_template)\n        rendered_prompt = template.render(\n            inputs_schema=inputs.get_schema() if self.use_inputs_schema else None,\n            outputs_schema=self.schema if self.use_outputs_schema else None,\n            examples=[\n                (pred.get(\"inputs\"), pred.get(\"outputs\"))\n                for pred in self.state.get(\"examples\")\n            ],\n            instructions=self.state.get(\"instructions\"),\n        )\n        system_message = ChatMessage(role=\"system\", content=rendered_prompt)\n        user_message = ChatMessage(\n            role=\"user\", content=f\"## Input:\\n{inputs.get_json()}\\n##Output:\\n\"\n        )\n        msgs = ChatMessages(messages=[system_message, user_message])\n        return msgs\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"return_inputs\": self.return_inputs,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        return {\n            **config,\n            **language_model_config,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\"),\n        )\n        return cls(\n            language_model=language_model,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Generator%20module/#synalinks.src.modules.core.generator.default_prompt_template","title":"<code>default_prompt_template()</code>","text":"<p>Returns the default prompt template.</p> <p>Returns:</p> Type Description <code>str</code> <p>The default prompt template.</p> Source code in <code>synalinks/src/modules/core/generator.py</code> <pre><code>@synalinks_export(\"synalinks.default_prompt_template\")\ndef default_prompt_template():\n    \"\"\"Returns the default prompt template.\n\n    Returns:\n        (str): The default prompt template.\n    \"\"\"\n    return \"\"\"\n# Instructions\n{{ instructions }}\n{% if inputs_schema %}\n# Input Schema\n{{ inputs_schema }}\n{% endif %}{% if outputs_schema %}\n# Output schema\n{{ outputs_schema }}\n{% endif %}{% if examples %}\n# Examples\n{% for example in examples %}\n## Input:\n{{ example[0] }}\n## Output:\n{{ example[1] }}\n{% endfor %}\n{% endif %}\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Identity%20module/","title":"Identity module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Identity%20module/#synalinks.src.modules.core.identity.Identity","title":"<code>Identity</code>","text":"<p>               Bases: <code>Module</code></p> <p>Identity module.</p> <p>This module should be used as a placeholder when no operation is to be performed. The module just returns its <code>inputs</code> argument as output.</p> <p>This module can be really useful during development process in order to implement the whole program architecture before the individual modules.</p> <p>It avoid any data models naming issue that you could have by just forwarding the inputs, that way you can implement the general program architecture, validate it and implement the individual modules later.</p> <p>Example:</p> <pre><code>import synalinks\n\nclass MyAwesomeModule(synalinks.Program):\n\n    def __init__(\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n\n    async def build(self, inputs):\n        outputs = await synalinks.Identity()(inputs)\n\n        super().__init__(\n            inputs=inputs,\n            outputs=outputs,\n            name=self.name,\n            description=self.description,\n            trainable=self.trainable,\n        )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>The default module's arguments</p> <code>{}</code> Source code in <code>synalinks/src/modules/core/identity.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Identity\", \"synalinks.Identity\"])\nclass Identity(Module):\n    \"\"\"Identity module.\n\n    This module should be used as a placeholder when no operation is to be\n    performed. The module just returns its `inputs` argument as output.\n\n    This module can be really useful during development process in order to\n    implement the whole program architecture before the individual modules.\n\n    It avoid any data models naming issue that you could have by just\n    forwarding the inputs, that way you can implement the general\n    program architecture, validate it and implement the individual\n    modules later.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    class MyAwesomeModule(synalinks.Program):\n\n        def __init__(\n            name=None,\n            description=None,\n            trainable=True,\n        ):\n            super().__init__(\n                name=name,\n                description=description,\n                trainable=trainable,\n            )\n\n        async def build(self, inputs):\n            outputs = await synalinks.Identity()(inputs)\n\n            super().__init__(\n                inputs=inputs,\n                outputs=outputs,\n                name=self.name,\n                description=self.description,\n                trainable=self.trainable,\n            )\n    ```\n\n    Args:\n        **kwargs (keyword arguments): The default module's arguments\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.built = True\n\n    async def call(self, inputs):\n        if isinstance(inputs, (JsonDataModel, SymbolicDataModel)):\n            return inputs.clone()\n        return tree.map_structure(\n            lambda x: x.clone(),\n            inputs,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Input%20module/","title":"Input module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Input%20module/#synalinks.src.modules.core.input_module.Input","title":"<code>Input(schema=None, data_model=None, optional=False, name=None)</code>","text":"<p>Used to instantiate a <code>SymbolicDataModel</code>.</p> <p>A <code>SymbolicDataModel</code> is a symbolic data model-like object, which we augment with certain attributes that allow us to build a Synalinks <code>Program</code> just by knowing the inputs and outputs of the program (similar to Keras symbolic tensor).</p> <p>Example:</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str\n\ninputs = synalinks.Input(data_model=Query)\n\n# You can also create it using a JSON schema like this:\n\ninputs = synalinks.Input(schema=Query.get_schema())\n\n# Or using a symbolic datamodel:\n\ninputs = synalinks.Input(data_model=Query.to_symbolic_data_model())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>A Json schema of the data_model. If not provided uses the <code>data_model</code> argument.</p> <code>None</code> <code>data_model</code> <code>DataModel</code> <p>Optional existing data model to wrap into the <code>Input</code> layer. If set, the module will use this data_model rather than creating a new placeholder data model.</p> <code>None</code> <code>optional</code> <code>bool</code> <p>Whether the input is optional or not. An optional input can accept <code>None</code> values.</p> <code>False</code> <code>name</code> <code>string</code> <p>Optional name string for the module. Should be unique in a program (do not reuse the same name twice). It will be autogenerated if it isn't provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>SymbolicDataModel</code> <p>The symbolic data model corresponding to the given data model/schema.</p> Source code in <code>synalinks/src/modules/core/input_module.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Input\", \"synalinks.Input\"])\ndef Input(\n    schema=None,\n    data_model=None,\n    optional=False,\n    name=None,\n):\n    \"\"\"Used to instantiate a `SymbolicDataModel`.\n\n    A `SymbolicDataModel` is a symbolic data model-like object, which we augment with\n    certain attributes that allow us to build a Synalinks `Program` just by knowing the\n    inputs and outputs of the program (similar to Keras symbolic tensor).\n\n    Example:\n\n    ```python\n    import synalinks\n\n    class Query(synalinks.DataModel):\n        query: str\n\n    inputs = synalinks.Input(data_model=Query)\n\n    # You can also create it using a JSON schema like this:\n\n    inputs = synalinks.Input(schema=Query.get_schema())\n\n    # Or using a symbolic datamodel:\n\n    inputs = synalinks.Input(data_model=Query.to_symbolic_data_model())\n    ```\n\n    Args:\n        schema (dict): A Json schema of the data_model.\n            If not provided uses the `data_model` argument.\n        data_model (DataModel): Optional existing data model to wrap into\n            the `Input` layer. If set, the module will use this data_model rather\n            than creating a new placeholder data model.\n        optional (bool): Whether the input is optional or not.\n            An optional input can accept `None` values.\n        name (string): Optional name string for the module.\n            Should be unique in a program (do not reuse the same name twice).\n            It will be autogenerated if it isn't provided.\n\n    Returns:\n        (SymbolicDataModel): The symbolic data model corresponding to\n            the given data model/schema.\n    \"\"\"\n    module = InputModule(\n        schema=schema,\n        input_data_model=data_model.to_symbolic_data_model() if data_model else None,\n        optional=optional,\n        name=name,\n    )\n    return module.output\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Not%20module/","title":"Not module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Not%20module/#synalinks.src.modules.core.not_module.Not","title":"<code>Not</code>","text":"<p>               Bases: <code>Module</code></p> <p>Not module.</p> <p>This module should be used as a placeholder when no operation is to be performed and the output should be None.</p> <p>This module is useful to implement stop conditions when combined with a conditional branch or as placeholder (like the Identity) before implementing guards that leverage the xor operation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>The default module's arguments</p> <code>{}</code> Source code in <code>synalinks/src/modules/core/not_module.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Not\", \"synalinks.Not\"])\nclass Not(Module):\n    \"\"\"Not module.\n\n    This module should be used as a placeholder when no operation is to be\n    performed and the output should be None.\n\n    This module is useful to implement stop conditions when combined with a conditional\n    branch or as placeholder (like the Identity) before implementing guards that leverage\n    the xor operation.\n\n    Args:\n        **kwargs (keyword arguments): The default module's arguments\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.built = True\n\n    async def call(self, inputs):\n        if isinstance(inputs, (JsonDataModel, SymbolicDataModel)):\n            return None\n        return tree.map_structure(\n            lambda x: None,\n            inputs,\n        )\n\n    async def compute_output_spec(self, inputs):\n        if isinstance(inputs, (JsonDataModel, SymbolicDataModel)):\n            return inputs.clone()\n        return tree.map_structure(\n            lambda x: x.clone(),\n            inputs,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/","title":"Tool module","text":""},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>Module</code></p> <p>A module that wraps an async function as a callable tool.</p> <p>The <code>Tool</code> module allows you to wrap any async function and use it as a module within Synalinks programs. It automatically extracts the function's schema from its type hints and docstring.</p> <p>Example:</p> <pre><code>import synalinks\n\n@synalinks.saving.register_synalinks_serializable()\nasync def calculate(expression: str):\n    \"\"\"Calculate the result of a mathematical expression.\n\n    Args:\n        expression (str): The mathematical expression to calculate.\n    \"\"\"\n    result = eval(expression)\n    return {\"result\": result}\n\ntool = synalinks.Tool(calculate)\nresult = await tool(expression=\"2 + 2\")\n</code></pre> Important <p>No Optional Parameters: All function parameters must be required. Optional parameters with default values are not supported because OpenAI and other providers require all parameters to be required in their structured output JSON schemas.</p> <p>Complete Docstring Required: The wrapped function must have a complete docstring with an <code>Args:</code> section that documents every parameter. The Tool extracts parameter descriptions from the docstring to build the JSON schema sent to the language model. Missing descriptions will raise a ValueError.</p> <p>Example of a properly documented tool function:</p> <pre><code>async def search(query: str, limit: int):\n    \"\"\"Search the database for matching records.\n\n    Args:\n        query (str): The search query string.\n        limit (int): Maximum number of results to return.\n    \"\"\"\n    # Implementation here\n    return {\"results\": [...]}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The async function to wrap as a tool.</p> required <code>name</code> <code>str</code> <p>Optional. The name of the module. Defaults to the function name.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module. Defaults to the function's docstring short description.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable. Defaults to False since tools typically don't have trainable state.</p> <code>False</code> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>@synalinks_export([\"synalinks.modules.Tool\", \"synalinks.Tool\"])\nclass Tool(Module):\n    \"\"\"A module that wraps an async function as a callable tool.\n\n    The `Tool` module allows you to wrap any async function and use it as a\n    module within Synalinks programs. It automatically extracts the function's\n    schema from its type hints and docstring.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    @synalinks.saving.register_synalinks_serializable()\n    async def calculate(expression: str):\n        \\\"\\\"\\\"Calculate the result of a mathematical expression.\n\n        Args:\n            expression (str): The mathematical expression to calculate.\n        \\\"\\\"\\\"\n        result = eval(expression)\n        return {\"result\": result}\n\n    tool = synalinks.Tool(calculate)\n    result = await tool(expression=\"2 + 2\")\n    ```\n\n    Important:\n        **No Optional Parameters**: All function parameters must be required.\n        Optional parameters with default values are not supported because\n        OpenAI and other providers require all parameters to be required\n        in their structured output JSON schemas.\n\n        **Complete Docstring Required**: The wrapped function must have a\n        complete docstring with an `Args:` section that documents every\n        parameter. The Tool extracts parameter descriptions from the docstring\n        to build the JSON schema sent to the language model. Missing\n        descriptions will raise a ValueError.\n\n        Example of a properly documented tool function:\n\n        ```python\n        async def search(query: str, limit: int):\n            \\\"\\\"\\\"Search the database for matching records.\n\n            Args:\n                query (str): The search query string.\n                limit (int): Maximum number of results to return.\n            \\\"\\\"\\\"\n            # Implementation here\n            return {\"results\": [...]}\n        ```\n\n    Args:\n        func (Callable): The async function to wrap as a tool.\n        name (str): Optional. The name of the module. Defaults to the function name.\n        description (str): Optional. The description of the module.\n            Defaults to the function's docstring short description.\n        trainable (bool): Whether the module's variables should be trainable.\n            Defaults to False since tools typically don't have trainable state.\n    \"\"\"\n\n    def __init__(\n        self,\n        func: typing.Callable,\n        name=None,\n        description=None,\n        trainable=False,\n    ):\n        self._func = func\n        if not inspect.iscoroutinefunction(self._func):\n            raise TypeError(f\"{self._func.__name__} is not an asynchronous function\")\n\n        doc = inspect.getdoc(func)\n        if not doc:\n            raise ValueError(f\"The tool ({self._func.__name__}) must have a docstring\")\n\n        self._docstring = docstring_parser.parse(doc)\n        self._signature = inspect.signature(func)\n        self._type_hints = typing.get_type_hints(func)\n        self._params_schema = {}\n        self._required_params = []\n\n        self._parse_arguments()\n\n        # Use function name if no name provided\n        if not name:\n            name = self._func.__name__\n\n        # Use docstring short description if no description provided\n        if not description:\n            description = self._docstring.short_description or \"\"\n\n        if not description:\n            logging.warning(\n                f\"The tool ({name}) has no description. \"\n                \"This is unsafe behavior and may lead to issues.\"\n            )\n\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n\n    def _parse_arguments(self):\n        \"\"\"Parse the function arguments to build the input parameter schema.\"\"\"\n        for param_name, param in self._signature.parameters.items():\n            param_schema = get_param_schema(\n                param_name,\n                param,\n                self._type_hints,\n                self._docstring,\n            )\n            self._params_schema[param_name] = param_schema\n            if param.default is param.empty:\n                self._required_params.append(param_name)\n\n    def _build_output_schema(self):\n        \"\"\"Build the output schema from the function's return type hint.\n\n        Since tools must always return a dict, this method ensures\n        the output schema is always of type \"object\".\n        \"\"\"\n        return_type = self._type_hints.get(\"return\", None)\n        base_schema = {\n            \"type\": \"object\",\n            \"title\": f\"{self.name}_output\",\n        }\n\n        if return_type is None:\n            # No return type hint, use generic object schema\n            base_schema[\"additionalProperties\"] = True\n            return base_schema\n\n        origin = typing.get_origin(return_type)\n        args = typing.get_args(return_type)\n\n        # Handle Dict[K, V] - extract value type for additionalProperties\n        if origin is dict or origin is typing.Dict:\n            if len(args) &gt;= 2:\n                value_type = args[1]\n                try:\n                    value_schema = json_schema_type(value_type)\n                    if isinstance(value_schema, dict):\n                        base_schema[\"additionalProperties\"] = value_schema\n                    else:\n                        base_schema[\"additionalProperties\"] = {\"type\": value_schema}\n                except ValueError:\n                    base_schema[\"additionalProperties\"] = True\n            else:\n                base_schema[\"additionalProperties\"] = True\n            return base_schema\n\n        # Handle TypedDict - extract properties from annotations\n        if hasattr(return_type, \"__annotations__\"):\n            properties = {}\n            required = []\n            for field_name, field_type in return_type.__annotations__.items():\n                try:\n                    field_schema = json_schema_type(field_type)\n                    if isinstance(field_schema, dict):\n                        properties[field_name] = field_schema\n                    else:\n                        properties[field_name] = {\"type\": field_schema}\n                    # Check if field is required (not Optional)\n                    if typing.get_origin(field_type) is not typing.Union:\n                        required.append(field_name)\n                    elif type(None) not in typing.get_args(field_type):\n                        required.append(field_name)\n                except ValueError:\n                    properties[field_name] = {}\n            if properties:\n                base_schema[\"properties\"] = properties\n                if required:\n                    base_schema[\"required\"] = required\n                base_schema[\"additionalProperties\"] = False\n                return base_schema\n\n        # Fallback to generic object schema\n        base_schema[\"additionalProperties\"] = True\n        return base_schema\n\n    def get_input_schema(self) -&gt; dict:\n        \"\"\"Get the JSON schema for this tool's input parameters.\n\n        Returns:\n            dict: The JSON schema describing the tool's input parameters.\n        \"\"\"\n        return {\n            \"additionalProperties\": False,\n            \"description\": self._docstring.short_description,\n            \"properties\": self._params_schema,\n            \"required\": self._required_params,\n            \"title\": self.name.title().replace(\"_\", \" \"),\n            \"type\": \"object\",\n        }\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=1, max=10),\n        retry=retry_if_exception_type((Exception,)),\n        reraise=True,\n    )\n    async def call(\n        self, training: bool = False, **kwargs: typing.Any\n    ) -&gt; typing.Optional[JsonDataModel]:\n        \"\"\"Execute the wrapped function with the provided arguments.\n\n        Args:\n            training (bool): Whether in training mode. Not used by Tool but\n                included for consistency with other modules.\n            **kwargs (Any): The arguments to pass to the wrapped function.\n\n        Returns:\n            JsonDataModel: The result wrapped in a JsonDataModel with the output schema.\n        \"\"\"\n        result = await self._func(**kwargs)\n        if result is None:\n            return None\n        if isinstance(result, dict):\n            return JsonDataModel(\n                json=result,\n                schema=self._build_output_schema(),\n                name=f\"{self.name}_output\",\n            )\n        # Wrap non-dict results in a dict\n        return JsonDataModel(\n            json={\"result\": result},\n            schema=self._build_output_schema(),\n            name=f\"{self.name}_output\",\n        )\n\n    async def compute_output_spec(\n        self, training: bool = False, **kwargs: typing.Any\n    ) -&gt; SymbolicDataModel:\n        \"\"\"Compute the output specification for the tool.\n\n        Uses the function's schema to define the output structure.\n\n        Args:\n            training (bool): Whether in training mode.\n            **kwargs (Any): The input arguments.\n\n        Returns:\n            SymbolicDataModel: A SymbolicDataModel with the tool's output schema.\n        \"\"\"\n        return SymbolicDataModel(\n            schema=self._build_output_schema(),\n            name=self.name,\n        )\n\n    def get_tool_schema(self) -&gt; dict:\n        \"\"\"Get the JSON schema for this tool's parameters.\n\n        Returns:\n            dict: The JSON schema describing the tool's input parameters.\n        \"\"\"\n        schema = {\n            \"additionalProperties\": False,\n            \"description\": self._docstring.short_description,\n            \"properties\": self._params_schema,\n            \"required\": self._required_params,\n            \"title\": self.name.title().replace(\"_\", \" \"),\n            \"type\": \"object\",\n        }\n        return schema\n\n    def get_config(self):\n        config = {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        func_config = {\"func\": serialization_lib.serialize_synalinks_object(self._func)}\n        return {**config, **func_config}\n\n    @classmethod\n    def from_config(cls, config):\n        func = serialization_lib.deserialize_synalinks_object(config.pop(\"func\"))\n        return cls(func=func, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.Tool.call","title":"<code>call(training=False, **kwargs)</code>  <code>async</code>","text":"<p>Execute the wrapped function with the provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>training</code> <code>bool</code> <p>Whether in training mode. Not used by Tool but included for consistency with other modules.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>The arguments to pass to the wrapped function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>JsonDataModel</code> <code>Optional[JsonDataModel]</code> <p>The result wrapped in a JsonDataModel with the output schema.</p> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=1, max=10),\n    retry=retry_if_exception_type((Exception,)),\n    reraise=True,\n)\nasync def call(\n    self, training: bool = False, **kwargs: typing.Any\n) -&gt; typing.Optional[JsonDataModel]:\n    \"\"\"Execute the wrapped function with the provided arguments.\n\n    Args:\n        training (bool): Whether in training mode. Not used by Tool but\n            included for consistency with other modules.\n        **kwargs (Any): The arguments to pass to the wrapped function.\n\n    Returns:\n        JsonDataModel: The result wrapped in a JsonDataModel with the output schema.\n    \"\"\"\n    result = await self._func(**kwargs)\n    if result is None:\n        return None\n    if isinstance(result, dict):\n        return JsonDataModel(\n            json=result,\n            schema=self._build_output_schema(),\n            name=f\"{self.name}_output\",\n        )\n    # Wrap non-dict results in a dict\n    return JsonDataModel(\n        json={\"result\": result},\n        schema=self._build_output_schema(),\n        name=f\"{self.name}_output\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.Tool.compute_output_spec","title":"<code>compute_output_spec(training=False, **kwargs)</code>  <code>async</code>","text":"<p>Compute the output specification for the tool.</p> <p>Uses the function's schema to define the output structure.</p> <p>Parameters:</p> Name Type Description Default <code>training</code> <code>bool</code> <p>Whether in training mode.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>The input arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SymbolicDataModel</code> <code>SymbolicDataModel</code> <p>A SymbolicDataModel with the tool's output schema.</p> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>async def compute_output_spec(\n    self, training: bool = False, **kwargs: typing.Any\n) -&gt; SymbolicDataModel:\n    \"\"\"Compute the output specification for the tool.\n\n    Uses the function's schema to define the output structure.\n\n    Args:\n        training (bool): Whether in training mode.\n        **kwargs (Any): The input arguments.\n\n    Returns:\n        SymbolicDataModel: A SymbolicDataModel with the tool's output schema.\n    \"\"\"\n    return SymbolicDataModel(\n        schema=self._build_output_schema(),\n        name=self.name,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.Tool.get_input_schema","title":"<code>get_input_schema()</code>","text":"<p>Get the JSON schema for this tool's input parameters.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The JSON schema describing the tool's input parameters.</p> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>def get_input_schema(self) -&gt; dict:\n    \"\"\"Get the JSON schema for this tool's input parameters.\n\n    Returns:\n        dict: The JSON schema describing the tool's input parameters.\n    \"\"\"\n    return {\n        \"additionalProperties\": False,\n        \"description\": self._docstring.short_description,\n        \"properties\": self._params_schema,\n        \"required\": self._required_params,\n        \"title\": self.name.title().replace(\"_\", \" \"),\n        \"type\": \"object\",\n    }\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.Tool.get_tool_schema","title":"<code>get_tool_schema()</code>","text":"<p>Get the JSON schema for this tool's parameters.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The JSON schema describing the tool's input parameters.</p> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>def get_tool_schema(self) -&gt; dict:\n    \"\"\"Get the JSON schema for this tool's parameters.\n\n    Returns:\n        dict: The JSON schema describing the tool's input parameters.\n    \"\"\"\n    schema = {\n        \"additionalProperties\": False,\n        \"description\": self._docstring.short_description,\n        \"properties\": self._params_schema,\n        \"required\": self._required_params,\n        \"title\": self.name.title().replace(\"_\", \" \"),\n        \"type\": \"object\",\n    }\n    return schema\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.get_param_schema","title":"<code>get_param_schema(param_name, param, type_hints, doc_parsed)</code>","text":"<p>Create a schema for a single parameter.</p> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>def get_param_schema(\n    param_name: str,\n    param: inspect.Parameter,\n    type_hints: typing.Dict[str, typing.Any],\n    doc_parsed: docstring_parser.Docstring,\n) -&gt; JsonSchema:\n    \"\"\"Create a schema for a single parameter.\"\"\"\n    if param_name not in type_hints:\n        raise ValueError(f\"Missing type hint for parameter '{param_name}'\")\n    param_type = type_hints[param_name]\n    param_type_str = json_schema_type(param_type)\n    descriptions = (p.description for p in doc_parsed.params if p.arg_name == param_name)\n    param_doc = next(descriptions, None)\n    if param_doc is None:\n        raise ValueError(f\"Missing description for parameter '{param_name}' in docstring\")\n\n    param_schema = {}\n    param_schema[\"description\"] = param_doc.replace(\"\\n\", \" \")\n    param_schema[\"title\"] = param_name.title().replace(\"_\", \" \")\n    if isinstance(param_type_str, dict):\n        param_schema.update(**param_type_str)\n    else:\n        param_schema[\"type\"] = param_type_str\n\n    if param.default is not param.empty:\n        param_schema[\"default\"] = param.default\n\n    return param_schema\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Core%20Modules/Tool%20module/#synalinks.src.modules.core.tool.json_schema_type","title":"<code>json_schema_type(py_type)</code>","text":"<p>Convert a Python type to a JSON schema type.</p> Source code in <code>synalinks/src/modules/core/tool.py</code> <pre><code>def json_schema_type(py_type: typing.Any) -&gt; JsonSchema:\n    \"\"\"Convert a Python type to a JSON schema type.\"\"\"\n    mapping = {\n        int: \"integer\",\n        float: \"number\",\n        bool: \"boolean\",\n        str: \"string\",\n        type(None): \"null\",\n    }\n\n    # Check if type is a basic type\n    if py_type in mapping:\n        return mapping[py_type]\n\n    # For unparameterized list and dict types\n    if py_type is list or py_type is typing.List:\n        return {\"type\": \"array\", \"items\": {}}\n    if py_type is dict or py_type is typing.Dict:\n        return {\"type\": \"object\", \"additionalProperties\": {}}\n\n    origin = typing.get_origin(py_type)\n    args = typing.get_args(py_type)\n\n    if origin is typing.Union:\n        # Handle Optional[type] which is Union[type, None]\n        if len(args) == 2 and type(None) in args:\n            return json_schema_type(args[0])\n        else:\n            return [json_schema_type(arg) for arg in args]\n\n    if origin is list or origin is typing.List:\n        schema_type = json_schema_type(args[0])\n        if isinstance(schema_type, dict):\n            return {\"type\": \"array\", \"items\": schema_type}\n        return {\n            \"type\": \"array\",\n            \"items\": {\"type\": json_schema_type(args[0])},\n        }\n\n    if origin is dict or origin is typing.Dict:\n        schema_type = json_schema_type(args[1])\n        if isinstance(schema_type, dict):\n            return {\"type\": \"object\", \"additionalProperties\": schema_type}\n        return {\"type\": \"object\", \"additionalProperties\": {\"type\": schema_type}}\n\n    raise ValueError(f\"Cannot convert {py_type} to a JSON schema type\")\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/","title":"Knowledge Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/#knowledge-modules","title":"Knowledge Modules","text":"<ul> <li>EmbedKnowledge module</li> <li>UpdateKnowledge module</li> <li>RetrieveKnowledge module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/EmbedKnowledge%20module/","title":"EmbedKnowledge module","text":""},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/EmbedKnowledge%20module/#synalinks.src.modules.knowledge.embed_knowledge.EmbedKnowledge","title":"<code>EmbedKnowledge</code>","text":"<p>               Bases: <code>Module</code></p> <p>Extracts a field of interest and generate the corresponding embedding vector.</p> <p>This module is designed to work with any data model structure. It supports to mask the entity fields in order to keep only one field to embed per data model.</p> <p>Note: Each data model should have the same field to compute the embedding     from like a <code>name</code> or <code>description</code> field using <code>in_mask</code>.     Or every data model should have only one field left after masking using     <code>out_mask</code> argument.</p> <pre><code>import synalinks\nimport asyncio\nfrom typing import Literal\n\nclass Document(synalinks.DataModel):\n    title: str = synalinks.Field(\n        description=\"The document title\",\n    )\n    text: str = synalinks.Field(\n        description=\"The document content\",\n    )\n\nasync def main():\n    inputs = synalinks.Input(data_model=Document)\n    outputs = await synalinks.EmbedKnowledge(\n        embedding_model=embedding_model,\n        in_mask=[\"text\"],\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"embbed_document\",\n        description=\"Embbed the given documents\"\n    )\n\n    doc = Document(\n        title=\"my title\",\n        text=\"my document\",\n    )\n\n    result = await program(doc)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>If you want to process batch asynchronously use <code>program.predict()</code> instead, see the FAQ to understand the difference between <code>program()</code> and <code>program.predict()</code></p> <p>Here is an example:</p> <pre><code>import synalinks\nimport asyncio\nimport numpy as np\nfrom typing import Literal\n\nclass Document(synalinks.Entity):\n    label: Literal[\"Document\"]\n    text: str = synalinks.Field(\n        description=\"The document content\",\n    )\n\nasync def main():\n    inputs = synalinks.Input(data_model=Document)\n    outputs = await synalinks.EmbedKnowledge(\n        embedding_model=embedding_model,\n        in_mask=[\"text\"],\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"embbed_document\",\n        description=\"Embbed the given documents\"\n    )\n\n    doc1 = Document(label=\"Document\", text=\"my document 1\")\n    doc2 = Document(label=\"Document\", text=\"my document 2\")\n    doc3 = Document(label=\"Document\", text=\"my document 3\")\n\n    docs = np.array([doc1, doc2, doc3], dtype=\"object\")\n\n    embedded_docs = await program.predict(docs)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> <code>None</code> <code>in_mask</code> <code>list</code> <p>A mask applied to keep specific entity fields.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>A mask applied to remove specific entity fields.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>False</code> Source code in <code>synalinks/src/modules/knowledge/embed_knowledge.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.EmbedKnowledge\",\n        \"synalinks.EmbedKnowledge\",\n    ]\n)\nclass EmbedKnowledge(Module):\n    \"\"\"Extracts a field of interest and generate the corresponding embedding vector.\n\n    This module is designed to work with any data model structure. It supports to mask the\n    entity fields in order to keep **only one** field to embed per data model.\n\n    **Note**: Each data model should have the *same field* to compute the embedding\n        from like a `name` or `description` field using `in_mask`.\n        **Or** every data model should have *only one field left* after masking using\n        `out_mask` argument.\n\n    ```python\n    import synalinks\n    import asyncio\n    from typing import Literal\n\n    class Document(synalinks.DataModel):\n        title: str = synalinks.Field(\n            description=\"The document title\",\n        )\n        text: str = synalinks.Field(\n            description=\"The document content\",\n        )\n\n    async def main():\n        inputs = synalinks.Input(data_model=Document)\n        outputs = await synalinks.EmbedKnowledge(\n            embedding_model=embedding_model,\n            in_mask=[\"text\"],\n        )(inputs)\n\n        program = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"embbed_document\",\n            description=\"Embbed the given documents\"\n        )\n\n        doc = Document(\n            title=\"my title\",\n            text=\"my document\",\n        )\n\n        result = await program(doc)\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    If you want to process batch asynchronously\n    use `program.predict()` instead, see the [FAQ](https://synalinks.github.io/synalinks/FAQ/#whats-the-difference-between-program-methods-predict-and-__call__)\n    to understand the difference between `program()` and `program.predict()`\n\n    Here is an example:\n\n    ```python\n    import synalinks\n    import asyncio\n    import numpy as np\n    from typing import Literal\n\n    class Document(synalinks.Entity):\n        label: Literal[\"Document\"]\n        text: str = synalinks.Field(\n            description=\"The document content\",\n        )\n\n    async def main():\n        inputs = synalinks.Input(data_model=Document)\n        outputs = await synalinks.EmbedKnowledge(\n            embedding_model=embedding_model,\n            in_mask=[\"text\"],\n        )(inputs)\n\n        program = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"embbed_document\",\n            description=\"Embbed the given documents\"\n        )\n\n        doc1 = Document(label=\"Document\", text=\"my document 1\")\n        doc2 = Document(label=\"Document\", text=\"my document 2\")\n        doc3 = Document(label=\"Document\", text=\"my document 3\")\n\n        docs = np.array([doc1, doc2, doc3], dtype=\"object\")\n\n        embedded_docs = await program.predict(docs)\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Args:\n        embedding_model (EmbeddingModel): The embedding model to use.\n        in_mask (list): A mask applied to keep specific entity fields.\n        out_mask (list): A mask applied to remove specific entity fields.\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_model=None,\n        in_mask=None,\n        out_mask=None,\n        name=None,\n        description=None,\n        trainable=False,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.embedding_model = embedding_model\n        self.in_mask = in_mask\n        self.out_mask = out_mask\n\n    async def _embed(self, data_model):\n        embeddings = data_model.get(\"embeddings\")\n        if embeddings:\n            warnings.warn(\n                \"Embeddings already generated for this data model. \"\n                \"Returning original data model.\"\n            )\n            return JsonDataModel(\n                json=data_model.get_json(),\n                schema=data_model.get_schema(),\n                name=\"embedded_\" + data_model.name,\n            )\n        masked_data_model = data_model\n        if self.out_mask:\n            masked_data_model = await ops.out_mask(\n                data_model,\n                mask=self.out_mask,\n                recursive=False,\n                name=\"out_mask_\" + data_model.name,\n            )\n        elif self.in_mask:\n            masked_data_model = await ops.in_mask(\n                data_model,\n                mask=self.in_mask,\n                recursive=False,\n                name=\"in_mask_\" + data_model.name,\n            )\n        embeddings = await ops.embedding(\n            masked_data_model,\n            embedding_model=self.embedding_model,\n            name=data_model.name + \"_embedding\",\n        )\n        if not embeddings or not embeddings.get(\"embeddings\"):\n            warnings.warn(\n                f\"No embeddings generated for data model {data_model.name}. \"\n                \"Please check that your schema is correct.\"\n            )\n            return None\n        embedding_list = embeddings.get(\"embeddings\")\n        if len(embedding_list) != 1:\n            warnings.warn(\n                \"Data models can only have one embedding vector per data model, \"\n                \"adjust `EmbedKnowledge` module's `in_mask` or `out_mask` \"\n                \"to keep only one field. Skipping embedding.\"\n            )\n            return None\n        vector = embedding_list[0]\n        return await ops.concat(\n            data_model,\n            EmbeddingVector(embedding=vector),\n            name=\"embedded_\" + data_model.name,\n        )\n\n    async def call(self, inputs):\n        if not inputs:\n            return None\n        return tree.map_structure(\n            lambda x: run_maybe_nested(self._embed(x)),\n            inputs,\n        )\n\n    async def compute_output_spec(self, inputs):\n        return tree.map_structure(\n            lambda x: x.clone(name=\"embedded_\" + x.name),\n            inputs,\n        )\n\n    def get_config(self):\n        config = {\n            \"in_mask\": self.in_mask,\n            \"out_mask\": self.out_mask,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        embedding_model_config = {\n            \"embedding_model\": serialization_lib.serialize_synalinks_object(\n                self.embedding_model\n            )\n        }\n        return {**embedding_model_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        embedding_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"embedding_model\")\n        )\n        return cls(embedding_model=embedding_model, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/RetrieveKnowledge%20module/","title":"RetrieveKnowledge module","text":""},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/RetrieveKnowledge%20module/#synalinks.src.modules.knowledge.retrieve_knowledge.RetrieveKnowledge","title":"<code>RetrieveKnowledge</code>","text":"<p>               Bases: <code>Module</code></p> <p>Module for retrieving knowledge from a knowledge base.</p> <p>This module uses a language model to generate search queries and retrieves relevant information from a knowledge base using configurable search methods.</p> <p>Parameters:</p> Name Type Description Default <code>knowledge_base</code> <code>KnowledgeBase</code> <p>The knowledge base to search.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model used to generate search queries.</p> <code>None</code> <code>data_models</code> <code>list</code> <p>List of data models to search. Defaults to all models in the knowledge base.</p> <code>None</code> <code>search_type</code> <code>str</code> <p>The type of search to perform. One of: - \"similarity\": Vector-based semantic search using embeddings. - \"fulltext\": BM25-based full-text search. - \"hybrid\": Combines both using Reciprocal Rank Fusion (default).</p> <code>'hybrid'</code> <code>k</code> <code>int</code> <p>Maximum number of results to return. Defaults to 10.</p> <code>10</code> <code>similarity_threshold</code> <code>float</code> <p>Maximum distance threshold for similarity search (lower = better match). Only used when search_type is \"similarity\" or \"hybrid\".</p> <code>None</code> <code>fulltext_threshold</code> <code>float</code> <p>Minimum BM25 score threshold for fulltext search (higher = better match). Only used when search_type is \"fulltext\" or \"hybrid\".</p> <code>None</code> <code>k_rank</code> <code>int</code> <p>RRF smoothing constant for hybrid search. Lower values emphasize top ranks more strongly. Defaults to 60.</p> <code>60</code> <code>prompt_template</code> <code>str</code> <p>Custom prompt template for the search query generator.</p> <code>None</code> <code>examples</code> <code>list</code> <p>Example inputs/outputs for few-shot learning.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Custom instructions for the search query generator.</p> <code>None</code> <code>seed_instructions</code> <code>str</code> <p>Seed instructions for variability.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Temperature for the language model. Defaults to 0.0.</p> <code>0.0</code> <code>use_inputs_schema</code> <code>bool</code> <p>Whether to include input schema in the prompt.</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Whether to include output schema in the prompt.</p> <code>False</code> <code>return_inputs</code> <code>bool</code> <p>Whether to include original inputs in the output.</p> <code>True</code> <code>return_query</code> <code>bool</code> <p>Whether to include the generated search query in the output.</p> <code>True</code> <code>name</code> <code>str</code> <p>Name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module is trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/knowledge/retrieve_knowledge.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.RetrieveKnowledge\",\n        \"synalinks.RetrieveKnowledge\",\n    ]\n)\nclass RetrieveKnowledge(Module):\n    \"\"\"Module for retrieving knowledge from a knowledge base.\n\n    This module uses a language model to generate search queries and retrieves\n    relevant information from a knowledge base using configurable search methods.\n\n    Args:\n        knowledge_base (KnowledgeBase): The knowledge base to search.\n        language_model (LanguageModel): The language model used to generate\n            search queries.\n        data_models (list): List of data models to search. Defaults to all\n            models in the knowledge base.\n        search_type (str): The type of search to perform. One of:\n            - \"similarity\": Vector-based semantic search using embeddings.\n            - \"fulltext\": BM25-based full-text search.\n            - \"hybrid\": Combines both using Reciprocal Rank Fusion (default).\n        k (int): Maximum number of results to return. Defaults to 10.\n        similarity_threshold (float): Maximum distance threshold for similarity\n            search (lower = better match). Only used when search_type is\n            \"similarity\" or \"hybrid\".\n        fulltext_threshold (float): Minimum BM25 score threshold for fulltext\n            search (higher = better match). Only used when search_type is\n            \"fulltext\" or \"hybrid\".\n        k_rank (int): RRF smoothing constant for hybrid search. Lower values\n            emphasize top ranks more strongly. Defaults to 60.\n        prompt_template (str): Custom prompt template for the search query\n            generator.\n        examples (list): Example inputs/outputs for few-shot learning.\n        instructions (str): Custom instructions for the search query generator.\n        seed_instructions (str): Seed instructions for variability.\n        temperature (float): Temperature for the language model. Defaults to 0.0.\n        use_inputs_schema (bool): Whether to include input schema in the prompt.\n        use_outputs_schema (bool): Whether to include output schema in the prompt.\n        return_inputs (bool): Whether to include original inputs in the output.\n        return_query (bool): Whether to include the generated search query in\n            the output.\n        name (str): Name of the module.\n        description (str): Description of the module.\n        trainable (bool): Whether the module is trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        knowledge_base=None,\n        language_model=None,\n        data_models=None,\n        search_type: Literal[\"similarity\", \"fulltext\", \"hybrid\"] = \"hybrid\",\n        k=10,\n        similarity_threshold=None,\n        fulltext_threshold=None,\n        k_rank=60,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        temperature=0.0,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        return_inputs=True,\n        return_query=True,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.knowledge_base = knowledge_base\n        self.language_model = language_model\n\n        if search_type not in SEARCH_TYPES:\n            raise ValueError(\n                f\"`search_type` must be one of {SEARCH_TYPES}, got '{search_type}'\"\n            )\n        self.search_type = search_type\n\n        self.k = k\n        self.similarity_threshold = similarity_threshold\n        self.fulltext_threshold = fulltext_threshold\n        self.k_rank = k_rank\n\n        self.prompt_template = prompt_template\n        self.examples = examples\n\n        if not data_models:\n            data_models = knowledge_base.get_symbolic_data_models()\n\n        self.data_models = data_models\n\n        tables = [data_model.get_schema().get(\"title\") for data_model in self.data_models]\n\n        if not instructions:\n            instructions = default_retriever_instructions(tables)\n\n        self.instructions = instructions\n        self.seed_instructions = seed_instructions\n        self.temperature = temperature\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.return_inputs = return_inputs\n        self.return_query = return_query\n\n        self.search_query_generator = Generator(\n            data_model=SearchQuery,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            instructions=self.instructions,\n            seed_instructions=self.seed_instructions,\n            temperature=self.temperature,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            return_inputs=False,\n            name=\"search_query_generator_\" + self.name,\n        )\n\n    async def _perform_search(self, search_terms, target_data_models):\n        \"\"\"Perform the search based on the configured search type.\n\n        Args:\n            search_terms: List of search query strings.\n            target_data_models: List of data models to search.\n\n        Returns:\n            List of search results.\n        \"\"\"\n        if self.search_type == \"similarity\":\n            return await self.knowledge_base.similarity_search(\n                search_terms,\n                data_models=target_data_models,\n                k=self.k,\n                threshold=self.similarity_threshold,\n            )\n        elif self.search_type == \"fulltext\":\n            return await self.knowledge_base.fulltext_search(\n                search_terms,\n                data_models=target_data_models,\n                k=self.k,\n                threshold=self.fulltext_threshold,\n            )\n        else:  # hybrid\n            return await self.knowledge_base.hybrid_search(\n                search_terms,\n                data_models=target_data_models,\n                k=self.k,\n                k_rank=self.k_rank,\n                similarity_threshold=self.similarity_threshold,\n                fulltext_threshold=self.fulltext_threshold,\n            )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n\n        # Generate search query using the language model\n        search_query = await self.search_query_generator(inputs, training=training)\n\n        if not search_query:\n            return None\n\n        # Get the tables and search terms from the generated query\n        tables = search_query.get_json().get(\"tables\", [])\n        search_terms = search_query.get_json().get(\"search\", [])\n\n        if not search_terms:\n            return None\n\n        # Filter data models to only those requested\n        target_data_models = []\n        for dm in self.data_models:\n            schema = dm.get_schema()\n            if schema.get(\"title\") in tables:\n                target_data_models.append(dm)\n\n        if not target_data_models:\n            target_data_models = self.data_models\n\n        # Perform search based on configured search type\n        search_results = await self._perform_search(search_terms, target_data_models)\n\n        results = JsonDataModel(\n            json={\"result\": search_results},\n            schema=GenericResult.get_schema(),\n            name=\"retrieval_results_\" + self.name,\n        )\n        if self.return_query:\n            results = await ops.logical_and(\n                search_query,\n                results,\n                name=\"results_with_query_\" + self.name,\n            )\n\n        if self.return_inputs:\n            results = await ops.logical_and(\n                inputs,\n                results,\n                name=\"results_with_inputs_\" + self.name,\n            )\n        return results\n\n    async def compute_output_spec(self, inputs, training=False):\n        search_query = await self.search_query_generator(inputs, training=training)\n        results = SymbolicDataModel(\n            schema=GenericResult.get_schema(),\n            name=\"retrieval_results_\" + self.name,\n        )\n        if self.return_query:\n            results = await ops.logical_and(\n                search_query,\n                results,\n                name=\"results_with_query_\" + self.name,\n            )\n        if self.return_inputs:\n            results = await ops.logical_and(\n                inputs,\n                results,\n                name=\"results_with_inputs_\" + self.name,\n            )\n        return results\n\n    def get_config(self):\n        config = {\n            \"search_type\": self.search_type,\n            \"k\": self.k,\n            \"similarity_threshold\": self.similarity_threshold,\n            \"fulltext_threshold\": self.fulltext_threshold,\n            \"k_rank\": self.k_rank,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"temperature\": self.temperature,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"return_inputs\": self.return_inputs,\n            \"return_query\": self.return_query,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        knowledge_base_config = {\n            \"knowledge_base\": serialization_lib.serialize_synalinks_object(\n                self.knowledge_base,\n            )\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        data_models_config = {\n            \"data_models\": [\n                (\n                    serialization_lib.serialize_synalinks_object(\n                        data_model.to_symbolic_data_model(\n                            name=\"data_models\" + (f\"_{i}_\" if i &gt; 0 else \"_\") + self.name\n                        )\n                    )\n                    if not is_symbolic_data_model(data_model)\n                    else serialization_lib.serialize_synalinks_object(data_model)\n                )\n                for i, data_model in enumerate(self.data_models)\n            ]\n        }\n        return {\n            **config,\n            **knowledge_base_config,\n            **language_model_config,\n            **data_models_config,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        knowledge_base = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"knowledge_base\"),\n        )\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\"),\n        )\n        data_models_config = config.pop(\"data_models\")\n        data_models = [\n            serialization_lib.deserialize_synalinks_object(data_model)\n            for data_model in data_models_config\n        ]\n        return cls(\n            knowledge_base=knowledge_base,\n            data_models=data_models,\n            language_model=language_model,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/RetrieveKnowledge%20module/#synalinks.src.modules.knowledge.retrieve_knowledge.default_retriever_instructions","title":"<code>default_retriever_instructions(tables)</code>","text":"<p>The default instructions for the entity retriever</p> Source code in <code>synalinks/src/modules/knowledge/retrieve_knowledge.py</code> <pre><code>def default_retriever_instructions(tables):\n    \"\"\"The default instructions for the entity retriever\"\"\"\n    return f\"\"\"\nYour task is to retrieve information among the following tables: {tables}.\nFirst, decide step-by-step which tables you need, then use the `search` to\nperform a lookup.\nThe `search` field should be a list of natural language search queries for the\ninformation to look for.\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/UpdateKnowledge%20module/","title":"UpdateKnowledge module","text":""},{"location":"Synalinks%20API/Modules%20API/Knowledge%20Modules/UpdateKnowledge%20module/#synalinks.src.modules.knowledge.update_knowledge.UpdateKnowledge","title":"<code>UpdateKnowledge</code>","text":"<p>               Bases: <code>Module</code></p> <p>Update (insert or upsert) data models in the given knowledge base.</p> <p>This module stores data models in the knowledge base, using the first field of the data model as the primary key for upsert operations.</p> <p>Parameters:</p> Name Type Description Default <code>knowledge_base</code> <code>KnowledgeBase</code> <p>The knowledge base to update.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>False</code> Source code in <code>synalinks/src/modules/knowledge/update_knowledge.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.UpdateKnowledge\",\n        \"synalinks.UpdateKnowledge\",\n    ]\n)\nclass UpdateKnowledge(Module):\n    \"\"\"Update (insert or upsert) data models in the given knowledge base.\n\n    This module stores data models in the knowledge base, using the first\n    field of the data model as the primary key for upsert operations.\n\n    Args:\n        knowledge_base (KnowledgeBase): The knowledge base to update.\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        knowledge_base=None,\n        name=None,\n        description=None,\n        trainable=False,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.knowledge_base = knowledge_base\n\n    async def _update(self, data_model):\n        await self.knowledge_base.update(data_model)\n        return data_model.clone(name=\"updated_\" + data_model.name)\n\n    async def call(self, inputs):\n        if not inputs:\n            return None\n        outputs = tree.map_structure(\n            lambda x: run_maybe_nested(self._update(x)),\n            inputs,\n        )\n        return outputs\n\n    async def compute_output_spec(self, inputs):\n        return tree.map_structure(\n            lambda x: x.clone(name=\"updated_\" + x.name),\n            inputs,\n        )\n\n    def get_config(self):\n        config = {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        knowledge_base_config = {\n            \"knowledge_base\": serialization_lib.serialize_synalinks_object(\n                self.knowledge_base\n            )\n        }\n        return {**knowledge_base_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        knowledge_base = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"knowledge_base\")\n        )\n        return cls(knowledge_base=knowledge_base, **config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Masking%20Modules/","title":"Masking Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Masking%20Modules/#masking-modules","title":"Masking Modules","text":"<ul> <li>InMask module</li> <li>OutMask module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Masking%20Modules/InMask%20module/","title":"InMask module","text":""},{"location":"Synalinks%20API/Modules%20API/Masking%20Modules/InMask%20module/#synalinks.src.modules.masking.in_mask.InMask","title":"<code>InMask</code>","text":"<p>               Bases: <code>Module</code></p> <p>A module to keep specific fields of the given data models</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"ollama/mistral\",\n)\n\nclass Document(synalinks.DataModel):\n    title: str = synalinks.Field(\n        description=\"The title of the document\",\n    )\n    text: str = synalinks.Field(\n        description=\"The content of the document\",\n    )\n\nclass Summary(synalinks.DataModel):\n    summary: str = synalinks.Field(\n        description=\"the concise summary of the document\",\n    )\n\nasync def main():\n    inputs = Input(data_model=Document)\n    summary = synalinks.ChainOfThought(\n        data_model=Summary,\n        language_model=language_model,\n    )(inputs)\n    masked_summary = synalinks.InMask(\n        # remove the thinking field from the chain of thought\n        # by keeping only the summary\n        mask=[\"summary\"],\n    )(summary)\n\n    program = Program(\n        inputs=inputs,\n        outputs=masked_summary,\n        name=\"summary_generator\",\n        description=\"Generate a summary from a document\",\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The list of keys to keep.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>False</code> Source code in <code>synalinks/src/modules/masking/in_mask.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.InMask\",\n        \"synalinks.modules.InMask\",\n    ]\n)\nclass InMask(Module):\n    \"\"\"A module to keep specific fields of the given data models\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    class Document(synalinks.DataModel):\n        title: str = synalinks.Field(\n            description=\"The title of the document\",\n        )\n        text: str = synalinks.Field(\n            description=\"The content of the document\",\n        )\n\n    class Summary(synalinks.DataModel):\n        summary: str = synalinks.Field(\n            description=\"the concise summary of the document\",\n        )\n\n    async def main():\n        inputs = Input(data_model=Document)\n        summary = synalinks.ChainOfThought(\n            data_model=Summary,\n            language_model=language_model,\n        )(inputs)\n        masked_summary = synalinks.InMask(\n            # remove the thinking field from the chain of thought\n            # by keeping only the summary\n            mask=[\"summary\"],\n        )(summary)\n\n        program = Program(\n            inputs=inputs,\n            outputs=masked_summary,\n            name=\"summary_generator\",\n            description=\"Generate a summary from a document\",\n        )\n\n    ```\n\n    Args:\n        mask (list): The list of keys to keep.\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        mask=None,\n        name=None,\n        description=None,\n        trainable=False,\n    ):\n        if not mask or not isinstance(mask, list):\n            raise ValueError(\"`mask` parameter should be a list of fields to keep\")\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.mask = mask\n\n    async def call(self, inputs):\n        outputs = tree.map_structure(\n            lambda x: x.in_mask(mask=self.mask),\n            inputs,\n        )\n        return outputs\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Masking%20Modules/OutMask%20module/","title":"OutMask module","text":""},{"location":"Synalinks%20API/Modules%20API/Masking%20Modules/OutMask%20module/#synalinks.src.modules.masking.out_mask.OutMask","title":"<code>OutMask</code>","text":"<p>               Bases: <code>Module</code></p> <p>A module to remove specific fields of the given data models</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nlanguage_model = synalinks.LanguageModel(\n    model=\"ollama/mistral\",\n)\n\nclass Document(synalinks.DataModel):\n    title: str = synalinks.Field(\n        description=\"The title of the document\",\n    )\n    text: str = synalinks.Field(\n        description=\"The content of the document\",\n    )\n\nclass Summary(synalinks.DataModel):\n    summary: str = synalinks.Field(\n        description=\"the concise summary of the document\",\n    )\n\nasync def main():\n    inputs = Input(data_model=Document)\n    summary = synalinks.ChainOfThought(\n        data_model=Summary,\n        language_model=language_model,\n    )(inputs)\n    masked_summary = synalinks.OutMask(\n        # remove the thinking field from the chain of thought\n        mask=[\"thinking\"],\n    )(summary)\n\n    program = Program(\n        inputs=inputs,\n        outputs=masked_summary,\n        name=\"summary_generator\",\n        description=\"Generate a summary from a document\",\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>list</code> <p>The list of keys to remove.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>False</code> Source code in <code>synalinks/src/modules/masking/out_mask.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.OutMask\",\n        \"synalinks.modules.OutMask\",\n    ]\n)\nclass OutMask(Module):\n    \"\"\"A module to remove specific fields of the given data models\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    class Document(synalinks.DataModel):\n        title: str = synalinks.Field(\n            description=\"The title of the document\",\n        )\n        text: str = synalinks.Field(\n            description=\"The content of the document\",\n        )\n\n    class Summary(synalinks.DataModel):\n        summary: str = synalinks.Field(\n            description=\"the concise summary of the document\",\n        )\n\n    async def main():\n        inputs = Input(data_model=Document)\n        summary = synalinks.ChainOfThought(\n            data_model=Summary,\n            language_model=language_model,\n        )(inputs)\n        masked_summary = synalinks.OutMask(\n            # remove the thinking field from the chain of thought\n            mask=[\"thinking\"],\n        )(summary)\n\n        program = Program(\n            inputs=inputs,\n            outputs=masked_summary,\n            name=\"summary_generator\",\n            description=\"Generate a summary from a document\",\n        )\n    ```\n\n    Args:\n        mask (list): The list of keys to remove.\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        mask=None,\n        name=None,\n        description=None,\n        trainable=False,\n    ):\n        if not mask or not isinstance(mask, list):\n            raise ValueError(\"`mask` parameter should be a list of fields to remove\")\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.mask = mask\n\n    async def call(self, inputs):\n        outputs = tree.map_structure(\n            lambda x: x.out_mask(mask=self.mask),\n            inputs,\n        )\n        return outputs\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/","title":"Merging Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/#merging-modules","title":"Merging Modules","text":"<ul> <li>Concat module</li> <li>And module</li> <li>Or module</li> <li>Xor module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/And%20module/","title":"And module","text":""},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/And%20module/#synalinks.src.modules.merging.logical_and.And","title":"<code>And</code>","text":"<p>               Bases: <code>Module</code></p> <p>Perform a logical And operation.</p> <p>It takes as input a list of data models, and returns a concatenation of them.</p> <p>If any input is None, then it output None.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Logical And (<code>&amp;</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>None</code> <code>None</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>Standard keyword arguments for the module.</p> <code>{}</code> Source code in <code>synalinks/src/modules/merging/logical_and.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.And\",\n        \"synalinks.modules.And\",\n    ]\n)\nclass And(Module):\n    \"\"\"Perform a logical And operation.\n\n    It takes as input a list of data models,\n    and returns a concatenation of them.\n\n    If any input is None, then it output None.\n\n    Table:\n\n    | `x1`   | `x2`   | Logical And (`&amp;`) |\n    | ------ | ------ | ----------------- |\n    | `x1`   | `x2`   | `x1 + x2`         |\n    | `x1`   | `None` | `None`            |\n    | `None` | `x2`   | `None`            |\n    | `None` | `None` | `None`            |\n\n    Args:\n        **kwargs (keyword arguments): Standard keyword arguments for the module.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    async def call(self, inputs, training=False):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output = await ops.logical_and(\n                output,\n                inputs[i],\n                name=f\"module_and_{i}_\" + self.name,\n            )\n        return output\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/Concat%20module/","title":"Concat module","text":""},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/Concat%20module/#synalinks.src.modules.merging.concat.Concat","title":"<code>Concat</code>","text":"<p>               Bases: <code>Module</code></p> <p>Perform a concatenation operation.</p> <p>It takes as input a list of data models, and returns a concatenation of them.</p> <p>If any input is None, an exception is raised.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Concat (<code>+</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>Exception</code> <code>None</code> <code>x2</code> <code>Exception</code> <code>None</code> <code>None</code> <code>Exception</code> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>Standard keyword arguments for the module.</p> <code>{}</code> Source code in <code>synalinks/src/modules/merging/concat.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.Concat\",\n        \"synalinks.Concatenate\",\n        \"synalinks.modules.Concat\",\n        \"synalinks.modules.Concatenate\",\n    ]\n)\nclass Concat(Module):\n    \"\"\"Perform a concatenation operation.\n\n    It takes as input a list of data models,\n    and returns a concatenation of them.\n\n    If any input is None, an exception is raised.\n\n    Table:\n\n    | `x1`   | `x2`   | Concat (`+`)      |\n    | ------ | ------ | ----------------- |\n    | `x1`   | `x2`   | `x1 + x2`         |\n    | `x1`   | `None` | `Exception`       |\n    | `None` | `x2`   | `Exception`       |\n    | `None` | `None` | `Exception`       |\n\n    Args:\n        **kwargs (keyword arguments): Standard keyword arguments for the module.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    async def call(self, inputs, training=False):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output = await ops.concat(\n                output,\n                inputs[i],\n                name=f\"module_concat_{i}_\" + self.name,\n            )\n        return output\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/Or%20module/","title":"Or module","text":""},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/Or%20module/#synalinks.src.modules.merging.logical_or.Or","title":"<code>Or</code>","text":"<p>               Bases: <code>Module</code></p> <p>Perform a logical Or operation.</p> <p>It takes as input a list of data models, and returns a concatenation of them (if all are provided) otherwise it output the one that is not None.</p> <p>If any input is None, it is ignored.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Logical Or (<code>|</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>x1</code> <code>None</code> <code>x2</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>Standard keyword arguments for the module.</p> <code>{}</code> Source code in <code>synalinks/src/modules/merging/logical_or.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.Or\",\n        \"synalinks.modules.Or\",\n    ]\n)\nclass Or(Module):\n    \"\"\"Perform a logical Or operation.\n\n    It takes as input a list of data models,\n    and returns a concatenation of them (if all are provided)\n    otherwise it output the one that is not None.\n\n    If any input is None, it is ignored.\n\n    Table:\n\n    | `x1`   | `x2`   | Logical Or (`|`) |\n    | ------ | ------ | ---------------- |\n    | `x1`   | `x2`   | `x1 + x2`        |\n    | `x1`   | `None` | `x1`             |\n    | `None` | `x2`   | `x2`             |\n    | `None` | `None` | `None`           |\n\n    Args:\n        **kwargs (keyword arguments): Standard keyword arguments for the module.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    async def call(self, inputs, training=False):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            output = await ops.logical_or(\n                output,\n                inputs[i],\n                name=f\"module_or_{i}_\" + self.name,\n            )\n        return output\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/Xor%20module/","title":"Xor module","text":""},{"location":"Synalinks%20API/Modules%20API/Merging%20Modules/Xor%20module/#synalinks.src.modules.merging.logical_xor.Xor","title":"<code>Xor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Perform a logical Xor operation.</p> <p>It takes as input a list of data models, If more than two data models are not None, then it output None. otherwise it output the one that is not None.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Logical Xor (<code>^</code>) <code>x1</code> <code>x2</code> <code>None</code> <code>x1</code> <code>None</code> <code>x1</code> <code>None</code> <code>x2</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>Standard keyword arguments for the module.</p> <code>{}</code> Source code in <code>synalinks/src/modules/merging/logical_xor.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.Xor\",\n        \"synalinks.modules.Xor\",\n    ]\n)\nclass Xor(Module):\n    \"\"\"Perform a logical Xor operation.\n\n    It takes as input a list of data models,\n    If more than two data models are not None, then it output None.\n    otherwise it output the one that is not None.\n\n    Table:\n\n    | `x1`   | `x2`   | Logical Xor (`^`)|\n    | ------ | ------ | ---------------- |\n    | `x1`   | `x2`   | `None`           |\n    | `x1`   | `None` | `x1`             |\n    | `None` | `x2`   | `x2`             |\n    | `None` | `None` | `None`           |\n\n    Args:\n        **kwargs (keyword arguments): Standard keyword arguments for the module.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    async def compute_output_spec(self, inputs, training=False):\n        return inputs[0].clone()\n\n    async def call(self, inputs, training=False):\n        output = inputs[0]\n        for i in range(1, len(inputs)):\n            if inputs[i]:\n                if not output:\n                    output = inputs[i]\n                else:\n                    return None\n        return output.clone(name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/","title":"Synthesis Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/#synthesis-modules","title":"Synthesis Modules","text":"<ul> <li>PythonSynthesis module</li> <li>SequentialPlanSynthesis module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/PythonSynthesis%20module/","title":"PythonSynthesis module","text":""},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/PythonSynthesis%20module/#synalinks.src.modules.synthesis.python_synthesis.PythonScript","title":"<code>PythonScript</code>","text":"<p>               Bases: <code>Trainable</code></p> <p>The python code to transform a JSON object into another JSON object</p> Source code in <code>synalinks/src/modules/synthesis/python_synthesis.py</code> <pre><code>class PythonScript(Trainable):\n    \"\"\"The python code to transform a JSON object into another JSON object\"\"\"\n\n    python_script: str = Field(\n        description=\"The python script to transform a JSON object into another object\"\n    )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/PythonSynthesis%20module/#synalinks.src.modules.synthesis.python_synthesis.PythonSynthesis","title":"<code>PythonSynthesis</code>","text":"<p>               Bases: <code>Module</code></p> <p>A code Python code transformation on JSON data.</p> <p>Note: This module is NOT completly safe (yet) for business applications.     Its is only provided for reseach purposes on program synthesis.     Altought the code don't evolve during inference, so it can't be prompt injected.</p> <p>This module features a python code as trainable variable, allowing the optimizers to refine the code during the training loop based on iterative feedback and automatic selection of the best script.</p> <p>This module works ONLY with advanced optimizers (NOT the <code>RandomFewShot</code> optimizer).</p> <p>The module executes the entire Python script and expects the result to be stored in a variable named 'result' at the end of execution.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\ndefault_python_script = \\\n\"\"\"\ndef transform(inputs):\n    # TODO implement the code to transform the input grid into the output grid\n    return {\"output_grid\": inputs.get(\"input_grid\")}\n\nresult = transform(inputs)\n\"\"\"\n\nasync def main():\n    inputs = synalinks.Input(\n        data_model=synalinks.datasets.arcagi.get_input_data_model(),\n    )\n    outputs = await synalinks.PythonSynthesis(\n        data_model=synalinks.datasets.arcagi.get_output_data_model()\n        python_script=default_python_script,\n        default_return_value={\"output_grid\": [[]]},\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"python_script_synthesis\",\n        description=\"A program to solve ARCAGI with python code\",\n    )\n</code></pre> <p>If you want to explore the future of neuro-symbolic self-evolving systems, contact us. While these systems are not \"hard\" to code thanks to Synalinks, they requires  technical knowledge and a deep understanding of multiple AI paradigm.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The target JSON schema. If not provided use the <code>data_model</code> to infer it.</p> <code>None</code> <code>data_model</code> <code>DataModel | SymbolicDataModel | JsonDataModel</code> <p>The target data model for structured output.</p> <code>None</code> <code>python_script</code> <code>str</code> <p>The default Python script.</p> <code>None</code> <code>seed_scripts</code> <code>list</code> <p>Optional. A list of Python scripts to use as seed for the evolution. If not provided, create a seed from the default configuration.</p> <code>None</code> <code>default_return_value</code> <code>dict</code> <p>Default return value.</p> <code>None</code> <code>return_python_script</code> <code>bool</code> <p>Wether or not to return the python script for  evaluation. (Default to False).</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Maximum execution time in seconds. (Default 5 seconds).</p> <code>5</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/synthesis/python_synthesis.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.PythonSynthesis\",\n        \"synalinks.PythonSynthesis\",\n    ]\n)\nclass PythonSynthesis(Module):\n    \"\"\"A code Python code transformation on JSON data.\n\n    **Note**: This module is **NOT** completly safe (yet) for business applications.\n        Its is only provided for reseach purposes on program synthesis.\n        Altought the code don't evolve during inference, so it can't be prompt injected.\n\n    This module features a python code as trainable variable, allowing the optimizers\n    to refine the code during the training loop based on iterative feedback and\n    automatic selection of the best script.\n\n    This module works **ONLY** with advanced optimizers (**NOT** the\n    `RandomFewShot` optimizer).\n\n    The module executes the entire Python script and expects the result to be stored\n    in a variable named 'result' at the end of execution.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    default_python_script = \\\\\n    \\\"\\\"\\\"\n    def transform(inputs):\n        # TODO implement the code to transform the input grid into the output grid\n        return {\"output_grid\": inputs.get(\"input_grid\")}\n\n    result = transform(inputs)\n    \\\"\\\"\\\"\n\n    async def main():\n        inputs = synalinks.Input(\n            data_model=synalinks.datasets.arcagi.get_input_data_model(),\n        )\n        outputs = await synalinks.PythonSynthesis(\n            data_model=synalinks.datasets.arcagi.get_output_data_model()\n            python_script=default_python_script,\n            default_return_value={\"output_grid\": [[]]},\n        )(inputs)\n\n        program = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"python_script_synthesis\",\n            description=\"A program to solve ARCAGI with python code\",\n        )\n    ```\n\n    If you want to explore the future of neuro-symbolic self-evolving systems, contact us.\n    While these systems are not \"hard\" to code thanks to Synalinks, they requires \n    technical knowledge and a deep understanding of multiple AI paradigm.\n\n    Args:\n        schema (dict): The target JSON schema.\n            If not provided use the `data_model` to infer it.\n        data_model (DataModel | SymbolicDataModel | JsonDataModel): The target data\n            model for structured output.\n        python_script (str): The default Python script.\n        seed_scripts (list): Optional. A list of Python scripts to use as seed\n            for the evolution. If not provided, create a seed from the default\n            configuration.\n        default_return_value (dict): Default return value.\n        return_python_script (bool): Wether or not to return the python script for \n            evaluation. (Default to False).\n        timeout (int): Maximum execution time in seconds. (Default 5 seconds).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        python_script=None,\n        seed_scripts=None,\n        default_return_value=None,\n        return_python_script=False,\n        timeout=5,\n        sandbox=False,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n\n        if not python_script:\n            raise ValueError(\"You should provide the `python_script` argument\")\n        self.python_script = python_script\n\n        if not default_return_value:\n            raise ValueError(\"You should provide the `default_return_value` argument\")\n\n        try:\n            jsonschema.validate(default_return_value, self.schema)\n        except ValidationError as e:\n            raise ValueError(\n                f\"`default_return_value` parameter does not conform to schema: {e}\"\n            )\n\n        self.default_return_value = default_return_value\n        self.return_python_script = return_python_script\n        self.timeout = timeout\n\n        if not seed_scripts:\n            seed_scripts = []\n        self.seed_scripts = seed_scripts\n\n        seed_candidates = [\n            {\"python_script\": seed_script} for seed_script in self.seed_scripts\n        ]\n\n        self.state = self.add_variable(\n            initializer=PythonScript(\n                python_script=self.python_script,\n                seed_candidates=seed_candidates,\n            ).get_json(),\n            data_model=PythonScript,\n            name=\"state_\" + self.name,\n        )\n\n    async def execute(self, inputs, python_script):\n        \"\"\"Execute the Python script with timeout using multiprocessing.\"\"\"\n        result_queue = Queue()\n\n        process = Process(\n            target=_execute_script_in_process,\n            args=(python_script, inputs.get_json(), self.schema, result_queue),\n        )\n        process.start()\n\n        start_time = asyncio.get_event_loop().time()\n        timeout_remaining = self.timeout\n\n        while process.is_alive() and timeout_remaining &gt; 0:\n            await asyncio.sleep(0.1)\n            elapsed = asyncio.get_event_loop().time() - start_time\n            timeout_remaining = self.timeout - elapsed\n\n        if process.is_alive():\n            process.terminate()\n            process.join(timeout=1)\n\n            if process.is_alive():\n                process.kill()\n                process.join()\n\n            return (\n                None,\n                \"\",\n                f\"Timeout Error: Script execution exceeded {self.timeout} second(s)\\n\",\n            )\n\n        process.join()\n\n        if not result_queue.empty():\n            result, stdout, stderr = result_queue.get()\n            return result, stdout, stderr\n        else:\n            return None, \"\", \"Error: Process terminated unexpectedly\\n\"\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        python_script = self.state.get(\"python_script\")\n        result, stdout, stderr = await self.execute(inputs, python_script)\n        if training:\n            predictions = self.state.get(\"current_predictions\")\n            if result:\n                if self.return_python_script:\n                    predictions.append(\n                        {\n                            \"inputs\": {\n                                **inputs.get_json(),\n                            },\n                            \"outputs\": {\n                                \"python_script\": python_script,\n                                **result,\n                                \"stdout\": stdout,\n                                \"stderr\": stderr,\n                            },\n                            \"reward\": None,\n                        }\n                    )\n                else:\n                    predictions.append(\n                        {\n                            \"inputs\": {\n                                **inputs.get_json(),\n                            },\n                            \"outputs\": {\n                                **result,\n                                \"stdout\": stdout,\n                                \"stderr\": stderr,\n                            },\n                            \"reward\": None,\n                        }\n                    )\n            else:\n                if self.return_python_script:\n                    predictions.append(\n                        {\n                            \"inputs\": {\n                                **inputs.get_json(),\n                            },\n                            \"outputs\": {\n                                \"python_script\": python_script,\n                                \"stdout\": stdout,\n                                \"stderr\": stderr,\n                            },\n                            \"reward\": None,\n                        }\n                    )\n                else:\n                    predictions.append(\n                        {\n                            \"inputs\": {\n                                **inputs.get_json(),\n                            },\n                            \"outputs\": {\n                                \"stdout\": stdout,\n                                \"stderr\": stderr,\n                            },\n                            \"reward\": None,\n                        }\n                    )\n        if result:\n            if self.return_python_script:\n                return JsonDataModel(\n                    json={\n                        \"python_script\": python_script,\n                        **result,\n                        \"stdout\": stdout,\n                        \"stderr\": stderr,\n                    },\n                    schema=self.schema,\n                    name=self.name,\n                )\n            else:\n                return JsonDataModel(\n                    json={\n                        **result,\n                        \"stdout\": stdout,\n                        \"stderr\": stderr,\n                    },\n                    schema=self.schema,\n                    name=self.name,\n                )\n        else:\n            if self.return_python_script:\n                return JsonDataModel(\n                    json={\n                        \"python_script\": python_script,\n                        **self.default_return_value,\n                        \"stdout\": stdout,\n                        \"stderr\": stderr,\n                    },\n                    schema=self.schema,\n                    name=self.name,\n                )\n            else:\n                return JsonDataModel(\n                    json={\n                        **self.default_return_value,\n                        \"stdout\": stdout,\n                        \"stderr\": stderr,\n                    },\n                    schema=self.schema,\n                    name=self.name,\n                )\n\n    async def compute_output_spec(self, inputs, training=False):\n        if self.return_python_script:\n            return await ops.concat(\n                await ops.out_mask(\n                    PythonScript.to_symbolic_data_model(),\n                    mask=list(Trainable.keys()),\n                    name=\"python_script_masked_\" + self.name,\n                ),\n                await ops.concat(\n                    SymbolicDataModel(schema=self.schema),\n                    PythonConsoleLog,\n                    name=\"python_logs_\" + self.name,\n                ),\n                name=self.name,\n            )\n        else:\n            return await ops.concat(\n                SymbolicDataModel(schema=self.schema),\n                PythonConsoleLog,\n                name=self.name,\n            )\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"python_script\": self.python_script,\n            \"seed_scripts\": self.seed_scripts,\n            \"default_return_value\": self.default_return_value,\n            \"return_python_script\": self.return_python_script,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/PythonSynthesis%20module/#synalinks.src.modules.synthesis.python_synthesis.PythonSynthesis.execute","title":"<code>execute(inputs, python_script)</code>  <code>async</code>","text":"<p>Execute the Python script with timeout using multiprocessing.</p> Source code in <code>synalinks/src/modules/synthesis/python_synthesis.py</code> <pre><code>async def execute(self, inputs, python_script):\n    \"\"\"Execute the Python script with timeout using multiprocessing.\"\"\"\n    result_queue = Queue()\n\n    process = Process(\n        target=_execute_script_in_process,\n        args=(python_script, inputs.get_json(), self.schema, result_queue),\n    )\n    process.start()\n\n    start_time = asyncio.get_event_loop().time()\n    timeout_remaining = self.timeout\n\n    while process.is_alive() and timeout_remaining &gt; 0:\n        await asyncio.sleep(0.1)\n        elapsed = asyncio.get_event_loop().time() - start_time\n        timeout_remaining = self.timeout - elapsed\n\n    if process.is_alive():\n        process.terminate()\n        process.join(timeout=1)\n\n        if process.is_alive():\n            process.kill()\n            process.join()\n\n        return (\n            None,\n            \"\",\n            f\"Timeout Error: Script execution exceeded {self.timeout} second(s)\\n\",\n        )\n\n    process.join()\n\n    if not result_queue.empty():\n        result, stdout, stderr = result_queue.get()\n        return result, stdout, stderr\n    else:\n        return None, \"\", \"Error: Process terminated unexpectedly\\n\"\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/PythonSynthesis%20module/#synalinks.src.modules.synthesis.python_synthesis.TimeoutException","title":"<code>TimeoutException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when script execution times out</p> Source code in <code>synalinks/src/modules/synthesis/python_synthesis.py</code> <pre><code>class TimeoutException(Exception):\n    \"\"\"Exception raised when script execution times out\"\"\"\n\n    pass\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/SequentialPlanSynthesis%20module/","title":"SequentialPlanSynthesis module","text":""},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/SequentialPlanSynthesis%20module/#synalinks.src.modules.synthesis.sequential_plan_synthesis.SequentialPlan","title":"<code>SequentialPlan</code>","text":"<p>               Bases: <code>Trainable</code></p> <p>The sequential step by step plan to achieve the task</p> Source code in <code>synalinks/src/modules/synthesis/sequential_plan_synthesis.py</code> <pre><code>class SequentialPlan(Trainable):\n    \"\"\"The sequential step by step plan to achieve the task\"\"\"\n\n    steps: List[str] = Field(\n        description=\"The list of steps\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/SequentialPlanSynthesis%20module/#synalinks.src.modules.synthesis.sequential_plan_synthesis.SequentialPlanSynthesis","title":"<code>SequentialPlanSynthesis</code>","text":"<p>               Bases: <code>Module</code></p> <p>A module that executes a sequential plan of steps.</p> <p>This module features a sequential plan as a trainable variable, allowing optimizers to refine the plan during the training loop based on iterative feedback.</p> <p>Basically learning to plan based on iterative feedback and automatic selection of the best plan.</p> <p>The module executes each step in the plan sequentially, passing the output of each step as input to the next step. The runner is responsible for executing each individual step. The most common runners are usually a <code>FunctionCallingAgent</code>, <code>ChainOfThought</code> or <code>Generator</code> module, but you can use any Module or Program.</p> <p>This module start by defaut without any plan, so it is equivalent to a single runner call.</p> <p>This module works ONLY with advanced optimizers (NOT the <code>RandomFewShot</code> optimizer).</p> <p>Note: The inputs are forwarded to the runner each time by concatenating the inputs with the previous steps outputs. So ensure that the runner doesn't returns the inputs, use <code>return_inputs=False</code> or <code>return_inputs_with_trajectory=False</code> when configuring your runner.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass FinalReport(synalinks.DataModel):\n    report: str = synalinks.Field(\n        description=\"The final report\",\n    )\n\nclass TaskSummary(synalinks.DataModel):\n    summary: str = synalinks.Field(\n        description=\"The summary of the executed task\",\n    )\n\nasync def main():\n    tools = # ... tools definition (see `FunctionCallingAgent`)\n\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.SequentialPlanSynthesis(\n        data_model=FinalReport,\n        language_model=language_model,\n        runner=synalinks.FunctionCallingAgent(\n            data_model=TaskSummary,\n            language_model=language_model,\n            tools=tools,\n            return_inputs_with_trajectory=False,\n        ),\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"planner_agent\",\n        description=\"An agent that learn a step by step plan to achieve a task\",\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The target JSON schema. If not provided use the <code>data_model</code> to infer it.</p> <code>None</code> <code>data_model</code> <code>DataModel | SymbolicDataModel | JsonDataModel</code> <p>The target data model for structured output.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>steps</code> <code>list</code> <p>Optional. The default list of steps being a list of strings.</p> <code>None</code> <code>seed_steps</code> <code>list</code> <p>Optional. A list of steps to use as seed for the optimization. If not provided, use the default steps as seed.</p> <code>None</code> <code>runner</code> <code>Module | Program</code> <p>Required. The runner that executes each step.</p> <code>None</code> <code>return_inputs</code> <code>bool</code> <p>Optional. Whether or not to concatenate the inputs to the outputs (Default to False).</p> <code>True</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/synthesis/sequential_plan_synthesis.py</code> <pre><code>class SequentialPlanSynthesis(Module):\n    \"\"\"A module that executes a sequential plan of steps.\n\n    This module features a sequential plan as a trainable variable, allowing\n    optimizers to refine the plan during the training loop based on iterative\n    feedback.\n\n    Basically learning to plan based on iterative feedback and automatic\n    selection of the best plan.\n\n    The module executes each step in the plan sequentially, passing the output\n    of each step as input to the next step. The runner is responsible for\n    executing each individual step. The most common runners are usually a\n    `FunctionCallingAgent`, `ChainOfThought` or `Generator` module, but you\n    can use any Module or Program.\n\n    This module start by defaut without any plan, so it is equivalent to a\n    single runner call.\n\n    This module works **ONLY** with advanced optimizers (**NOT** the\n    `RandomFewShot` optimizer).\n\n    **Note**: The inputs are forwarded to the runner each time by concatenating\n    the inputs with the previous steps outputs. So **ensure that the runner\n    doesn't returns the inputs**, use `return_inputs=False` or\n    `return_inputs_with_trajectory=False` when configuring your runner.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class FinalReport(synalinks.DataModel):\n        report: str = synalinks.Field(\n            description=\"The final report\",\n        )\n\n    class TaskSummary(synalinks.DataModel):\n        summary: str = synalinks.Field(\n            description=\"The summary of the executed task\",\n        )\n\n    async def main():\n        tools = # ... tools definition (see `FunctionCallingAgent`)\n\n        inputs = synalinks.Input(data_model=Query)\n        outputs = await synalinks.SequentialPlanSynthesis(\n            data_model=FinalReport,\n            language_model=language_model,\n            runner=synalinks.FunctionCallingAgent(\n                data_model=TaskSummary,\n                language_model=language_model,\n                tools=tools,\n                return_inputs_with_trajectory=False,\n            ),\n        )(inputs)\n\n        program = synalinks.Program(\n            inputs=inputs,\n            outputs=outputs,\n            name=\"planner_agent\",\n            description=\"An agent that learn a step by step plan to achieve a task\",\n        )\n\n    ```\n\n    Args:\n        schema (dict): The target JSON schema.\n            If not provided use the `data_model` to infer it.\n        data_model (DataModel | SymbolicDataModel | JsonDataModel): The target data\n            model for structured output.\n        language_model (LanguageModel): The language model to use.\n        steps (list): Optional. The default list of steps being a list of strings.\n        seed_steps (list): Optional. A list of steps to use as seed for the\n            optimization. If not provided, use the default steps as seed.\n        runner (Module | Program): Required. The runner that executes each step.\n        return_inputs (bool): Optional. Whether or not to concatenate the inputs to\n            the outputs (Default to False).\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        language_model=None,\n        steps=None,\n        seed_steps=None,\n        runner=None,\n        return_inputs=True,\n        reasoning_effort=None,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n\n        if not steps:\n            steps = []\n        self.steps = steps\n\n        if not seed_steps:\n            seed_steps = [[]]\n\n        self.seed_steps = seed_steps\n        if not runner:\n            raise ValueError(\"The `runner` parameter is required.\")\n        if not isinstance(runner, Module):\n            raise ValueError(\"The `runner` parameter should be a `Module` or `Program`.\")\n\n        self.language_model = language_model\n        self.runner = runner\n        self.return_inputs = return_inputs\n        self.reasoning_effort = reasoning_effort\n\n        self.state = self.add_variable(\n            initializer=SequentialPlan(\n                steps=self.steps,\n                seed_candidates=self.seed_steps,\n            ).get_json(),\n            data_model=SequentialPlan,\n            name=\"state\" + self.name,\n        )\n\n        self.final_generator = ChainOfThought(\n            schema=self.schema,\n            language_model=self.language_model,\n            return_inputs=self.return_inputs,\n            reasoning_effort=self.reasoning_effort,\n            name=\"final_generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        steps = self.state.get(\"steps\")\n        previous_steps = None\n        if steps:\n            for i, step in enumerate(steps):\n                step_result = await self.runner(inputs, training=training)\n                if not previous_steps:\n                    previous_steps = step_result\n                else:\n                    previous_steps = await ops.concat(\n                        previous_steps,\n                        step_result,\n                        name=+f\"step_{i}_with_inputs\" + self.name,\n                    )\n                inputs = await ops.concat(\n                    inputs,\n                    await ops.concat(\n                        previous_steps,\n                        Step(step=step),\n                        name=f\"step_{i}_\" + self.name,\n                    ),\n                    name=f\"step_{i}_with_inputs_\" + self.name,\n                )\n        else:\n            result = await self.runner(inputs, training=training)\n            inputs = await ops.concat(\n                inputs,\n                result,\n                name=\"with_inputs_\" + self.name,\n            )\n        return await self.final_generator(inputs, training=training)\n\n    async def compute_output_spec(self, inputs, training=False):\n        _ = await self.runner(inputs)\n        return await self.final_generator(inputs)\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"steps\": self.steps,\n            \"seed_steps\": self.seed_steps,\n            \"return_inputs\": self.return_inputs,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        runner_config = {\n            \"runner\": serialization_lib.serialize_synalinks_object(\n                self.runner,\n            )\n        }\n        return {\n            **config,\n            **language_model_config,\n            **runner_config,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\"),\n        )\n        runner = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"runner\"),\n        )\n        return cls(\n            language_model=language_model,\n            runner=runner,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Synthesis%20Modules/SequentialPlanSynthesis%20module/#synalinks.src.modules.synthesis.sequential_plan_synthesis.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>The individual step to execute</p> Source code in <code>synalinks/src/modules/synthesis/sequential_plan_synthesis.py</code> <pre><code>class Step(DataModel):\n    \"\"\"The individual step to execute\"\"\"\n\n    step: str = Field(\n        description=\"The step to execute\",\n    )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Test%20Time%20Compute%20Modules/","title":"Test Time Compute Modules","text":""},{"location":"Synalinks%20API/Modules%20API/Test%20Time%20Compute%20Modules/#test-time-compute-modules","title":"Test Time Compute Modules","text":"<ul> <li>ChainOfThought module</li> <li>SelfCritique module</li> </ul>"},{"location":"Synalinks%20API/Modules%20API/Test%20Time%20Compute%20Modules/ChainOfThought%20module/","title":"ChainOfThought module","text":""},{"location":"Synalinks%20API/Modules%20API/Test%20Time%20Compute%20Modules/ChainOfThought%20module/#synalinks.src.modules.ttc.chain_of_thought.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Module</code></p> <p>Useful to answer in a step by step manner.</p> <p>This component concatenate a thinking field to your data model/schema and generate a prediction allowing the LM to think step by step before answering.</p> <p>By default the reasoning_effort is set to 'low' which uses the model's internal reasoning capabilities (extended thinking) to populate the thinking field.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"anthropic/claude-3-7-sonnet-20250219\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    x1 = await synalinks.ChainOfThought(\n        data_model=Answer,\n        language_model=language_model,\n    )(x0)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x1,\n        name=\"answer_with_chain_of_thought\",\n        description=\"Useful to answer step by step\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> References <ul> <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</li> </ul> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>The target JSON schema. If not provided use the <code>data_model</code> to infer it.</p> <code>None</code> <code>data_model</code> <code>DataModel | SymbolicDataModel | JsonDataModel</code> <p>The target data model.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The jinja2 prompt template (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default list of examples, the examples are a list of tuples containing input/output JSON pairs.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>The default instructions being a string containing instructions for the language model.</p> <code>None</code> <code>seed_instructions</code> <code>list</code> <p>Optional. A list of instructions to use as seed for the optimization. If not provided, use the default instructions as seed.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none']. (Default to 'low'). If reasoning effort is none or disabled, a thinking field is automatically added to the output data model. Otherwise, the thinking field is automatically populated by the model's reasoning content.</p> <code>None</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>return_inputs</code> <code>bool</code> <p>Optional. Whether or not to concatenate the inputs to the outputs (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/ttc/chain_of_thought.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.ChainOfThought\",\n        \"synalinks.ChainOfThought\",\n    ]\n)\nclass ChainOfThought(Module):\n    \"\"\"Useful to answer in a step by step manner.\n\n    This component concatenate a thinking field to your data model/schema and generate\n    a prediction allowing the LM to think step by step before answering.\n\n    By default the reasoning_effort is set to 'low' which uses the model's internal\n    reasoning capabilities (extended thinking) to populate the thinking field.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class Answer(synalinks.DataModel):\n        answer: str = synalinks.Field(\n            description=\"The correct answer\",\n        )\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"anthropic/claude-3-7-sonnet-20250219\",\n        )\n\n        x0 = synalinks.Input(data_model=Query)\n        x1 = await synalinks.ChainOfThought(\n            data_model=Answer,\n            language_model=language_model,\n        )(x0)\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x1,\n            name=\"answer_with_chain_of_thought\",\n            description=\"Useful to answer step by step\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    References:\n        - [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n\n    Args:\n        schema (dict): The target JSON schema.\n            If not provided use the `data_model` to infer it.\n        data_model (DataModel | SymbolicDataModel | JsonDataModel): The target data model.\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The jinja2 prompt template (see `Generator`).\n        examples (list): The default list of examples, the examples\n            are a list of tuples containing input/output JSON pairs.\n        instructions (str): The default instructions being a string containing\n            instructions for the language model.\n        seed_instructions (list): Optional. A list of instructions to use as seed for the\n            optimization. If not provided, use the default instructions as seed.\n        temperature (float): Optional. The temperature for the LM call.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none'].\n            (Default to 'low'). If reasoning effort is none or disabled, a thinking\n            field is automatically added to the output data model. Otherwise,\n            the thinking field is automatically populated by the model's\n            reasoning content.\n        use_inputs_schema (bool): Optional. Whether or not use the inputs schema in\n            the prompt (Default to False) (see `Generator`).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs schema in\n            the prompt (Default to False) (see `Generator`).\n        return_inputs (bool): Optional. Whether or not to concatenate the inputs to\n            the outputs (Default to False) (see `Generator`).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        temperature=0.0,\n        reasoning_effort=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        return_inputs=False,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.examples = examples\n        self.instructions = instructions\n        self.seed_instructions = seed_instructions\n        self.temperature = temperature\n        # Default to \"low\" reasoning effort for ChainOfThought\n        if reasoning_effort is None:\n            reasoning_effort = \"low\"\n        self.reasoning_effort = reasoning_effort\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.return_inputs = return_inputs\n\n        final_data_model = Thinking + SymbolicDataModel(schema=self.schema)\n\n        self.generator = Generator(\n            data_model=final_data_model,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            instructions=self.instructions,\n            seed_instructions=self.seed_instructions,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            return_inputs=self.return_inputs,\n            name=\"generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        return await self.generator(inputs, training=training)\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"return_inputs\": self.return_inputs,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        return {\n            **config,\n            **language_model_config,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\"),\n        )\n        return cls(\n            language_model=language_model,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Modules%20API/Test%20Time%20Compute%20Modules/SelfCritique%20module/","title":"SelfCritique module","text":""},{"location":"Synalinks%20API/Modules%20API/Test%20Time%20Compute%20Modules/SelfCritique%20module/#synalinks.src.modules.ttc.self_critique.SelfCritique","title":"<code>SelfCritique</code>","text":"<p>               Bases: <code>Module</code></p> <p>Useful to critique the given inputs.</p> <p>This component critique the inputs given and eventually generate an intermediate reward between 0.0 and 1.0.</p> <p>You can enable or disable the intermediate reward computation by using the <code>return_reward</code> flag (default to True).</p> <p>To have more accurate results, ensure that the inputs are provided along with the output to evaluate using <code>return_inputs</code> in your modules.</p> <p>Example:</p> <pre><code>import synalink\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(\n        description=\"The correct answer\",\n    )\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    x1 = await synalinks.ChainOfThought(\n        data_model=Answer,\n        language_model=language_model,\n        return_inputs=True,\n    )(x0)\n    x2 = await synalinks.SelfCritique(\n        language_model=language_model,\n    )(x1)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x2,\n        name=\"answer_with_cot_and_self_critique\",\n        description=\"Useful to answer accurately\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The jinja2 prompt template (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default list of examples, the examples are a list of tuples containing input/output JSON pairs.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>The default instructions being a string containing instructions for the language model.</p> <code>None</code> <code>seed_instructions</code> <code>list</code> <p>Optional. A list of instructions to use as seed for the optimization. If not provided, use the default instructions as seed.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Optional. The temperature for the LM call.</p> <code>0.0</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>use_inputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the inputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>use_outputs_schema</code> <code>bool</code> <p>Optional. Whether or not use the outputs schema in the prompt (Default to False) (see <code>Generator</code>).</p> <code>False</code> <code>return_reward</code> <code>bool</code> <p>Optional. Whether or not to compute an intermediate reward.</p> <code>True</code> <code>return_inputs</code> <code>bool</code> <p>Optional. Whether or not to concatenate the inputs to the outputs (Default to True) (see <code>Generator</code>).</p> <code>True</code> <code>name</code> <code>str</code> <p>Optional. The name of the module.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the module.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the module's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/modules/ttc/self_critique.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.modules.SelfCritique\",\n        \"synalinks.SelfCritique\",\n    ]\n)\nclass SelfCritique(Module):\n    \"\"\"Useful to critique the given inputs.\n\n    This component critique the inputs given and eventually generate\n    an intermediate reward between 0.0 and 1.0.\n\n    You can enable or disable the intermediate reward computation by\n    using the `return_reward` flag (default to True).\n\n    To have more accurate results, ensure that the inputs are provided along\n    with the output to evaluate using `return_inputs` in your modules.\n\n    Example:\n\n    ```python\n    import synalink\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class Answer(synalinks.DataModel):\n        answer: str = synalinks.Field(\n            description=\"The correct answer\",\n        )\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        x0 = synalinks.Input(data_model=Query)\n        x1 = await synalinks.ChainOfThought(\n            data_model=Answer,\n            language_model=language_model,\n            return_inputs=True,\n        )(x0)\n        x2 = await synalinks.SelfCritique(\n            language_model=language_model,\n        )(x1)\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x2,\n            name=\"answer_with_cot_and_self_critique\",\n            description=\"Useful to answer accurately\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Args:\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The jinja2 prompt template (see `Generator`).\n        examples (list): The default list of examples, the examples\n            are a list of tuples containing input/output JSON pairs.\n        instructions (str): The default instructions being a string containing\n            instructions for the language model.\n        seed_instructions (list): Optional. A list of instructions to use as seed for the\n            optimization. If not provided, use the default instructions as seed.\n        temperature (float): Optional. The temperature for the LM call.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        use_inputs_schema (bool): Optional. Whether or not use the inputs schema in\n            the prompt (Default to False) (see `Generator`).\n        use_outputs_schema (bool): Optional. Whether or not use the outputs schema in\n            the prompt (Default to False) (see `Generator`).\n        return_reward (bool): Optional. Whether or not to compute an intermediate reward.\n        return_inputs (bool): Optional. Whether or not to concatenate the inputs to\n            the outputs (Default to True) (see `Generator`).\n        name (str): Optional. The name of the module.\n        description (str): Optional. The description of the module.\n        trainable (bool): Whether the module's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        seed_instructions=None,\n        temperature=0.0,\n        reasoning_effort=None,\n        use_inputs_schema=False,\n        use_outputs_schema=False,\n        return_reward=True,\n        return_inputs=True,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.examples = examples\n        self.instructions = instructions\n        self.seed_instructions = seed_instructions\n        self.temperature = temperature\n        self.reasoning_effort = reasoning_effort\n        self.use_inputs_schema = use_inputs_schema\n        self.use_outputs_schema = use_outputs_schema\n        self.return_reward = return_reward\n        self.return_inputs = return_inputs\n\n        if self.return_reward:\n            schema = CritiqueWithReward.get_schema()\n        else:\n            schema = Critique.get_schema()\n\n        self.generator = Generator(\n            schema=schema,\n            language_model=self.language_model,\n            prompt_template=self.prompt_template,\n            examples=self.examples,\n            instructions=self.instructions,\n            seed_instructions=self.seed_instructions,\n            temperature=self.temperature,\n            reasoning_effort=self.reasoning_effort,\n            use_inputs_schema=self.use_inputs_schema,\n            use_outputs_schema=self.use_outputs_schema,\n            return_inputs=self.return_inputs,\n            name=\"generator_\" + self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        return await self.generator(inputs, training=training)\n\n    def get_config(self):\n        config = {\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"seed_instructions\": self.seed_instructions,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n            \"use_inputs_schema\": self.use_inputs_schema,\n            \"use_outputs_schema\": self.use_outputs_schema,\n            \"return_reward\": self.return_reward,\n            \"return_inputs\": self.return_inputs,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model,\n            )\n        }\n        return {\n            **config,\n            **language_model_config,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\"),\n        )\n        return cls(\n            language_model=language_model,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/","title":"Ops API","text":""},{"location":"Synalinks%20API/Ops%20API/#ops-api","title":"Ops API","text":""},{"location":"Synalinks%20API/Ops%20API/#json-ops","title":"JSON Ops","text":"<ul> <li>concat function</li> <li>factorize function</li> <li>in_mask function</li> <li>out_mask function</li> <li>logical_and function</li> <li>logical_or function</li> <li>logical_xor function</li> <li>suffix function</li> <li>prefix function</li> </ul>"},{"location":"Synalinks%20API/Ops%20API/#language-models-ops","title":"Language Models Ops","text":"<ul> <li>predict function</li> </ul>"},{"location":"Synalinks%20API/Ops%20API/#embedding-models-ops","title":"Embedding Models Ops","text":"<ul> <li>embedding function</li> </ul>"},{"location":"Synalinks%20API/Ops%20API/Embedding%20Models%20Ops/","title":"Embedding Models Ops","text":""},{"location":"Synalinks%20API/Ops%20API/Embedding%20Models%20Ops/#synalinks.src.ops.embedding_models.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Extract the embedding vectors from a data model using an <code>EmbeddingModel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. Description of the operation.</p> <code>None</code> <code>**kwargs</code> <code>keyword warguments</code> <p>Additional keyword arguments send to the embedding model.</p> <code>{}</code> Source code in <code>synalinks/src/ops/embedding_models.py</code> <pre><code>class Embedding(Operation):\n    \"\"\"Extract the embedding vectors from a data model using an `EmbeddingModel`.\n\n    Args:\n        embedding_model (EmbeddingModel): The embedding model to use.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. Description of the operation.\n        **kwargs (keyword warguments): Additional keyword arguments send to the\n            embedding model.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_model=None,\n        name=None,\n        description=None,\n        **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.embedding_model = embedding_model\n        self.em_kwargs = kwargs\n\n    async def call(self, x):\n        texts = tree.flatten(tree.map_structure(lambda field: str(field), x.get_json()))\n        embeddings = await self.embedding_model(texts, **self.em_kwargs)\n        return JsonDataModel(data_model=Embeddings(**embeddings), name=self.name)\n\n    async def compute_output_spec(self, x):\n        return SymbolicDataModel(schema=Embeddings.get_schema(), name=self.name)\n\n    def get_config(self):\n        config = {\n            \"name\": self.name,\n            \"description\": self.description,\n        }\n        embedding_model_config = serialization_lib.serialize_synalinks_object(\n            self.embedding_model\n        )\n        config.update({\"em_kwargs\": self.em_kwargs})\n        return {\"embedding_model\": embedding_model_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        embedding_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"embedding_model\")\n        )\n        em_kwargs = config.pop(\"em_kwargs\")\n        return cls(embedding_model=embedding_model, **config, **em_kwargs)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/Embedding%20Models%20Ops/#synalinks.src.ops.embedding_models.embedding","title":"<code>embedding(x, embedding_model=None, name=None, description=None, **kwargs)</code>  <code>async</code>","text":"<p>Extract the embedding vectors from a data model using an <code>EmbeddingModel</code>.</p> <p>Embedding consist in converting the given data_model into a vector representation. This function always output a data model that uses <code>Embeddings</code> schema.</p> <p>If the input data model have multiple fields, each one is embedded.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data_model</p> required <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keywords forwarded to the EmbeddingModel call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data_model</p> Source code in <code>synalinks/src/ops/embedding_models.py</code> <pre><code>@synalinks_export([\"synalinks.ops.embedding\", \"synalinks.ops.embedding_models.embedding\"])\nasync def embedding(x, embedding_model=None, name=None, description=None, **kwargs):\n    \"\"\"Extract the embedding vectors from a data model using an `EmbeddingModel`.\n\n    Embedding consist in converting the given data_model into a vector representation.\n    This function always output a data model that uses `Embeddings` schema.\n\n    If the input data model have multiple fields, each one is embedded.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data_model\n        embedding_model (EmbeddingModel): The embedding model to use\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n        **kwargs (keyword arguments): Additional keywords forwarded to the\n            EmbeddingModel call.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data_model\n    \"\"\"\n    if embedding_model is None:\n        raise ValueError(\"You should provide the `embedding_model` argument\")\n    if any_symbolic_data_models(x):\n        return await Embedding(\n            embedding_model=embedding_model,\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await Embedding(\n        embedding_model=embedding_model,\n        name=name,\n        description=description,\n        **kwargs,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/","title":"JSON Ops","text":""},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.And","title":"<code>And</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Perform a logical <code>And</code> operation between data models.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class And(Operation):\n    \"\"\"Perform a logical `And` operation between data models.\"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n\n    async def call(self, x1, x2):\n        if x1 and x2:\n            json = concatenate_json(x1.get_json(), x2.get_json())\n            schema = concatenate_schema(x1.get_schema(), x2.get_schema())\n            return JsonDataModel(json=json, schema=schema, name=self.name)\n        elif x1 and not x2:\n            return None\n        elif not x1 and x2:\n            return None\n        else:\n            return None\n\n    async def compute_output_spec(self, x1, x2):\n        schema = concatenate_schema(x1.get_schema(), x2.get_schema())\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Concat","title":"<code>Concat</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Concatenate two data models together.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Concat(Operation):\n    \"\"\"Concatenate two data models together.\"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n\n    async def call(self, x1, x2):\n        if not x1:\n            raise ValueError(f\"Received x1={x1} and x2={x2}\")\n        if not x2:\n            raise ValueError(f\"Received x1={x1} and x2={x2}\")\n        json = concatenate_json(x1.get_json(), x2.get_json())\n        schema = concatenate_schema(x1.get_schema(), x2.get_schema())\n        return JsonDataModel(json=json, schema=schema, name=self.name)\n\n    async def compute_output_spec(self, x1, x2):\n        schema = concatenate_schema(x1.get_schema(), x2.get_schema())\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Factorize","title":"<code>Factorize</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Factorize a data model by grouping similar properties together.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Factorize(Operation):\n    \"\"\"Factorize a data model by grouping similar properties together.\"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n\n    async def call(self, x):\n        if not x:\n            return None\n        json = factorize_json(x.get_json())\n        schema = factorize_schema(x.get_schema())\n        return JsonDataModel(json=json, schema=schema, name=self.name)\n\n    async def compute_output_spec(self, x):\n        schema = factorize_schema(x.get_schema())\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.InMask","title":"<code>InMask</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Keep specific fields of a data model.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class InMask(Operation):\n    \"\"\"Keep specific fields of a data model.\"\"\"\n\n    def __init__(\n        self,\n        mask=None,\n        recursive=True,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.mask = mask\n        self.recursive = recursive\n\n    async def call(self, x):\n        if not x:\n            return None\n        json = in_mask_json(x.get_json(), mask=self.mask, recursive=self.recursive)\n        schema = in_mask_schema(x.get_schema(), mask=self.mask, recursive=self.recursive)\n        return JsonDataModel(json=json, schema=schema, name=self.name)\n\n    async def compute_output_spec(self, x):\n        schema = in_mask_schema(x.get_schema(), mask=self.mask, recursive=self.recursive)\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Not","title":"<code>Not</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Negation/invert operator to be used in logical flows.</p> <p>When used the output is always <code>None</code>.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Not(Operation):\n    \"\"\"Negation/invert operator to be used in logical flows.\n\n    When used the output is always `None`.\n    \"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n\n    async def call(self, x):\n        return None\n\n    async def compute_output_spec(self, x):\n        return SymbolicDataModel(schema=x.get_schema(), name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Or","title":"<code>Or</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Perform a logical <code>Or</code> operation between data models.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Or(Operation):\n    \"\"\"Perform a logical `Or` operation between data models.\"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n\n    async def call(self, x1, x2):\n        if x1 and x2:\n            json = concatenate_json(x1.get_json(), x2.get_json())\n            schema = concatenate_schema(x1.get_schema(), x2.get_schema())\n            return JsonDataModel(json=json, schema=schema, name=self.name)\n        elif x1 and not x2:\n            return JsonDataModel(\n                json=x1.get_json(), schema=x1.get_schema(), name=self.name\n            )\n        elif not x1 and x2:\n            return JsonDataModel(\n                json=x2.get_json(), schema=x2.get_schema(), name=self.name\n            )\n        else:\n            return None\n\n    async def compute_output_spec(self, x1, x2):\n        return SymbolicDataModel(schema=x1.get_schema(), name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.OutMask","title":"<code>OutMask</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Mask specific fields of a data model.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class OutMask(Operation):\n    \"\"\"Mask specific fields of a data model.\"\"\"\n\n    def __init__(\n        self,\n        mask=None,\n        recursive=True,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.mask = mask\n        self.recursive = recursive\n\n    async def call(self, x):\n        if not x:\n            return None\n        json = out_mask_json(x.get_json(), mask=self.mask, recursive=self.recursive)\n        schema = out_mask_schema(x.get_schema(), mask=self.mask, recursive=self.recursive)\n        return JsonDataModel(json=json, schema=schema, name=self.name)\n\n    async def compute_output_spec(self, x):\n        schema = out_mask_schema(x.get_schema(), mask=self.mask, recursive=self.recursive)\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Prefix","title":"<code>Prefix</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Add a prefix to all the data model fields (non-recursive).</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Prefix(Operation):\n    \"\"\"Add a prefix to **all** the data model fields (non-recursive).\"\"\"\n\n    def __init__(\n        self,\n        prefix=None,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.prefix = prefix\n\n    async def call(self, x):\n        if not x:\n            return None\n        json = prefix_json(x.get_json(), self.prefix)\n        schema = prefix_schema(x.get_schema(), self.prefix)\n        return JsonDataModel(json=json, schema=schema, name=self.name)\n\n    async def compute_output_spec(self, x):\n        schema = prefix_schema(x.get_schema(), self.prefix)\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Suffix","title":"<code>Suffix</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Add a suffix to all the data model fields (non-recursive).</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Suffix(Operation):\n    \"\"\"Add a suffix to **all** the data model fields (non-recursive).\"\"\"\n\n    def __init__(\n        self,\n        suffix=None,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        self.suffix = suffix\n\n    async def call(self, x):\n        if not x:\n            return None\n        json = suffix_json(x.get_json(), self.suffix)\n        schema = suffix_schema(x.get_schema(), self.suffix)\n        return JsonDataModel(json=json, schema=schema, name=self.name)\n\n    async def compute_output_spec(self, x):\n        schema = suffix_schema(x.get_schema(), self.suffix)\n        return SymbolicDataModel(schema=schema, name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.Xor","title":"<code>Xor</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Perform a logical <code>Xor</code> operation between data models.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>class Xor(Operation):\n    \"\"\"Perform a logical `Xor` operation between data models.\"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n\n    async def call(self, x1, x2):\n        if x1 and x2:\n            return None\n        elif x1 and not x2:\n            return JsonDataModel(\n                json=x1.get_json(), schema=x1.get_schema(), name=self.name\n            )\n        elif not x1 and x2:\n            return JsonDataModel(\n                json=x2.get_json(), schema=x2.get_schema(), name=self.name\n            )\n        else:\n            return None\n\n    async def compute_output_spec(self, x1, x2):\n        return SymbolicDataModel(schema=x1.get_schema(), name=self.name)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.concat","title":"<code>concat(x1, x2, name=None, description=None)</code>  <code>async</code>","text":"<p>Concatenate two data models together.</p> <p>Concatenation consist in creating a new data model containing all the elements of the two inputs into a new one. Each field name is made unique if needed by adding a numerical suffix <code>_n</code>, with <code>n</code> being an incremental integer.</p> <p>This operation is implemented in the <code>+</code> Python operator.</p> <p>If any of the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>If any of the inputs is None, then an exception is raised.</p> <p>If the keys are used more than once, a numerical suffix is added.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Concat (<code>+</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>Exception</code> <code>None</code> <code>x2</code> <code>Exception</code> <code>None</code> <code>None</code> <code>Exception</code> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the first input data model.</p> required <code>x2</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the second input data model.</p> required <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.ops.concat\",\n        \"synalinks.ops.concatenate\",\n        \"synalinks.ops.json.concat\",\n        \"synalinks.ops.json.concatenate\",\n    ]\n)\nasync def concat(x1, x2, name=None, description=None):\n    \"\"\"Concatenate two data models together.\n\n    Concatenation consist in creating a new data model containing\n    all the elements of the two inputs into a new one.\n    Each field name is made unique if needed by adding a numerical suffix `_n`,\n    with `n` being an incremental integer.\n\n    This operation is implemented in the `+` Python operator.\n\n    If any of the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    If any of the inputs is None, then an exception is raised.\n\n    If the keys are used more than once, a numerical suffix is added.\n\n    Table:\n\n    | `x1`   | `x2`   | Concat (`+`)      |\n    | ------ | ------ | ----------------- |\n    | `x1`   | `x2`   | `x1 + x2`         |\n    | `x1`   | `None` | `Exception`       |\n    | `None` | `x2`   | `Exception`       |\n    | `None` | `None` | `Exception`       |\n\n    Args:\n        x1 (JsonDataModel | SymbolicDataModel): the first input data model.\n        x2 (JsonDataModel | SymbolicDataModel): the second input data model.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model\n    \"\"\"\n    if any_symbolic_data_models(x1, x2):\n        return await Concat(\n            name=name,\n            description=description,\n        ).symbolic_call(x1, x2)\n    return await Concat(\n        name=name,\n        description=description,\n    )(x1, x2)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.factorize","title":"<code>factorize(x, name=None, description=None)</code>  <code>async</code>","text":"<p>Factorize a data model by grouping similar properties together.</p> <p>Factorization consist in grouping the same properties into lists. The property key of the resulting grouped property is changed to its plural form. For example <code>action</code> become <code>actions</code>, or <code>query</code> become <code>queries</code>.</p> <p>If the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>This operation is implemented in <code>.factorize()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model.</p> required <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.factorize\", \"synalinks.ops.json.factorize\"])\nasync def factorize(x, name=None, description=None):\n    \"\"\"Factorize a data model by grouping similar properties together.\n\n    Factorization consist in grouping the same properties into lists.\n    The property key of the resulting grouped property is changed to its plural form.\n    For example `action` become `actions`, or `query` become `queries`.\n\n    If the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    This operation is implemented in `.factorize()`\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model.\n    \"\"\"\n    if any_symbolic_data_models(x):\n        return await Factorize(\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await Factorize(\n        name=name,\n        description=description,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.in_mask","title":"<code>in_mask(x, mask=None, recursive=True, name=None, description=None)</code>  <code>async</code>","text":"<p>Keep specific fields of a data model.</p> <p>In masking consists in keeping the properties that match with the keys given in the mask. The masking process ignores the numerical suffixes that could be added by other operations.</p> <p>If the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model</p> required <code>mask</code> <code>list</code> <p>the input mask (list of keys)</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Whether or not to keep recursively for nested objects (default True).</p> <code>True</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.in_mask\", \"synalinks.ops.json.in_mask\"])\nasync def in_mask(x, mask=None, recursive=True, name=None, description=None):\n    \"\"\"Keep specific fields of a data model.\n\n    In masking consists in keeping the properties that match with the keys given\n    in the mask. The masking process ignores the numerical suffixes that could be added\n    by other operations.\n\n    If the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model\n        mask (list): the input mask (list of keys)\n        recursive (bool): Whether or not to keep\n            recursively for nested objects (default True).\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model.\n    \"\"\"\n    if mask is None:\n        raise ValueError(\"You should specify the `mask` argument\")\n    if any_symbolic_data_models(x):\n        return await InMask(\n            mask=mask,\n            recursive=recursive,\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await InMask(\n        mask=mask,\n        recursive=recursive,\n        name=name,\n        description=description,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.logical_and","title":"<code>logical_and(x1, x2, name=None, description=None)</code>  <code>async</code>","text":"<p>Perform a logical <code>And</code> operation between two data models.</p> <p>If one of the inputs is <code>None</code>, then this operation output <code>None</code>. If both inputs are provided, the output is a concatenation of the two given data models.</p> <p>This operation is implemented in the Python <code>&amp;</code> operator.</p> <p>If any of the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Logical And (<code>&amp;</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>None</code> <code>None</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>JsonDataModel | SymbolicDataModel</code> <p>The first input data model.</p> required <code>x2</code> <code>JsonDataModel | SymbolicDataModel</code> <p>The second input data model.</p> required <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The resulting data model or None if the condition is not met.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.logical_and\", \"synalinks.ops.json.logical_and\"])\nasync def logical_and(x1, x2, name=None, description=None):\n    \"\"\"Perform a logical `And` operation between two data models.\n\n    If one of the inputs is `None`, then this operation output `None`.\n    If both inputs are provided, the output is a concatenation\n    of the two given data models.\n\n    This operation is implemented in the Python `&amp;` operator.\n\n    If any of the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    Table:\n\n    | `x1`   | `x2`   | Logical And (`&amp;`) |\n    | ------ | ------ | ----------------- |\n    | `x1`   | `x2`   | `x1 + x2`         |\n    | `x1`   | `None` | `None`            |\n    | `None` | `x2`   | `None`            |\n    | `None` | `None` | `None`            |\n\n    Args:\n        x1 (JsonDataModel | SymbolicDataModel): The first input data model.\n        x2 (JsonDataModel | SymbolicDataModel): The second input data model.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The resulting data model or\n            None if the condition is not met.\n    \"\"\"\n    if any_symbolic_data_models(x1, x2):\n        return await And(\n            name=name,\n            description=description,\n        ).symbolic_call(x1, x2)\n    return await And(\n        name=name,\n        description=description,\n    )(x1, x2)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.logical_not","title":"<code>logical_not(x, name=None, description=None)</code>  <code>async</code>","text":"<p>Negation/invert operator to be used in logical flows.</p> <p>When used the output is always <code>None</code>.</p> <p>If the data models used is a metaclass or symbolic data model the output is a symbolic data model with the same schema than the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model</p> required <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>`None` | SymbolicDataModel</code> <p>The resulting data model</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.logical_not\", \"synalinks.ops.json.logical_not\"])\nasync def logical_not(x, name=None, description=None):\n    \"\"\"Negation/invert operator to be used in logical flows.\n\n    When used the output is always `None`.\n\n    If the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model with the same schema than the input.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (`None` | SymbolicDataModel): The resulting data model\n    \"\"\"\n    if any_symbolic_data_models(x):\n        return await Not(\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await Not(\n        name=name,\n        description=description,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.logical_or","title":"<code>logical_or(x1, x2, name=None, description=None)</code>  <code>async</code>","text":"<p>Perform a logical <code>Or</code> between two data models.</p> <p>If one of the input is <code>None</code>, then output the other one. If both inputs are provided, the output is a concatenation of the two given data models.</p> <p>If any of the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>This operation is implemented in the Python <code>|</code> operator.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Logical Or (<code>|</code>) <code>x1</code> <code>x2</code> <code>x1 + x2</code> <code>x1</code> <code>None</code> <code>x1</code> <code>None</code> <code>x2</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>JsonDataModel | SymbolicDataModel</code> <p>The first input data model.</p> required <code>x2</code> <code>JsonDataModel | SymbolicDataModel</code> <p>The second input data model.</p> required <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The resulting data model or None if the condition is not met.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.logical_or\", \"synalinks.ops.json.logical_or\"])\nasync def logical_or(x1, x2, name=None, description=None):\n    \"\"\"Perform a logical `Or` between two data models.\n\n    If one of the input is `None`, then output the other one.\n    If both inputs are provided, the output is a concatenation\n    of the two given data models.\n\n    If any of the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    This operation is implemented in the Python `|` operator.\n\n    Table:\n\n    | `x1`   | `x2`   | Logical Or (`|`) |\n    | ------ | ------ | ---------------- |\n    | `x1`   | `x2`   | `x1 + x2`        |\n    | `x1`   | `None` | `x1`             |\n    | `None` | `x2`   | `x2`             |\n    | `None` | `None` | `None`           |\n\n    Args:\n        x1 (JsonDataModel | SymbolicDataModel): The first input data model.\n        x2 (JsonDataModel | SymbolicDataModel): The second input data model.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The resulting data model or\n            None if the condition is not met.\n    \"\"\"\n    if any_symbolic_data_models(x1, x2):\n        return await Or(\n            name=name,\n            description=description,\n        ).symbolic_call(x1, x2)\n    return await Or(\n        name=name,\n        description=description,\n    )(x1, x2)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.logical_xor","title":"<code>logical_xor(x1, x2, name=None, description=None)</code>  <code>async</code>","text":"<p>Perform a logical <code>Xor</code> between two data models.</p> <p>If one of the input is <code>None</code>, then output the other one. If both inputs are provided, the output is <code>None</code>.</p> <p>If any of the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>This operation is implemented in the Python <code>^</code> operator.</p> <p>Table:</p> <code>x1</code> <code>x2</code> Logical Xor (<code>^</code>) <code>x1</code> <code>x2</code> <code>None</code> <code>x1</code> <code>None</code> <code>x1</code> <code>None</code> <code>x2</code> <code>x2</code> <code>None</code> <code>None</code> <code>None</code> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>JsonDataModel | SymbolicDataModel</code> <p>The first input data model.</p> required <code>x2</code> <code>JsonDataModel | SymbolicDataModel</code> <p>The second input data model.</p> required <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel | None</code> <p>The resulting data model or None if the condition is not met.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.logical_xor\", \"synalinks.ops.json.logical_xor\"])\nasync def logical_xor(x1, x2, name=None, description=None):\n    \"\"\"Perform a logical `Xor` between two data models.\n\n    If one of the input is `None`, then output the other one.\n    If both inputs are provided, the output is `None`.\n\n    If any of the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    This operation is implemented in the Python `^` operator.\n\n    Table:\n\n    | `x1`   | `x2`   | Logical Xor (`^`)|\n    | ------ | ------ | ---------------- |\n    | `x1`   | `x2`   | `None`           |\n    | `x1`   | `None` | `x1`             |\n    | `None` | `x2`   | `x2`             |\n    | `None` | `None` | `None`           |\n\n    Args:\n        x1 (JsonDataModel | SymbolicDataModel): The first input data model.\n        x2 (JsonDataModel | SymbolicDataModel): The second input data model.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel | None): The resulting data model or\n            None if the condition is not met.\n    \"\"\"\n    if any_symbolic_data_models(x1, x2):\n        return await Xor(\n            name=name,\n            description=description,\n        ).symbolic_call(x1, x2)\n    return await Xor(\n        name=name,\n        description=description,\n    )(x1, x2)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.out_mask","title":"<code>out_mask(x, mask=None, recursive=True, name=None, description=None)</code>  <code>async</code>","text":"<p>Mask specific fields of a data model.</p> <p>Out masking consist in removing the properties that match with the keys given in the mask. The masking process ignore the numerical suffixes that could be added by other operations.</p> <p>If the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model.</p> required <code>mask</code> <code>list</code> <p>the input mask (list of keys).</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Whether or not to remove recursively for nested objects (default True).</p> <code>True</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.out_mask\", \"synalinks.ops.json.out_mask\"])\nasync def out_mask(x, mask=None, recursive=True, name=None, description=None):\n    \"\"\"Mask specific fields of a data model.\n\n    Out masking consist in removing the properties that match with the keys given\n    in the mask. The masking process ignore the numerical suffixes that could be added\n    by other operations.\n\n    If the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model.\n        mask (list): the input mask (list of keys).\n        recursive (bool): Whether or not to remove\n            recursively for nested objects (default True).\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model.\n    \"\"\"\n    if mask is None:\n        raise ValueError(\"You should specify the `mask` argument\")\n    if any_symbolic_data_models(x):\n        return await OutMask(\n            mask=mask,\n            recursive=recursive,\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await OutMask(\n        mask=mask,\n        recursive=recursive,\n        name=name,\n        description=description,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.prefix","title":"<code>prefix(x, prefix=None, name=None, description=None)</code>  <code>async</code>","text":"<p>Add a prefix to all the data model fields (non-recursive).</p> <p>If the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model</p> required <code>prefix</code> <code>str</code> <p>the prefix to add.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model.</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.prefix\", \"synalinks.ops.json.prefix\"])\nasync def prefix(x, prefix=None, name=None, description=None):\n    \"\"\"Add a prefix to **all** the data model fields (non-recursive).\n\n    If the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model\n        prefix (str): the prefix to add.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model.\n    \"\"\"\n    if prefix is None:\n        raise ValueError(\"You should specify the `prefix` argument\")\n    if any_symbolic_data_models(x):\n        return await Prefix(\n            prefix=prefix,\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await Prefix(\n        prefix=prefix,\n        name=name,\n        description=description,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/JSON%20Ops/#synalinks.src.ops.json.suffix","title":"<code>suffix(x, suffix=None, name=None, description=None)</code>  <code>async</code>","text":"<p>Add a suffix to all the data model fields (non-recursive).</p> <p>If the data models used is a metaclass or symbolic data model the output is a symbolic data model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model</p> required <code>suffix</code> <code>str</code> <p>the suffix to add.</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model</p> Source code in <code>synalinks/src/ops/json.py</code> <pre><code>@synalinks_export([\"synalinks.ops.suffix\", \"synalinks.ops.json.suffix\"])\nasync def suffix(x, suffix=None, name=None, description=None):\n    \"\"\"Add a suffix to **all** the data model fields (non-recursive).\n\n    If the data models used is a metaclass or symbolic data model\n    the output is a symbolic data model.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model\n        suffix (str): the suffix to add.\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model\n    \"\"\"\n    if suffix is None:\n        raise ValueError(\"You should specify the `suffix` argument\")\n    if any_symbolic_data_models(x):\n        return await Suffix(\n            suffix=suffix,\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await Suffix(\n        suffix=suffix,\n        name=name,\n        description=description,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/Language%20Models%20Ops/","title":"Language Models Ops","text":""},{"location":"Synalinks%20API/Ops%20API/Language%20Models%20Ops/#synalinks.src.ops.language_models.Predict","title":"<code>Predict</code>","text":"<p>               Bases: <code>Operation</code></p> <p>Perform a prediction using a <code>LanguageModel</code>.</p> Source code in <code>synalinks/src/ops/language_models.py</code> <pre><code>class Predict(Operation):\n    \"\"\"Perform a prediction using a `LanguageModel`.\"\"\"\n\n    def __init__(\n        self,\n        schema=None,\n        data_model=None,\n        language_model=None,\n        streaming=False,\n        name=None,\n        description=None,\n        **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n        )\n        if not schema and data_model:\n            schema = data_model.get_schema()\n        self.schema = schema\n        self.data_model = data_model\n        self.language_model = language_model\n        if schema and streaming:\n            streaming = False\n        self.streaming = streaming\n        self.lm_kwargs = kwargs\n\n    async def call(self, x):\n        value = await self.language_model(\n            x,\n            schema=self.schema,\n            streaming=self.streaming,\n            **self.lm_kwargs,\n        )\n        if isinstance(value, StreamingIterator):\n            return value\n        if not value:\n            return None\n        if self.schema:\n            return JsonDataModel(json=value, schema=self.schema, name=self.name)\n        else:\n            return JsonDataModel(\n                json=value, schema=ChatMessage.get_schema(), name=self.name\n            )\n\n    async def compute_output_spec(self, x):\n        if self.schema:\n            return SymbolicDataModel(schema=self.schema, name=self.name)\n        else:\n            return SymbolicDataModel(schema=ChatMessage.get_schema(), name=self.name)\n\n    def get_config(self):\n        config = {\n            \"schema\": self.schema,\n            \"streaming\": self.streaming,\n            \"name\": self.name,\n            \"description\": self.description,\n        }\n        language_model_config = serialization_lib.serialize_synalinks_object(\n            self.language_model\n        )\n        config.update({\"lm_kwargs\": self.lm_kwargs})\n        return {\"language_model\": language_model_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        lm_kwargs = config.pop(\"lm_kwargs\")\n        return cls(language_model=language_model, **config, **lm_kwargs)\n</code></pre>"},{"location":"Synalinks%20API/Ops%20API/Language%20Models%20Ops/#synalinks.src.ops.language_models.predict","title":"<code>predict(x, schema=None, data_model=None, language_model=None, streaming=False, name=None, description=None, **kwargs)</code>  <code>async</code>","text":"<p>Perform a prediction using a <code>LanguageModel</code>.</p> <p>Predict consist in predicting a target data_model from an input data_model. This function uses a backend DataModel to get the target schema.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>JsonDataModel | SymbolicDataModel</code> <p>the input data model.</p> required <code>data_model</code> <code>DataModel</code> <p>The target data model.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Enable streaming if True (Default to False)</p> <code>False</code> <code>name</code> <code>str</code> <p>Optional. The name of the operation.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the operation.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keywords forwarded to the LanguageModel call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>JsonDataModel | SymbolicDataModel</code> <p>The resulting data model.</p> Source code in <code>synalinks/src/ops/language_models.py</code> <pre><code>@synalinks_export([\"synalinks.ops.predict\", \"synalinks.ops.json.predict\"])\nasync def predict(\n    x,\n    schema=None,\n    data_model=None,\n    language_model=None,\n    streaming=False,\n    name=None,\n    description=None,\n    **kwargs,\n):\n    \"\"\"Perform a prediction using a `LanguageModel`.\n\n    Predict consist in predicting a target data_model from an input data_model.\n    This function uses a backend DataModel to get the target schema.\n\n    Args:\n        x (JsonDataModel | SymbolicDataModel): the input data model.\n        data_model (DataModel): The target data model.\n        language_model (LanguageModel): The language model to use\n        streaming (bool): Enable streaming if True (Default to False)\n        name (str): Optional. The name of the operation.\n        description (str): Optional. The description of the operation.\n        **kwargs (keyword arguments): Additional keywords forwarded to the\n            LanguageModel call.\n\n    Returns:\n        (JsonDataModel | SymbolicDataModel): The resulting data model.\n    \"\"\"\n    if language_model is None:\n        raise ValueError(\"You should provide the `language_model` argument\")\n    if any_symbolic_data_models(x):\n        return await Predict(\n            schema=schema,\n            data_model=data_model,\n            language_model=language_model,\n            streaming=False,\n            name=name,\n            description=description,\n        ).symbolic_call(x)\n    return await Predict(\n        schema=schema,\n        data_model=data_model,\n        language_model=language_model,\n        streaming=streaming,\n        name=name,\n        description=description,\n        **kwargs,\n    )(x)\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/","title":"Optimizers API","text":""},{"location":"Synalinks%20API/Optimizers%20API/#optimizers-api","title":"Optimizers API","text":"<p>The <code>Optimizer</code>s are a key element in Synalinks, they updates the variables and backpropagate the rewards.</p> <p>They are in charge of modifying and optimizing the variables (including the prompts) of each individual module composing a synalinks program.</p> <pre><code>graph LR\nA[Training Data] --&gt;|Provide x:DataModel| B[Program];\nB --&gt;|Generate y_pred:JsonDataModel| C[Reward];\nA --&gt;|Provide y_true:DataModel| C;\nC --&gt;|Compute reward:Float| D[Optimizer];\nD --&gt;|Update trainable_variable:Variable| B;</code></pre> <p>This reinforcement loop is what makes possible for the system to learn by repeatedly making predictions and refining its knowledge/methodology in order  to maximize the reward.</p>"},{"location":"Synalinks%20API/Optimizers%20API/#optimizers-api-overview","title":"Optimizers API overview","text":"<ul> <li>Base Optimizer class</li> <li>RandomFewShot optimizer</li> <li>OMEGA optimizer</li> </ul>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/","title":"Base Optimizer class","text":"<p>               Bases: <code>SynalinksSaveable</code></p> <p>Optimizer base class: all Synalinks optimizers inherit from this class.</p> <p>This abstract base class provides the common infrastructure for all optimizers in Synalinks.</p> <p>Concrete optimizer implementations must inherit from this class and implement the <code>propose_new_candidates()</code> method with their specific optimization logic.</p> <p>Parameters:</p> Name Type Description Default <code>population_size</code> <code>int</code> <p>The maximum number of best candidates to keep during the optimization process.</p> <code>10</code> <code>name</code> <code>str</code> <p>Optional. The name of the optimizer.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the optimizer.</p> <code>None</code> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>class Optimizer(SynalinksSaveable):\n    \"\"\"Optimizer base class: all Synalinks optimizers inherit from this class.\n\n    This abstract base class provides the common infrastructure for all\n    optimizers in Synalinks.\n\n    Concrete optimizer implementations must inherit from this class and implement\n    the `propose_new_candidates()` method with their specific optimization logic.\n\n    Args:\n        population_size (int): The maximum number of best candidates to keep\n            during the optimization process.\n        name (str): Optional. The name of the optimizer.\n        description (str): Optional. The description of the optimizer.\n    \"\"\"\n\n    def __init__(\n        self,\n        population_size=10,\n        name=None,\n        description=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the base optimizer.\n\n        Sets up the optimizer's internal state, variable tracking, and naming.\n\n        Args:\n            population_size (int): The maximum number of best candidates to keep\n                during the optimization process.\n            name (str): Optional name for the optimizer instance\n            description (str): Optional description for the optimizer\n            **kwargs (keyword params): Additional arguments (will raise error if provided)\n\n        Raises:\n            ValueError: If unexpected keyword arguments are provided\n        \"\"\"\n        self._lock = False\n\n        if kwargs:\n            raise ValueError(f\"Argument(s) not recognized: {kwargs}\")\n\n        self.population_size = population_size\n\n        if name is None:\n            name = auto_name(self.__class__.__name__)\n        self.name = name\n\n        if description is None:\n            if self.__class__.__doc__:\n                description = docstring_parser.parse(\n                    self.__class__.__doc__\n                ).short_description\n            else:\n                description = \"\"\n        self.description = description\n\n        self.built = False\n        self._program = None\n\n        self._initialize_tracker()\n\n        with backend.name_scope(self.name, caller=self):\n            iterations = backend.Variable(\n                initializer=Empty(data_model=Iterations),\n                data_model=Iterations,\n                trainable=False,\n                name=\"iterations_\" + self.name,\n            )\n        self._iterations = iterations\n\n    @property\n    def iterations(self):\n        \"\"\"Get the current iteration count.\n\n        Returns:\n            (int): Number of optimization iterations performed\n        \"\"\"\n        return self._iterations.get(\"iterations\")\n\n    @property\n    def epochs(self):\n        \"\"\"Get the current epoch number.\n\n        Returns:\n            (int): Number of epochs performed\n        \"\"\"\n        return self._iterations.get(\"epochs\")\n\n    def increment_iterations(self):\n        \"\"\"Increment the iteration counter by 1.\n\n        This method is called after each optimization step to track progress.\n        \"\"\"\n        iterations = self._iterations.get(\"iterations\")\n        self._iterations.update({\"iterations\": iterations + 1})\n\n    def increment_epochs(self):\n        \"\"\"Increment the epoch counter by 1.\n\n        This method is called after each epoch step to track progress.\n        \"\"\"\n        iterations = self._iterations.get(\"epochs\")\n        self._iterations.update({\"epochs\": iterations + 1})\n\n    def set_program(self, program):\n        \"\"\"Set the program that this optimizer will optimize.\n\n        The program contains the model/pipeline that the optimizer will work on.\n\n        Args:\n            program (Program): The Synalinks program to optimize\n        \"\"\"\n        self._program = program\n\n    @property\n    def program(self):\n        \"\"\"Get the program associated with this optimizer.\n\n        Returns:\n            (Program): The Synalinks program being optimized, or None if not set\n        \"\"\"\n        return self._program\n\n    @property\n    def reward_tracker(self):\n        \"\"\"Get the reward tracker from the associated program.\n\n        The reward tracker monitors the performance/rewards during optimization.\n\n        Returns:\n            (RewardTracker): The reward tracker from the program, or None if\n                no program is set.\n        \"\"\"\n        if self._program:\n            return self._program._reward_tracker\n        return None\n\n    @tracking.no_automatic_dependency_tracking\n    def _initialize_tracker(self):\n        if hasattr(self, \"_tracker\"):\n            return\n\n        trainable_variables = []\n        non_trainable_variables = []\n        modules = []\n        self._tracker = tracking.Tracker(\n            {\n                \"trainable_variables\": (\n                    lambda x: isinstance(x, backend.Variable) and x.trainable,\n                    trainable_variables,\n                ),\n                \"non_trainable_variables\": (\n                    lambda x: isinstance(x, backend.Variable) and not x.trainable,\n                    non_trainable_variables,\n                ),\n                \"modules\": (\n                    lambda x: isinstance(x, Module) and not isinstance(x, Metric),\n                    modules,\n                ),\n            },\n            exclusions={\"non_trainable_variables\": [\"trainable_variables\"]},\n        )\n        self._trainable_variables = trainable_variables\n        self._non_trainable_variables = non_trainable_variables\n        self._modules = modules\n\n    def __setattr__(self, name, value):\n        # Track Variables, Modules, Metrics.\n        if name != \"_tracker\":\n            if not hasattr(self, \"_tracker\"):\n                self._initialize_tracker()\n            value = self._tracker.track(value)\n        return super().__setattr__(name, value)\n\n    @property\n    def variables(self):\n        return self._non_trainable_variables[:] + self._trainable_variables[:]\n\n    @property\n    def non_trainable_variables(self):\n        return self._non_trainable_variables[:]\n\n    @property\n    def trainable_variables(self):\n        variables = []\n        for module in self._modules:\n            variables.extend(module.trainable_variables)\n        return variables\n\n    def save_own_variables(self, store):\n        \"\"\"Get the state of this optimizer object.\"\"\"\n        for i, variable in enumerate(self.variables):\n            store[str(i)] = variable.numpy()\n\n    def load_own_variables(self, store):\n        \"\"\"Set the state of this optimizer object.\"\"\"\n        if len(store.keys()) != len(self.variables):\n            msg = (\n                f\"Skipping variable loading for optimizer '{self.name}', \"\n                f\"because it has {len(self.variables)} variables whereas \"\n                f\"the saved optimizer has {len(store.keys())} variables. \"\n            )\n            if len(self.variables) == 0:\n                msg += (\n                    \"This is likely because the optimizer has not been called/built yet.\"\n                )\n            warnings.warn(msg, stacklevel=2)\n            return\n        for i, variable in enumerate(self.variables):\n            variable.assign(store[str(i)])\n\n    def _check_super_called(self):\n        if not hasattr(self, \"_lock\"):\n            raise RuntimeError(\n                f\"In optimizer '{self.__class__.__name__}', you forgot to call \"\n                \"`super().__init__()` as the first statement \"\n                \"in the `__init__()` method. \"\n                \"Go add it!\"\n            )\n\n    async def select_variable_name_to_update(self, trainable_variables):\n        rewards = []\n        for trainable_variable in trainable_variables:\n            nb_visit = trainable_variable.get(\"nb_visit\")\n            cumulative_reward = trainable_variable.get(\"cumulative_reward\")\n            if nb_visit == 0:\n                variable_reward = 100000\n            else:\n                variable_reward = cumulative_reward / nb_visit\n            rewards.append(variable_reward)\n        rewards = np.array(rewards)\n        inverted_rewards = -rewards\n        scaled_rewards = inverted_rewards / self.sampling_temperature\n        exp_rewards = np.exp(scaled_rewards - np.max(scaled_rewards))\n        probabilities = exp_rewards / np.sum(exp_rewards)\n        selected_variable = np.random.choice(\n            trainable_variables,\n            size=1,\n            replace=False,\n            p=probabilities,\n        ).tolist()[0]\n        return selected_variable.name\n\n    async def select_candidate_to_merge(\n        self,\n        step,\n        trainable_variable,\n    ):\n        best_candidates = trainable_variable.get(\"best_candidates\")\n        if len(best_candidates) &gt; 0:\n            selected_candidate = random.choice(best_candidates)\n            return selected_candidate\n        return None\n\n    async def on_train_begin(\n        self,\n        trainable_variables,\n    ):\n        \"\"\"Called at the beginning of the training\n\n        Args:\n            trainable_variables (list): The list of trainable variables.\n        \"\"\"\n        mask = list(Trainable.keys())\n        mask.remove(\"examples\")\n\n        for trainable_variable in trainable_variables:\n            seed_candidates = trainable_variable.get(\"seed_candidates\")\n            masked_variable = out_mask_json(\n                trainable_variable.get_json(),\n                mask=mask,\n            )\n            if not seed_candidates:\n                seed_candidates.append(\n                    {\n                        **masked_variable,\n                    }\n                )\n            trainable_variable.update(\n                {\n                    \"candidates\": [],\n                    \"best_candidates\": [],\n                }\n            )\n\n    async def on_train_end(\n        self,\n        trainable_variables,\n    ):\n        \"\"\"Called at the end of the training\n\n        Args:\n            trainable_variables (list): The list of trainable variables\n        \"\"\"\n        for variable in trainable_variables:\n            candidates = variable.get(\"candidates\")\n            best_candidates = variable.get(\"best_candidates\")\n            all_candidates = candidates + best_candidates\n            sorted_candidates = sorted(\n                all_candidates,\n                key=lambda x: x.get(\"reward\"),\n                reverse=True,\n            )\n            best_candidate = sorted_candidates[0]\n            best_candidate = out_mask_json(\n                best_candidate,\n                mask=[\"reward\"],\n            )\n            variable.update(\n                {\n                    **best_candidate,\n                },\n            )\n\n    async def on_epoch_begin(\n        self,\n        epoch,\n        trainable_variables,\n    ):\n        \"\"\"Called at the beginning of an epoch\n\n        Args:\n            epoch (int): The epoch number\n            trainable_variables (list): The list of trainable variables\n        \"\"\"\n        for trainable_variable in trainable_variables:\n            trainable_variable.update(\n                {\n                    \"predictions\": [],\n                    \"candidates\": [],\n                }\n            )\n\n    async def on_epoch_end(\n        self,\n        epoch,\n        trainable_variables,\n    ):\n        \"\"\"Called at the end of an epoch\n\n        Args:\n            epoch (int): The epoch number\n            trainable_variables (list): The list of trainable variables\n        \"\"\"\n        mask = list(Trainable.keys())\n        mask.remove(\"examples\")\n\n        for trainable_variable in trainable_variables:\n            candidates = trainable_variable.get(\"candidates\")\n            best_candidates = trainable_variable.get(\"best_candidates\")\n            all_candidates = candidates + best_candidates\n            sorted_candidates = sorted(\n                all_candidates,\n                key=lambda x: x.get(\"reward\"),\n                reverse=True,\n            )\n            selected_candidates = sorted_candidates[: self.population_size]\n            trainable_variable.update(\n                {\n                    \"best_candidates\": selected_candidates,\n                }\n            )\n            best_candidate = selected_candidates[0]\n            best_candidate = out_mask_json(\n                best_candidate,\n                mask=[\"reward\"],\n            )\n            trainable_variable.update(\n                {\n                    **best_candidate,\n                },\n            )\n            history = trainable_variable.get(\"history\")\n            if not history or history[-1] != best_candidate:\n                history.append(best_candidate)\n                trainable_variable.update({\"history\": history})\n        self.increment_epochs()\n\n    async def on_batch_begin(\n        self,\n        step,\n        epoch,\n        trainable_variables,\n    ):\n        \"\"\"Called at the beginning of a batch\n\n        Args:\n            step (int): The batch number\n            epoch (int): The epoch number\n            trainable_variables (list): The list of trainable variables\n        \"\"\"\n        for trainable_variable in trainable_variables:\n            best_candidates = trainable_variable.get(\"best_candidates\")\n            if epoch == 0:\n                seed_candidates = trainable_variable.get(\"seed_candidates\")\n                if len(seed_candidates) &gt; 0:\n                    seed_candidate = random.choice(seed_candidates)\n                    trainable_variable.update(\n                        {\n                            **seed_candidate,\n                        },\n                    )\n            else:\n                if len(best_candidates) &gt; 0:\n                    best_candidate = random.choice(best_candidates)\n                    best_candidate = out_mask_json(\n                        best_candidate,\n                        mask=[\"reward\"],\n                    )\n                    trainable_variable.update(\n                        {\n                            **best_candidate,\n                        },\n                    )\n                else:\n                    seed_candidates = trainable_variable.get(\"seed_candidates\")\n                    if len(seed_candidates) &gt; 0:\n                        seed_candidate = random.choice(seed_candidates)\n                        trainable_variable.update(\n                            {\n                                **seed_candidate,\n                            },\n                        )\n            trainable_variable.update(\n                {\n                    \"nb_visit\": 0,\n                    \"cumulative_reward\": 0.0,\n                },\n            )\n\n    async def on_batch_end(\n        self,\n        step,\n        epoch,\n        trainable_variables,\n    ):\n        \"\"\"Called at the end of a batch\n\n        Args:\n            step (int): The batch number\n            epoch (int): The epoch number\n            trainable_variables (list): The list of trainable variables\n        \"\"\"\n        for trainable_variable in trainable_variables:\n            candidates = trainable_variable.get(\"candidates\")\n            best_candidates = trainable_variable.get(\"best_candidates\")\n            all_candidates = candidates + best_candidates\n            if len(all_candidates) &gt; 0:\n                sorted_candidates = sorted(\n                    all_candidates,\n                    key=lambda x: x.get(\"reward\"),\n                    reverse=True,\n                )\n                best_candidate = sorted_candidates[0]\n                best_candidate = out_mask_json(\n                    best_candidate,\n                    mask=[\"reward\"],\n                )\n                trainable_variable.update(\n                    {\n                        **best_candidate,\n                    },\n                )\n        self.increment_iterations()\n\n    async def optimize(\n        self,\n        step,\n        trainable_variables,\n        x=None,\n        y=None,\n        val_x=None,\n        val_y=None,\n    ):\n        \"\"\"Method for performing optimization.\n\n        Args:\n            step (int): The training step.\n            trainable_variables (list): Variables to be optimized\n            x (np.ndarray): Training batch input data. Must be array-like.\n            y (np.ndarray): Training batch target data. Must be array-like.\n            val_x (np.ndarray): Input validation data. Must be array-like.\n            val_y (np.ndarray): Target validation data. Must be array-like.\n        \"\"\"\n        self._check_super_called()\n        if not self.built:\n            await self.build(trainable_variables)\n\n        y_pred = await self.program.predict_on_batch(\n            x=x,\n            training=True,\n        )\n\n        reward = await self.program.compute_reward(\n            x=x,\n            y=y,\n            y_pred=y_pred,\n        )\n\n        await self.assign_reward_to_predictions(\n            trainable_variables,\n            reward=reward,\n        )\n\n        await self.propose_new_candidates(\n            step,\n            trainable_variables,\n            x=x,\n            y=y,\n            y_pred=y_pred,\n            training=True,\n        )\n\n        y_pred = await self.program.predict_on_batch(\n            x=val_x,\n            training=False,\n        )\n\n        reward = await self.program.compute_reward(\n            x=val_x,\n            y=val_y,\n            y_pred=y_pred,\n        )\n\n        if self.trainable_variables:\n            await self.assign_reward_to_predictions(\n                self.trainable_variables,\n                reward=reward,\n            )\n\n        for trainable_variable in trainable_variables:\n            await self.maybe_add_candidate(\n                step,\n                trainable_variable,\n                reward=reward,\n            )\n\n        await self.reward_tracker.update_state(reward)\n        metrics = await self.program.compute_metrics(val_x, val_y, y_pred)\n        return metrics\n\n    async def propose_new_candidates(\n        self,\n        step,\n        trainable_variables,\n        x=None,\n        y=None,\n        y_pred=None,\n        training=False,\n    ):\n        raise NotImplementedError(\n            \"Optimizer subclasses must implement the `propose_new_candidates()` method.\"\n        )\n\n    async def assign_reward_to_predictions(\n        self,\n        trainable_variables,\n        reward=None,\n    ):\n        \"\"\"Assign rewards to predictions that don't have them yet.\n\n        This method updates all predictions in trainable variables that have\n        None as their reward value. It's typically called after computing\n        rewards for a batch of predictions.\n\n        Args:\n            trainable_variables (list): Variables containing predictions\n            reward (float): Reward value to assign (defaults to 0.0 if None/False)\n        \"\"\"\n        if not reward:\n            reward = 0.0\n        for trainable_variable in trainable_variables:\n            current_predictions = trainable_variable.get(\"current_predictions\")\n            predictions = trainable_variable.get(\"predictions\")\n            for p in current_predictions:\n                if p[\"reward\"] is None:\n                    p[\"reward\"] = reward\n                    nb_visit = trainable_variable.get(\"nb_visit\")\n                    cumulative_reward = trainable_variable.get(\"cumulative_reward\")\n                    trainable_variable.update(\n                        {\n                            \"nb_visit\": nb_visit + 1,\n                            \"cumulative_reward\": cumulative_reward + reward,\n                        }\n                    )\n            trainable_variable.update(\n                {\n                    \"predictions\": predictions + current_predictions,\n                    \"current_predictions\": [],\n                }\n            )\n\n    async def assign_candidate(\n        self,\n        trainable_variable,\n        new_candidate=None,\n        examples=None,\n    ):\n        \"\"\"Assign a new candidate configuration to a trainable variable.\n\n        This method updates a variable with either a complete new candidate\n        or just new examples for few-shot learning.\n\n        Args:\n            trainable_variable (Variable): The variable to update\n            new_candidate (JsonDataModel): New candidate (optional)\n            examples (list): New examples for few-shot learning (optional)\n        \"\"\"\n        if new_candidate:\n            if examples:\n                # Update with both new candidate and examples\n                trainable_variable.update(\n                    {\n                        **new_candidate.get_json(),\n                        \"examples\": examples,\n                    },\n                )\n            else:\n                # Update with just new candidate\n                trainable_variable.update(\n                    {\n                        **new_candidate.get_json(),\n                    },\n                )\n        elif examples:\n            # Update with just new examples\n            trainable_variable.update(\n                {\n                    \"examples\": examples,\n                },\n            )\n\n    async def maybe_add_candidate(\n        self,\n        step,\n        trainable_variable,\n        new_candidate=None,\n        examples=None,\n        reward=None,\n    ):\n        \"\"\"Maybe add new candidate to candidates.\n\n        Args:\n            step (int): The training step.\n            trainable_variable (Variable): The variable to add candidate to.\n            new_candidate (dict): New candidate configuration (optional).\n            examples (list): New examples for few-shot learning (optional).\n            reward (float): The candidate reward.\n        \"\"\"\n        if not reward:\n            reward = 0.0\n        mask = list(Trainable.keys())\n        mask.append(\"reward\")\n        if new_candidate:\n            new_candidate = out_mask_json(\n                new_candidate.get_json(),\n                mask=mask,\n            )\n        else:\n            new_candidate = out_mask_json(\n                trainable_variable.get_json(),\n                mask=mask,\n            )\n        if not examples:\n            examples = trainable_variable.get(\"examples\")\n\n        candidates = trainable_variable.get(\"candidates\")\n        best_candidates = trainable_variable.get(\"best_candidates\")\n        all_candidates = best_candidates + candidates\n        is_present = False\n        for candidate in all_candidates:\n            if out_mask_json(candidate, mask=mask) == new_candidate:\n                is_present = True\n                break\n        if not is_present:\n            candidates.append(\n                {\n                    **new_candidate,\n                    \"examples\": examples,\n                    \"reward\": reward,\n                }\n            )\n\n    def get_config(self):\n        return {\n            \"population_size\": self.population_size,\n            \"name\": self.name,\n            \"description\": self.description,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n    def __repr__(self):\n        return f\"&lt;Optimizer name={self.name} description={self.description}&gt;\"\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.epochs","title":"<code>epochs</code>  <code>property</code>","text":"<p>Get the current epoch number.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of epochs performed</p>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.iterations","title":"<code>iterations</code>  <code>property</code>","text":"<p>Get the current iteration count.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of optimization iterations performed</p>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.program","title":"<code>program</code>  <code>property</code>","text":"<p>Get the program associated with this optimizer.</p> <p>Returns:</p> Type Description <code>Program</code> <p>The Synalinks program being optimized, or None if not set</p>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.reward_tracker","title":"<code>reward_tracker</code>  <code>property</code>","text":"<p>Get the reward tracker from the associated program.</p> <p>The reward tracker monitors the performance/rewards during optimization.</p> <p>Returns:</p> Type Description <code>RewardTracker</code> <p>The reward tracker from the program, or None if no program is set.</p>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.__init__","title":"<code>__init__(population_size=10, name=None, description=None, **kwargs)</code>","text":"<p>Initialize the base optimizer.</p> <p>Sets up the optimizer's internal state, variable tracking, and naming.</p> <p>Parameters:</p> Name Type Description Default <code>population_size</code> <code>int</code> <p>The maximum number of best candidates to keep during the optimization process.</p> <code>10</code> <code>name</code> <code>str</code> <p>Optional name for the optimizer instance</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional description for the optimizer</p> <code>None</code> <code>**kwargs</code> <code>keyword params</code> <p>Additional arguments (will raise error if provided)</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unexpected keyword arguments are provided</p> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>def __init__(\n    self,\n    population_size=10,\n    name=None,\n    description=None,\n    **kwargs,\n):\n    \"\"\"Initialize the base optimizer.\n\n    Sets up the optimizer's internal state, variable tracking, and naming.\n\n    Args:\n        population_size (int): The maximum number of best candidates to keep\n            during the optimization process.\n        name (str): Optional name for the optimizer instance\n        description (str): Optional description for the optimizer\n        **kwargs (keyword params): Additional arguments (will raise error if provided)\n\n    Raises:\n        ValueError: If unexpected keyword arguments are provided\n    \"\"\"\n    self._lock = False\n\n    if kwargs:\n        raise ValueError(f\"Argument(s) not recognized: {kwargs}\")\n\n    self.population_size = population_size\n\n    if name is None:\n        name = auto_name(self.__class__.__name__)\n    self.name = name\n\n    if description is None:\n        if self.__class__.__doc__:\n            description = docstring_parser.parse(\n                self.__class__.__doc__\n            ).short_description\n        else:\n            description = \"\"\n    self.description = description\n\n    self.built = False\n    self._program = None\n\n    self._initialize_tracker()\n\n    with backend.name_scope(self.name, caller=self):\n        iterations = backend.Variable(\n            initializer=Empty(data_model=Iterations),\n            data_model=Iterations,\n            trainable=False,\n            name=\"iterations_\" + self.name,\n        )\n    self._iterations = iterations\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.assign_candidate","title":"<code>assign_candidate(trainable_variable, new_candidate=None, examples=None)</code>  <code>async</code>","text":"<p>Assign a new candidate configuration to a trainable variable.</p> <p>This method updates a variable with either a complete new candidate or just new examples for few-shot learning.</p> <p>Parameters:</p> Name Type Description Default <code>trainable_variable</code> <code>Variable</code> <p>The variable to update</p> required <code>new_candidate</code> <code>JsonDataModel</code> <p>New candidate (optional)</p> <code>None</code> <code>examples</code> <code>list</code> <p>New examples for few-shot learning (optional)</p> <code>None</code> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def assign_candidate(\n    self,\n    trainable_variable,\n    new_candidate=None,\n    examples=None,\n):\n    \"\"\"Assign a new candidate configuration to a trainable variable.\n\n    This method updates a variable with either a complete new candidate\n    or just new examples for few-shot learning.\n\n    Args:\n        trainable_variable (Variable): The variable to update\n        new_candidate (JsonDataModel): New candidate (optional)\n        examples (list): New examples for few-shot learning (optional)\n    \"\"\"\n    if new_candidate:\n        if examples:\n            # Update with both new candidate and examples\n            trainable_variable.update(\n                {\n                    **new_candidate.get_json(),\n                    \"examples\": examples,\n                },\n            )\n        else:\n            # Update with just new candidate\n            trainable_variable.update(\n                {\n                    **new_candidate.get_json(),\n                },\n            )\n    elif examples:\n        # Update with just new examples\n        trainable_variable.update(\n            {\n                \"examples\": examples,\n            },\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.assign_reward_to_predictions","title":"<code>assign_reward_to_predictions(trainable_variables, reward=None)</code>  <code>async</code>","text":"<p>Assign rewards to predictions that don't have them yet.</p> <p>This method updates all predictions in trainable variables that have None as their reward value. It's typically called after computing rewards for a batch of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>trainable_variables</code> <code>list</code> <p>Variables containing predictions</p> required <code>reward</code> <code>float</code> <p>Reward value to assign (defaults to 0.0 if None/False)</p> <code>None</code> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def assign_reward_to_predictions(\n    self,\n    trainable_variables,\n    reward=None,\n):\n    \"\"\"Assign rewards to predictions that don't have them yet.\n\n    This method updates all predictions in trainable variables that have\n    None as their reward value. It's typically called after computing\n    rewards for a batch of predictions.\n\n    Args:\n        trainable_variables (list): Variables containing predictions\n        reward (float): Reward value to assign (defaults to 0.0 if None/False)\n    \"\"\"\n    if not reward:\n        reward = 0.0\n    for trainable_variable in trainable_variables:\n        current_predictions = trainable_variable.get(\"current_predictions\")\n        predictions = trainable_variable.get(\"predictions\")\n        for p in current_predictions:\n            if p[\"reward\"] is None:\n                p[\"reward\"] = reward\n                nb_visit = trainable_variable.get(\"nb_visit\")\n                cumulative_reward = trainable_variable.get(\"cumulative_reward\")\n                trainable_variable.update(\n                    {\n                        \"nb_visit\": nb_visit + 1,\n                        \"cumulative_reward\": cumulative_reward + reward,\n                    }\n                )\n        trainable_variable.update(\n            {\n                \"predictions\": predictions + current_predictions,\n                \"current_predictions\": [],\n            }\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.increment_epochs","title":"<code>increment_epochs()</code>","text":"<p>Increment the epoch counter by 1.</p> <p>This method is called after each epoch step to track progress.</p> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>def increment_epochs(self):\n    \"\"\"Increment the epoch counter by 1.\n\n    This method is called after each epoch step to track progress.\n    \"\"\"\n    iterations = self._iterations.get(\"epochs\")\n    self._iterations.update({\"epochs\": iterations + 1})\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.increment_iterations","title":"<code>increment_iterations()</code>","text":"<p>Increment the iteration counter by 1.</p> <p>This method is called after each optimization step to track progress.</p> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>def increment_iterations(self):\n    \"\"\"Increment the iteration counter by 1.\n\n    This method is called after each optimization step to track progress.\n    \"\"\"\n    iterations = self._iterations.get(\"iterations\")\n    self._iterations.update({\"iterations\": iterations + 1})\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.load_own_variables","title":"<code>load_own_variables(store)</code>","text":"<p>Set the state of this optimizer object.</p> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>def load_own_variables(self, store):\n    \"\"\"Set the state of this optimizer object.\"\"\"\n    if len(store.keys()) != len(self.variables):\n        msg = (\n            f\"Skipping variable loading for optimizer '{self.name}', \"\n            f\"because it has {len(self.variables)} variables whereas \"\n            f\"the saved optimizer has {len(store.keys())} variables. \"\n        )\n        if len(self.variables) == 0:\n            msg += (\n                \"This is likely because the optimizer has not been called/built yet.\"\n            )\n        warnings.warn(msg, stacklevel=2)\n        return\n    for i, variable in enumerate(self.variables):\n        variable.assign(store[str(i)])\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.maybe_add_candidate","title":"<code>maybe_add_candidate(step, trainable_variable, new_candidate=None, examples=None, reward=None)</code>  <code>async</code>","text":"<p>Maybe add new candidate to candidates.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The training step.</p> required <code>trainable_variable</code> <code>Variable</code> <p>The variable to add candidate to.</p> required <code>new_candidate</code> <code>dict</code> <p>New candidate configuration (optional).</p> <code>None</code> <code>examples</code> <code>list</code> <p>New examples for few-shot learning (optional).</p> <code>None</code> <code>reward</code> <code>float</code> <p>The candidate reward.</p> <code>None</code> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def maybe_add_candidate(\n    self,\n    step,\n    trainable_variable,\n    new_candidate=None,\n    examples=None,\n    reward=None,\n):\n    \"\"\"Maybe add new candidate to candidates.\n\n    Args:\n        step (int): The training step.\n        trainable_variable (Variable): The variable to add candidate to.\n        new_candidate (dict): New candidate configuration (optional).\n        examples (list): New examples for few-shot learning (optional).\n        reward (float): The candidate reward.\n    \"\"\"\n    if not reward:\n        reward = 0.0\n    mask = list(Trainable.keys())\n    mask.append(\"reward\")\n    if new_candidate:\n        new_candidate = out_mask_json(\n            new_candidate.get_json(),\n            mask=mask,\n        )\n    else:\n        new_candidate = out_mask_json(\n            trainable_variable.get_json(),\n            mask=mask,\n        )\n    if not examples:\n        examples = trainable_variable.get(\"examples\")\n\n    candidates = trainable_variable.get(\"candidates\")\n    best_candidates = trainable_variable.get(\"best_candidates\")\n    all_candidates = best_candidates + candidates\n    is_present = False\n    for candidate in all_candidates:\n        if out_mask_json(candidate, mask=mask) == new_candidate:\n            is_present = True\n            break\n    if not is_present:\n        candidates.append(\n            {\n                **new_candidate,\n                \"examples\": examples,\n                \"reward\": reward,\n            }\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.on_batch_begin","title":"<code>on_batch_begin(step, epoch, trainable_variables)</code>  <code>async</code>","text":"<p>Called at the beginning of a batch</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The batch number</p> required <code>epoch</code> <code>int</code> <p>The epoch number</p> required <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def on_batch_begin(\n    self,\n    step,\n    epoch,\n    trainable_variables,\n):\n    \"\"\"Called at the beginning of a batch\n\n    Args:\n        step (int): The batch number\n        epoch (int): The epoch number\n        trainable_variables (list): The list of trainable variables\n    \"\"\"\n    for trainable_variable in trainable_variables:\n        best_candidates = trainable_variable.get(\"best_candidates\")\n        if epoch == 0:\n            seed_candidates = trainable_variable.get(\"seed_candidates\")\n            if len(seed_candidates) &gt; 0:\n                seed_candidate = random.choice(seed_candidates)\n                trainable_variable.update(\n                    {\n                        **seed_candidate,\n                    },\n                )\n        else:\n            if len(best_candidates) &gt; 0:\n                best_candidate = random.choice(best_candidates)\n                best_candidate = out_mask_json(\n                    best_candidate,\n                    mask=[\"reward\"],\n                )\n                trainable_variable.update(\n                    {\n                        **best_candidate,\n                    },\n                )\n            else:\n                seed_candidates = trainable_variable.get(\"seed_candidates\")\n                if len(seed_candidates) &gt; 0:\n                    seed_candidate = random.choice(seed_candidates)\n                    trainable_variable.update(\n                        {\n                            **seed_candidate,\n                        },\n                    )\n        trainable_variable.update(\n            {\n                \"nb_visit\": 0,\n                \"cumulative_reward\": 0.0,\n            },\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.on_batch_end","title":"<code>on_batch_end(step, epoch, trainable_variables)</code>  <code>async</code>","text":"<p>Called at the end of a batch</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The batch number</p> required <code>epoch</code> <code>int</code> <p>The epoch number</p> required <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def on_batch_end(\n    self,\n    step,\n    epoch,\n    trainable_variables,\n):\n    \"\"\"Called at the end of a batch\n\n    Args:\n        step (int): The batch number\n        epoch (int): The epoch number\n        trainable_variables (list): The list of trainable variables\n    \"\"\"\n    for trainable_variable in trainable_variables:\n        candidates = trainable_variable.get(\"candidates\")\n        best_candidates = trainable_variable.get(\"best_candidates\")\n        all_candidates = candidates + best_candidates\n        if len(all_candidates) &gt; 0:\n            sorted_candidates = sorted(\n                all_candidates,\n                key=lambda x: x.get(\"reward\"),\n                reverse=True,\n            )\n            best_candidate = sorted_candidates[0]\n            best_candidate = out_mask_json(\n                best_candidate,\n                mask=[\"reward\"],\n            )\n            trainable_variable.update(\n                {\n                    **best_candidate,\n                },\n            )\n    self.increment_iterations()\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.on_epoch_begin","title":"<code>on_epoch_begin(epoch, trainable_variables)</code>  <code>async</code>","text":"<p>Called at the beginning of an epoch</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number</p> required <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def on_epoch_begin(\n    self,\n    epoch,\n    trainable_variables,\n):\n    \"\"\"Called at the beginning of an epoch\n\n    Args:\n        epoch (int): The epoch number\n        trainable_variables (list): The list of trainable variables\n    \"\"\"\n    for trainable_variable in trainable_variables:\n        trainable_variable.update(\n            {\n                \"predictions\": [],\n                \"candidates\": [],\n            }\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.on_epoch_end","title":"<code>on_epoch_end(epoch, trainable_variables)</code>  <code>async</code>","text":"<p>Called at the end of an epoch</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number</p> required <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def on_epoch_end(\n    self,\n    epoch,\n    trainable_variables,\n):\n    \"\"\"Called at the end of an epoch\n\n    Args:\n        epoch (int): The epoch number\n        trainable_variables (list): The list of trainable variables\n    \"\"\"\n    mask = list(Trainable.keys())\n    mask.remove(\"examples\")\n\n    for trainable_variable in trainable_variables:\n        candidates = trainable_variable.get(\"candidates\")\n        best_candidates = trainable_variable.get(\"best_candidates\")\n        all_candidates = candidates + best_candidates\n        sorted_candidates = sorted(\n            all_candidates,\n            key=lambda x: x.get(\"reward\"),\n            reverse=True,\n        )\n        selected_candidates = sorted_candidates[: self.population_size]\n        trainable_variable.update(\n            {\n                \"best_candidates\": selected_candidates,\n            }\n        )\n        best_candidate = selected_candidates[0]\n        best_candidate = out_mask_json(\n            best_candidate,\n            mask=[\"reward\"],\n        )\n        trainable_variable.update(\n            {\n                **best_candidate,\n            },\n        )\n        history = trainable_variable.get(\"history\")\n        if not history or history[-1] != best_candidate:\n            history.append(best_candidate)\n            trainable_variable.update({\"history\": history})\n    self.increment_epochs()\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.on_train_begin","title":"<code>on_train_begin(trainable_variables)</code>  <code>async</code>","text":"<p>Called at the beginning of the training</p> <p>Parameters:</p> Name Type Description Default <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables.</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def on_train_begin(\n    self,\n    trainable_variables,\n):\n    \"\"\"Called at the beginning of the training\n\n    Args:\n        trainable_variables (list): The list of trainable variables.\n    \"\"\"\n    mask = list(Trainable.keys())\n    mask.remove(\"examples\")\n\n    for trainable_variable in trainable_variables:\n        seed_candidates = trainable_variable.get(\"seed_candidates\")\n        masked_variable = out_mask_json(\n            trainable_variable.get_json(),\n            mask=mask,\n        )\n        if not seed_candidates:\n            seed_candidates.append(\n                {\n                    **masked_variable,\n                }\n            )\n        trainable_variable.update(\n            {\n                \"candidates\": [],\n                \"best_candidates\": [],\n            }\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.on_train_end","title":"<code>on_train_end(trainable_variables)</code>  <code>async</code>","text":"<p>Called at the end of the training</p> <p>Parameters:</p> Name Type Description Default <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def on_train_end(\n    self,\n    trainable_variables,\n):\n    \"\"\"Called at the end of the training\n\n    Args:\n        trainable_variables (list): The list of trainable variables\n    \"\"\"\n    for variable in trainable_variables:\n        candidates = variable.get(\"candidates\")\n        best_candidates = variable.get(\"best_candidates\")\n        all_candidates = candidates + best_candidates\n        sorted_candidates = sorted(\n            all_candidates,\n            key=lambda x: x.get(\"reward\"),\n            reverse=True,\n        )\n        best_candidate = sorted_candidates[0]\n        best_candidate = out_mask_json(\n            best_candidate,\n            mask=[\"reward\"],\n        )\n        variable.update(\n            {\n                **best_candidate,\n            },\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.optimize","title":"<code>optimize(step, trainable_variables, x=None, y=None, val_x=None, val_y=None)</code>  <code>async</code>","text":"<p>Method for performing optimization.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The training step.</p> required <code>trainable_variables</code> <code>list</code> <p>Variables to be optimized</p> required <code>x</code> <code>ndarray</code> <p>Training batch input data. Must be array-like.</p> <code>None</code> <code>y</code> <code>ndarray</code> <p>Training batch target data. Must be array-like.</p> <code>None</code> <code>val_x</code> <code>ndarray</code> <p>Input validation data. Must be array-like.</p> <code>None</code> <code>val_y</code> <code>ndarray</code> <p>Target validation data. Must be array-like.</p> <code>None</code> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>async def optimize(\n    self,\n    step,\n    trainable_variables,\n    x=None,\n    y=None,\n    val_x=None,\n    val_y=None,\n):\n    \"\"\"Method for performing optimization.\n\n    Args:\n        step (int): The training step.\n        trainable_variables (list): Variables to be optimized\n        x (np.ndarray): Training batch input data. Must be array-like.\n        y (np.ndarray): Training batch target data. Must be array-like.\n        val_x (np.ndarray): Input validation data. Must be array-like.\n        val_y (np.ndarray): Target validation data. Must be array-like.\n    \"\"\"\n    self._check_super_called()\n    if not self.built:\n        await self.build(trainable_variables)\n\n    y_pred = await self.program.predict_on_batch(\n        x=x,\n        training=True,\n    )\n\n    reward = await self.program.compute_reward(\n        x=x,\n        y=y,\n        y_pred=y_pred,\n    )\n\n    await self.assign_reward_to_predictions(\n        trainable_variables,\n        reward=reward,\n    )\n\n    await self.propose_new_candidates(\n        step,\n        trainable_variables,\n        x=x,\n        y=y,\n        y_pred=y_pred,\n        training=True,\n    )\n\n    y_pred = await self.program.predict_on_batch(\n        x=val_x,\n        training=False,\n    )\n\n    reward = await self.program.compute_reward(\n        x=val_x,\n        y=val_y,\n        y_pred=y_pred,\n    )\n\n    if self.trainable_variables:\n        await self.assign_reward_to_predictions(\n            self.trainable_variables,\n            reward=reward,\n        )\n\n    for trainable_variable in trainable_variables:\n        await self.maybe_add_candidate(\n            step,\n            trainable_variable,\n            reward=reward,\n        )\n\n    await self.reward_tracker.update_state(reward)\n    metrics = await self.program.compute_metrics(val_x, val_y, y_pred)\n    return metrics\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.save_own_variables","title":"<code>save_own_variables(store)</code>","text":"<p>Get the state of this optimizer object.</p> Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>def save_own_variables(self, store):\n    \"\"\"Get the state of this optimizer object.\"\"\"\n    for i, variable in enumerate(self.variables):\n        store[str(i)] = variable.numpy()\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/Base%20Optimizer%20class/#synalinks.src.optimizers.optimizer.Optimizer.set_program","title":"<code>set_program(program)</code>","text":"<p>Set the program that this optimizer will optimize.</p> <p>The program contains the model/pipeline that the optimizer will work on.</p> <p>Parameters:</p> Name Type Description Default <code>program</code> <code>Program</code> <p>The Synalinks program to optimize</p> required Source code in <code>synalinks/src/optimizers/optimizer.py</code> <pre><code>def set_program(self, program):\n    \"\"\"Set the program that this optimizer will optimize.\n\n    The program contains the model/pipeline that the optimizer will work on.\n\n    Args:\n        program (Program): The Synalinks program to optimize\n    \"\"\"\n    self._program = program\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/","title":"OMEGA","text":""},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.OMEGA","title":"<code>OMEGA</code>","text":"<p>               Bases: <code>EvolutionaryOptimizer</code></p> <p>OMEGA: OptiMizEr as Genetic Algorithm.</p> <p>A genetic optimizer with dominated novelty search.</p> <p>This optimizer is unique to Synalinks and the result of our research effort on advancing neuro-symbolic AI.</p> <p>Dominated Novelty Search (DNS), is a SOTA Quality-Diversity optimization method that implements a competition function in a classic genetic algorithm.</p> <p>The key insight behind Dominated Novelty Search is that candidates should be eliminated from the population if they are both:</p> <ul> <li>Inferior in reward/fitness</li> <li>Similar to existing candidates/solutions</li> </ul> <p>This algorithm creates an evolutionary pressure to focus on high performing candidates Or candidates that explore other approaches.</p> <p>This approach only add one step to the traditional genetic algorithm and outperform MAP-Elites, Threshold-Elites and Cluster-Elites.</p> <p>This allow the system to explore the search space more quickly by eliminating non-promising candidates while preserving diversity to avoid local optimum.</p> <p>At Synalinks, we adapted this algorithm for LM-based optimization, to do so we use an embedding model to compute the candidate's descriptor and a cosine distance between solutions.</p> <p>Note: In Synalinks, unlike other In-Context learning frameworks, a variable (the module's state to optimize) is a JSON object not a simple string. Which has multiple implications, we maintain a 100% correct structure through constrained JSON decoding, and we allow the state to have variable/dynamic number of fields, which is handled by this approach by embedding each field and averaging them before computing the distance required by DNS.</p> <p>Example: </p><pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    # ... your program definition\n\n    program.compile(\n        reward=synalinks.rewards.ExactMatch(),\n        optimizer=synalinks.optimizers.OMEGA(\n            language_model=language_model,\n            embedding_model=embedding_model,\n        )\n    )\n\n    history = await program.fit(...)\n</code></pre><p></p> Concerning the inspirations for this optimizer <ul> <li>Dominated Novelty Search for their elegant Quality-Diversity   algorithm that outperform many other evolutionary strategies.</li> <li>DSPY's GEPA for feeding the optimizer program with the raw training   data and for formalizing the evolutionary optimization strategy   (NOT the MAP-Elites method used).</li> <li>DeepMind's AlphaEvolve have been a huge inspiration, more on the   motivational side as they didn't released the code.</li> </ul> References <ul> <li>Dominated Novelty Search: Rethinking Local Competition in   Quality-Diversity (https://arxiv.org/html/2502.00593v1)</li> <li>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement   Learning (https://arxiv.org/pdf/2507.19457)</li> <li>AlphaEvolve: A coding agent for scientific and algorithmic   discovery (https://arxiv.org/pdf/2506.13131)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>Additional instructions about the task for the optimizer.</p> <code>None</code> <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use to compute candidates descriptors according to Dominated Novelty Search.</p> <code>None</code> <code>k_nearest_fitter</code> <code>int</code> <p>The K nearest fitter used by Dominated Novelty Search.</p> <code>5</code> <code>distance_function</code> <code>callable</code> <p>Optional. The distance function to use by Dominated Novelty Search. If no function is provided, use the default cosine distance.</p> <code>None</code> <code>mutation_temperature</code> <code>float</code> <p>The temperature for the LM calls of the mutation programs.</p> <code>0.3</code> <code>crossover_temperature</code> <code>float</code> <p>The temperature for the LM calls of the crossover programs.</p> <code>0.3</code> <code>reasoning_effort</code> <code>string</code> <p>Optional. The reasoning effort for the LM call between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None]. Default to None (no reasoning).</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>The mechanism to use for the genetic algorithm between ['ga', 'dns']. This parameter is provided for ablation studies and shouldn't be modified. (Default to 'dns').</p> <code>'dns'</code> <code>selection</code> <code>str</code> <p>The method to select the candidate to evolve at the beginning of a batch between ['random', 'best', 'softmax']. (Default to 'softmax').</p> <code>'softmax'</code> <code>selection_temperature</code> <code>float</code> <p>The temperature for softmax selection. Used only when <code>selection='softmax'</code>. Lower values concentrate selection on high-reward candidates, higher values make selection more uniform (Default 0.3).</p> <code>0.3</code> <code>merging_rate</code> <code>float</code> <p>Rate at which crossover vs mutation is selected. (Default to 0.02).</p> <code>0.02</code> <code>population_size</code> <code>int</code> <p>The maximum number of best candidates to keep during the optimization process.</p> <code>10</code> <code>name</code> <code>str</code> <p>Optional name for the optimizer instance.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional description of the optimizer instance.</p> <code>None</code> Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>@synalinks_export(\"synalinks.optimizers.OMEGA\")\nclass OMEGA(EvolutionaryOptimizer):\n    \"\"\"OMEGA: OptiMizEr as Genetic Algorithm.\n\n    A genetic optimizer with dominated novelty search.\n\n    This optimizer is **unique to Synalinks** and the result of our research\n    effort on advancing neuro-symbolic AI.\n\n    Dominated Novelty Search (DNS), is a SOTA Quality-Diversity optimization\n    method that implements a competition function in a classic genetic\n    algorithm.\n\n    The key insight behind Dominated Novelty Search is that candidates should\n    be eliminated from the population if they are both:\n\n    - Inferior in reward/fitness\n    - Similar to existing candidates/solutions\n\n    This algorithm creates an evolutionary pressure to focus on high performing\n    candidates **Or** candidates that explore other approaches.\n\n    This approach only add one step to the traditional genetic algorithm and\n    *outperform* MAP-Elites, Threshold-Elites and Cluster-Elites.\n\n    This allow the system to explore the search space more quickly by\n    eliminating non-promising candidates while preserving diversity to avoid\n    local optimum.\n\n    At Synalinks, we adapted this algorithm for LM-based optimization, to do\n    so we use an embedding model to compute the candidate's descriptor and a\n    cosine distance between solutions.\n\n    **Note**: In Synalinks, unlike other In-Context learning frameworks, a\n    variable (the module's state to optimize) is a JSON object not a simple\n    string. Which has multiple implications, we maintain a 100% correct\n    structure through constrained JSON decoding, and we allow the state to\n    have variable/dynamic number of fields, which is handled by this approach\n    by embedding each field and averaging them before computing the distance\n    required by DNS.\n\n    Example:\n    ```\n    import synalinks\n    import asyncio\n\n    async def main():\n        # ... your program definition\n\n        program.compile(\n            reward=synalinks.rewards.ExactMatch(),\n            optimizer=synalinks.optimizers.OMEGA(\n                language_model=language_model,\n                embedding_model=embedding_model,\n            )\n        )\n\n        history = await program.fit(...)\n    ```\n\n    Concerning the inspirations for this optimizer:\n        - Dominated Novelty Search for their elegant Quality-Diversity\n          algorithm that outperform many other evolutionary strategies.\n        - DSPY's GEPA for feeding the optimizer program with the raw training\n          data and for formalizing the evolutionary optimization strategy\n          (**NOT** the MAP-Elites method used).\n        - DeepMind's AlphaEvolve have been a huge inspiration, more on the\n          motivational side as they didn't released the code.\n\n    References:\n        - Dominated Novelty Search: Rethinking Local Competition in\n          Quality-Diversity (https://arxiv.org/html/2502.00593v1)\n        - GEPA: Reflective Prompt Evolution Can Outperform Reinforcement\n          Learning (https://arxiv.org/pdf/2507.19457)\n        - AlphaEvolve: A coding agent for scientific and algorithmic\n          discovery (https://arxiv.org/pdf/2506.13131)\n\n    Args:\n        instructions (str): Additional instructions about the task for the\n            optimizer.\n        language_model (LanguageModel): The language model to use.\n        embedding_model (EmbeddingModel): The embedding model to use to\n            compute candidates descriptors according to Dominated Novelty\n            Search.\n        k_nearest_fitter (int): The K nearest fitter used by Dominated\n            Novelty Search.\n        distance_function (callable): Optional. The distance function to use\n            by Dominated Novelty Search. If no function is provided, use\n            the default cosine distance.\n        mutation_temperature (float): The temperature for the LM calls of\n            the mutation programs.\n        crossover_temperature (float): The temperature for the LM calls of\n            the crossover programs.\n        reasoning_effort (string): Optional. The reasoning effort for the LM call\n            between ['minimal', 'low', 'medium', 'high', 'disable', 'none', None].\n            Default to None (no reasoning).\n        algorithm (str): The mechanism to use for the genetic algorithm\n            between ['ga', 'dns']. This parameter is provided for ablation\n            studies and shouldn't be modified. (Default to 'dns').\n        selection (str): The method to select the candidate to evolve at the\n            beginning of a batch between ['random', 'best', 'softmax'].\n            (Default to 'softmax').\n        selection_temperature (float): The temperature for softmax selection.\n            Used only when `selection='softmax'`. Lower values concentrate\n            selection on high-reward candidates, higher values make selection\n            more uniform (Default 0.3).\n        merging_rate (float): Rate at which crossover vs mutation is selected.\n            (Default to 0.02).\n        population_size (int): The maximum number of best candidates to keep\n            during the optimization process.\n        name (str): Optional name for the optimizer instance.\n        description (str): Optional description of the optimizer instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        instructions=None,\n        language_model=None,\n        embedding_model=None,\n        k_nearest_fitter=5,\n        distance_function=None,\n        mutation_temperature=0.3,\n        crossover_temperature=0.3,\n        reasoning_effort=None,\n        merging_rate=0.02,\n        algorithm=\"dns\",\n        selection=\"softmax\",\n        selection_temperature=0.3,\n        population_size=10,\n        name=None,\n        description=None,\n        **kwargs,\n    ):\n        super().__init__(\n            language_model=language_model,\n            mutation_temperature=mutation_temperature,\n            crossover_temperature=crossover_temperature,\n            selection=selection,\n            selection_temperature=selection_temperature,\n            merging_rate=merging_rate,\n            population_size=population_size,\n            name=name,\n            description=description,\n            **kwargs,\n        )\n        if not instructions:\n            instructions = \"\"\n        self.instructions = instructions\n        self.reasoning_effort = reasoning_effort\n\n        # DNS-specific parameters\n        self.embedding_model = embedding_model\n        self.k_nearest_fitter = k_nearest_fitter\n        self.distance_function = distance_function\n\n        algorithms = [\"ga\", \"dns\"]\n        if algorithm not in algorithms:\n            raise ValueError(f\"Parameter `algorithm` should be between {algorithms}\")\n        self.algorithm = algorithm\n\n    async def build(self, trainable_variables):\n        \"\"\"\n        Build the optimizer programs based on the trainable variables.\n\n        Args:\n            trainable_variables (list): List of variables that will be optimized\n        \"\"\"\n        for trainable_variable in trainable_variables:\n            schema_id = id(trainable_variable.get_schema())\n            mask = list(Trainable.keys())\n            symbolic_variable = trainable_variable.to_symbolic_data_model().out_mask(\n                mask=mask\n            )\n\n            if schema_id not in self.mutation_programs:\n                inputs = Input(data_model=MutationInputs)\n                outputs = await ChainOfThought(\n                    data_model=symbolic_variable,\n                    language_model=self.language_model,\n                    temperature=self.mutation_temperature,\n                    reasoning_effort=self.reasoning_effort,\n                    instructions=(\n                        \"\\n\".join(\n                            [\n                                base_instructions(),\n                                mutation_instructions(list(symbolic_variable.keys())),\n                            ]\n                        )\n                        if not self.instructions\n                        else \"\\n\".join(\n                            [\n                                self.instructions,\n                                base_instructions(),\n                                mutation_instructions(list(symbolic_variable.keys())),\n                            ]\n                        )\n                    ),\n                    name=f\"mutation_cot_{schema_id}_\" + self.name,\n                )(inputs)\n                outputs = outputs.in_mask(mask=list(symbolic_variable.keys()))\n                program = Program(\n                    inputs=inputs,\n                    outputs=outputs,\n                    name=f\"mutation_{schema_id}_\" + self.name,\n                    description=\"The mutation program that fix/optimize variables\",\n                )\n                self.mutation_programs[schema_id] = program\n\n            if schema_id not in self.crossover_programs:\n                inputs = Input(data_model=CrossoverInputs)\n                outputs = await ChainOfThought(\n                    data_model=symbolic_variable,\n                    language_model=self.language_model,\n                    temperature=self.crossover_temperature,\n                    reasoning_effort=self.reasoning_effort,\n                    instructions=(\n                        \"\\n\".join(\n                            [\n                                base_instructions(),\n                                crossover_instructions(list(symbolic_variable.keys())),\n                            ]\n                        )\n                        if not self.instructions\n                        else \"\\n\".join(\n                            [\n                                self.instructions,\n                                base_instructions(),\n                                crossover_instructions(list(symbolic_variable.keys())),\n                            ]\n                        )\n                    ),\n                    name=f\"crossover_cot_{schema_id}_\" + self.name,\n                )(inputs)\n                outputs = outputs.in_mask(mask=list(symbolic_variable.keys()))\n                program = Program(\n                    inputs=inputs,\n                    outputs=outputs,\n                    name=f\"crossover_{schema_id}_\" + self.name,\n                    description=\"Crossover program combining high performing variables\",\n                )\n                self.crossover_programs[schema_id] = program\n\n        self.built = True\n\n    async def mutate_candidate(\n        self,\n        step: int,\n        trainable_variable: \"Variable\",\n        selected_candidate: Dict[str, Any],\n        x: Optional[List[Any]] = None,\n        y: Optional[List[Any]] = None,\n        y_pred: Optional[List[Any]] = None,\n        training: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Apply mutation to generate a new candidate using LLM.\n\n        Creates mutation inputs from the selected candidate and training data,\n        then calls the mutation program to generate an optimized variant.\n\n        Args:\n            step (int): The current training step\n            trainable_variable (Variable): The trainable variable (for metadata access)\n            selected_candidate (dict): The selected candidate to mutate\n            x (list): Input data batch\n            y (list): Ground truth data batch\n            y_pred (list): Predicted outputs from the current model\n            training (bool): Whether in training mode\n\n        Returns:\n            dict: The mutated candidate from the mutation program\n        \"\"\"\n        mask = list(Trainable.keys())\n        schema_id = id(trainable_variable.get_schema())\n        masked_variable = out_mask_json(\n            selected_candidate,\n            mask=mask,\n        )\n        inputs = MutationInputs(\n            program_description=self.program.description,\n            program_inputs=[inp.get_json() for inp in x],\n            program_predicted_outputs=[\n                pred.get_json() if pred else None for pred in y_pred\n            ],\n            program_ground_truth=([gt.get_json() for gt in y] if y is not None else []),\n            variable_description=trainable_variable.description,\n            current_variable=masked_variable,\n        )\n        program = self.mutation_programs[schema_id]\n        return await program(inputs, training=training)\n\n    async def merge_candidate(\n        self,\n        step: int,\n        trainable_variable: \"Variable\",\n        current_candidate: Dict[str, Any],\n        other_candidate: Dict[str, Any],\n        x: Optional[List[Any]] = None,\n        y: Optional[List[Any]] = None,\n        y_pred: Optional[List[Any]] = None,\n        training: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Apply crossover to merge two selected candidates.\n\n        Creates crossover inputs combining two high-performing candidates,\n        then calls the crossover program to generate a merged variant.\n\n        Args:\n            step (int): The current training step\n            trainable_variable (Variable): The trainable variable (for metadata access)\n            current_candidate (dict): First selected candidate to merge\n            other_candidate (dict): Second selected candidate to merge\n            x (list): Input data batch\n            y (list): Ground truth data batch\n            y_pred (list): Predicted outputs from the current model\n            training (bool): Whether in training mode\n\n        Returns:\n            dict: The merged candidate from the crossover program\n        \"\"\"\n        mask = list(Trainable.keys())\n        schema_id = id(trainable_variable.get_schema())\n        current_variable = out_mask_json(\n            current_candidate,\n            mask=mask,\n        )\n        other_variable = out_mask_json(\n            other_candidate,\n            mask=mask,\n        )\n        inputs = CrossoverInputs(\n            program_description=self.program.description,\n            program_inputs=[inp.get_json() for inp in x],\n            program_predicted_outputs=[\n                pred.get_json() if pred else None for pred in y_pred\n            ],\n            program_ground_truth=([gt.get_json() for gt in y] if y is not None else []),\n            variable_description=trainable_variable.description,\n            other_variable=other_variable,\n            current_variable=current_variable,\n        )\n        program = self.crossover_programs[schema_id]\n        return await program(inputs, training=training)\n\n    async def competition(self, candidates: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Apply Dominated Novelty Search (DNS) competition.\n\n        DNS filters candidates by removing those that are both:\n        - Inferior in reward (dominated)\n        - Similar to existing candidates (not novel)\n\n        This maintains diversity while focusing on high-performing candidates.\n\n        Args:\n            candidates (list): List of candidate dictionaries with 'reward' key\n\n        Returns:\n            list: Filtered list of candidates that passed the DNS competition\n        \"\"\"\n        if len(candidates) &lt;= 1:\n            return candidates\n\n        distance_function = (\n            self.distance_function if self.distance_function else similarity_distance\n        )\n\n        selected_candidates = []\n        for candidate in candidates:\n            is_dominated = False\n            for other in candidates:\n                if other is candidate:\n                    continue\n                distance = await distance_function(\n                    candidate,\n                    other,\n                    embedding_model=self.embedding_model,\n                )\n                # Check if within k-nearest neighborhood\n                if distance &lt; 1.0 / self.k_nearest_fitter:\n                    # Check if dominated (lower reward)\n                    if candidate.get(\"reward\", 0) &lt; other.get(\"reward\", 0):\n                        is_dominated = True\n                        break\n            if not is_dominated:\n                selected_candidates.append(candidate)\n\n        return selected_candidates if selected_candidates else [candidates[0]]\n\n    async def on_epoch_end(self, epoch, trainable_variables):\n        \"\"\"Called at the end of each epoch.\n\n        Applies DNS competition (if algorithm='dns') to filter candidates,\n        then selects the top candidates based on population_size.\n\n        Args:\n            epoch (int): The epoch number\n            trainable_variables (list): The list of trainable variables\n        \"\"\"\n        for trainable_variable in trainable_variables:\n            candidates = trainable_variable.get(\"candidates\")\n            best_candidates = trainable_variable.get(\"best_candidates\")\n\n            # Combine current candidates with best candidates\n            all_candidates = candidates + best_candidates\n\n            # Apply DNS competition if enabled\n            if self.algorithm == \"dns\" and len(all_candidates) &gt; 1:\n                all_candidates = await self.competition(all_candidates)\n\n            # Sort by reward and keep top population_size candidates\n            all_candidates = sorted(\n                all_candidates,\n                key=lambda x: x.get(\"reward\", 0),\n                reverse=True,\n            )\n            trainable_variable.update(\n                {\n                    \"candidates\": [],\n                    \"best_candidates\": all_candidates[: self.population_size],\n                }\n            )\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"instructions\": self.instructions,\n                \"reasoning_effort\": self.reasoning_effort,\n                \"k_nearest_fitter\": self.k_nearest_fitter,\n                \"algorithm\": self.algorithm,\n            }\n        )\n        if self.embedding_model:\n            config[\"embedding_model\"] = serialization_lib.serialize_synalinks_object(\n                self.embedding_model\n            )\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        embedding_model = None\n        if \"embedding_model\" in config:\n            embedding_model = serialization_lib.deserialize_synalinks_object(\n                config.pop(\"embedding_model\")\n            )\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(\n            language_model=language_model,\n            embedding_model=embedding_model,\n            **config,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.OMEGA.build","title":"<code>build(trainable_variables)</code>  <code>async</code>","text":"<p>Build the optimizer programs based on the trainable variables.</p> <p>Parameters:</p> Name Type Description Default <code>trainable_variables</code> <code>list</code> <p>List of variables that will be optimized</p> required Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>async def build(self, trainable_variables):\n    \"\"\"\n    Build the optimizer programs based on the trainable variables.\n\n    Args:\n        trainable_variables (list): List of variables that will be optimized\n    \"\"\"\n    for trainable_variable in trainable_variables:\n        schema_id = id(trainable_variable.get_schema())\n        mask = list(Trainable.keys())\n        symbolic_variable = trainable_variable.to_symbolic_data_model().out_mask(\n            mask=mask\n        )\n\n        if schema_id not in self.mutation_programs:\n            inputs = Input(data_model=MutationInputs)\n            outputs = await ChainOfThought(\n                data_model=symbolic_variable,\n                language_model=self.language_model,\n                temperature=self.mutation_temperature,\n                reasoning_effort=self.reasoning_effort,\n                instructions=(\n                    \"\\n\".join(\n                        [\n                            base_instructions(),\n                            mutation_instructions(list(symbolic_variable.keys())),\n                        ]\n                    )\n                    if not self.instructions\n                    else \"\\n\".join(\n                        [\n                            self.instructions,\n                            base_instructions(),\n                            mutation_instructions(list(symbolic_variable.keys())),\n                        ]\n                    )\n                ),\n                name=f\"mutation_cot_{schema_id}_\" + self.name,\n            )(inputs)\n            outputs = outputs.in_mask(mask=list(symbolic_variable.keys()))\n            program = Program(\n                inputs=inputs,\n                outputs=outputs,\n                name=f\"mutation_{schema_id}_\" + self.name,\n                description=\"The mutation program that fix/optimize variables\",\n            )\n            self.mutation_programs[schema_id] = program\n\n        if schema_id not in self.crossover_programs:\n            inputs = Input(data_model=CrossoverInputs)\n            outputs = await ChainOfThought(\n                data_model=symbolic_variable,\n                language_model=self.language_model,\n                temperature=self.crossover_temperature,\n                reasoning_effort=self.reasoning_effort,\n                instructions=(\n                    \"\\n\".join(\n                        [\n                            base_instructions(),\n                            crossover_instructions(list(symbolic_variable.keys())),\n                        ]\n                    )\n                    if not self.instructions\n                    else \"\\n\".join(\n                        [\n                            self.instructions,\n                            base_instructions(),\n                            crossover_instructions(list(symbolic_variable.keys())),\n                        ]\n                    )\n                ),\n                name=f\"crossover_cot_{schema_id}_\" + self.name,\n            )(inputs)\n            outputs = outputs.in_mask(mask=list(symbolic_variable.keys()))\n            program = Program(\n                inputs=inputs,\n                outputs=outputs,\n                name=f\"crossover_{schema_id}_\" + self.name,\n                description=\"Crossover program combining high performing variables\",\n            )\n            self.crossover_programs[schema_id] = program\n\n    self.built = True\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.OMEGA.competition","title":"<code>competition(candidates)</code>  <code>async</code>","text":"<p>Apply Dominated Novelty Search (DNS) competition.</p> <p>DNS filters candidates by removing those that are both: - Inferior in reward (dominated) - Similar to existing candidates (not novel)</p> <p>This maintains diversity while focusing on high-performing candidates.</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>list</code> <p>List of candidate dictionaries with 'reward' key</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Any]]</code> <p>Filtered list of candidates that passed the DNS competition</p> Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>async def competition(self, candidates: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n    \"\"\"Apply Dominated Novelty Search (DNS) competition.\n\n    DNS filters candidates by removing those that are both:\n    - Inferior in reward (dominated)\n    - Similar to existing candidates (not novel)\n\n    This maintains diversity while focusing on high-performing candidates.\n\n    Args:\n        candidates (list): List of candidate dictionaries with 'reward' key\n\n    Returns:\n        list: Filtered list of candidates that passed the DNS competition\n    \"\"\"\n    if len(candidates) &lt;= 1:\n        return candidates\n\n    distance_function = (\n        self.distance_function if self.distance_function else similarity_distance\n    )\n\n    selected_candidates = []\n    for candidate in candidates:\n        is_dominated = False\n        for other in candidates:\n            if other is candidate:\n                continue\n            distance = await distance_function(\n                candidate,\n                other,\n                embedding_model=self.embedding_model,\n            )\n            # Check if within k-nearest neighborhood\n            if distance &lt; 1.0 / self.k_nearest_fitter:\n                # Check if dominated (lower reward)\n                if candidate.get(\"reward\", 0) &lt; other.get(\"reward\", 0):\n                    is_dominated = True\n                    break\n        if not is_dominated:\n            selected_candidates.append(candidate)\n\n    return selected_candidates if selected_candidates else [candidates[0]]\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.OMEGA.merge_candidate","title":"<code>merge_candidate(step, trainable_variable, current_candidate, other_candidate, x=None, y=None, y_pred=None, training=False)</code>  <code>async</code>","text":"<p>Apply crossover to merge two selected candidates.</p> <p>Creates crossover inputs combining two high-performing candidates, then calls the crossover program to generate a merged variant.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The current training step</p> required <code>trainable_variable</code> <code>Variable</code> <p>The trainable variable (for metadata access)</p> required <code>current_candidate</code> <code>dict</code> <p>First selected candidate to merge</p> required <code>other_candidate</code> <code>dict</code> <p>Second selected candidate to merge</p> required <code>x</code> <code>list</code> <p>Input data batch</p> <code>None</code> <code>y</code> <code>list</code> <p>Ground truth data batch</p> <code>None</code> <code>y_pred</code> <code>list</code> <p>Predicted outputs from the current model</p> <code>None</code> <code>training</code> <code>bool</code> <p>Whether in training mode</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The merged candidate from the crossover program</p> Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>async def merge_candidate(\n    self,\n    step: int,\n    trainable_variable: \"Variable\",\n    current_candidate: Dict[str, Any],\n    other_candidate: Dict[str, Any],\n    x: Optional[List[Any]] = None,\n    y: Optional[List[Any]] = None,\n    y_pred: Optional[List[Any]] = None,\n    training: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Apply crossover to merge two selected candidates.\n\n    Creates crossover inputs combining two high-performing candidates,\n    then calls the crossover program to generate a merged variant.\n\n    Args:\n        step (int): The current training step\n        trainable_variable (Variable): The trainable variable (for metadata access)\n        current_candidate (dict): First selected candidate to merge\n        other_candidate (dict): Second selected candidate to merge\n        x (list): Input data batch\n        y (list): Ground truth data batch\n        y_pred (list): Predicted outputs from the current model\n        training (bool): Whether in training mode\n\n    Returns:\n        dict: The merged candidate from the crossover program\n    \"\"\"\n    mask = list(Trainable.keys())\n    schema_id = id(trainable_variable.get_schema())\n    current_variable = out_mask_json(\n        current_candidate,\n        mask=mask,\n    )\n    other_variable = out_mask_json(\n        other_candidate,\n        mask=mask,\n    )\n    inputs = CrossoverInputs(\n        program_description=self.program.description,\n        program_inputs=[inp.get_json() for inp in x],\n        program_predicted_outputs=[\n            pred.get_json() if pred else None for pred in y_pred\n        ],\n        program_ground_truth=([gt.get_json() for gt in y] if y is not None else []),\n        variable_description=trainable_variable.description,\n        other_variable=other_variable,\n        current_variable=current_variable,\n    )\n    program = self.crossover_programs[schema_id]\n    return await program(inputs, training=training)\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.OMEGA.mutate_candidate","title":"<code>mutate_candidate(step, trainable_variable, selected_candidate, x=None, y=None, y_pred=None, training=False)</code>  <code>async</code>","text":"<p>Apply mutation to generate a new candidate using LLM.</p> <p>Creates mutation inputs from the selected candidate and training data, then calls the mutation program to generate an optimized variant.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The current training step</p> required <code>trainable_variable</code> <code>Variable</code> <p>The trainable variable (for metadata access)</p> required <code>selected_candidate</code> <code>dict</code> <p>The selected candidate to mutate</p> required <code>x</code> <code>list</code> <p>Input data batch</p> <code>None</code> <code>y</code> <code>list</code> <p>Ground truth data batch</p> <code>None</code> <code>y_pred</code> <code>list</code> <p>Predicted outputs from the current model</p> <code>None</code> <code>training</code> <code>bool</code> <p>Whether in training mode</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The mutated candidate from the mutation program</p> Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>async def mutate_candidate(\n    self,\n    step: int,\n    trainable_variable: \"Variable\",\n    selected_candidate: Dict[str, Any],\n    x: Optional[List[Any]] = None,\n    y: Optional[List[Any]] = None,\n    y_pred: Optional[List[Any]] = None,\n    training: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Apply mutation to generate a new candidate using LLM.\n\n    Creates mutation inputs from the selected candidate and training data,\n    then calls the mutation program to generate an optimized variant.\n\n    Args:\n        step (int): The current training step\n        trainable_variable (Variable): The trainable variable (for metadata access)\n        selected_candidate (dict): The selected candidate to mutate\n        x (list): Input data batch\n        y (list): Ground truth data batch\n        y_pred (list): Predicted outputs from the current model\n        training (bool): Whether in training mode\n\n    Returns:\n        dict: The mutated candidate from the mutation program\n    \"\"\"\n    mask = list(Trainable.keys())\n    schema_id = id(trainable_variable.get_schema())\n    masked_variable = out_mask_json(\n        selected_candidate,\n        mask=mask,\n    )\n    inputs = MutationInputs(\n        program_description=self.program.description,\n        program_inputs=[inp.get_json() for inp in x],\n        program_predicted_outputs=[\n            pred.get_json() if pred else None for pred in y_pred\n        ],\n        program_ground_truth=([gt.get_json() for gt in y] if y is not None else []),\n        variable_description=trainable_variable.description,\n        current_variable=masked_variable,\n    )\n    program = self.mutation_programs[schema_id]\n    return await program(inputs, training=training)\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.OMEGA.on_epoch_end","title":"<code>on_epoch_end(epoch, trainable_variables)</code>  <code>async</code>","text":"<p>Called at the end of each epoch.</p> <p>Applies DNS competition (if algorithm='dns') to filter candidates, then selects the top candidates based on population_size.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number</p> required <code>trainable_variables</code> <code>list</code> <p>The list of trainable variables</p> required Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>async def on_epoch_end(self, epoch, trainable_variables):\n    \"\"\"Called at the end of each epoch.\n\n    Applies DNS competition (if algorithm='dns') to filter candidates,\n    then selects the top candidates based on population_size.\n\n    Args:\n        epoch (int): The epoch number\n        trainable_variables (list): The list of trainable variables\n    \"\"\"\n    for trainable_variable in trainable_variables:\n        candidates = trainable_variable.get(\"candidates\")\n        best_candidates = trainable_variable.get(\"best_candidates\")\n\n        # Combine current candidates with best candidates\n        all_candidates = candidates + best_candidates\n\n        # Apply DNS competition if enabled\n        if self.algorithm == \"dns\" and len(all_candidates) &gt; 1:\n            all_candidates = await self.competition(all_candidates)\n\n        # Sort by reward and keep top population_size candidates\n        all_candidates = sorted(\n            all_candidates,\n            key=lambda x: x.get(\"reward\", 0),\n            reverse=True,\n        )\n        trainable_variable.update(\n            {\n                \"candidates\": [],\n                \"best_candidates\": all_candidates[: self.population_size],\n            }\n        )\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.base_instructions","title":"<code>base_instructions()</code>","text":"<p>Base instructions that define the context for all optimization programs.</p> <p>These instructions explain that the system optimizes JSON variables in a computation graph.</p> Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>def base_instructions():\n    \"\"\"Base instructions that define the context for all optimization programs.\n\n    These instructions explain that the system optimizes JSON variables\n    in a computation graph.\n    \"\"\"\n    return \"\"\"\nYou are an integral part of an optimization system designed to improve\nJSON variables within a computation graph (i.e. the program).\nEach module in the graph performs specific computations, with JSON variables\nserving as the state.\nThese variables can represent prompts, code, plans, rules, or any other\nJSON-compatible data.\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.crossover_instructions","title":"<code>crossover_instructions(variables_keys)</code>","text":"<p>Instructions for the crossover program that optimizes variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables_keys</code> <code>list</code> <p>List of keys that the variable should contain</p> required Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>def crossover_instructions(variables_keys):\n    \"\"\"Instructions for the crossover program that optimizes variables.\n\n    Args:\n        variables_keys (list): List of keys that the variable should contain\n    \"\"\"\n    return f\"\"\"\nYour responsibility is to create a new, optimized variable by strategically\ncombining features from the current variable and a high-performing candidate.\nThe new variable should improve the alignment of the predicted output with\nthe ground truth.\n\nGuidelines:\n- Analyze both the current variable and the other high-performing variable,\n  identifying their respective strengths and weaknesses.\n- Pay close attention to the variable's description, its intended use, and the\n  broader context of the computation graph.\n- Ensure the new variable is generalizable and performs well across various\n  inputs of the same kind.\n- Include all specified keys: {variables_keys}.\n- Justify each feature you incorporate, explaining how it contributes to\n  better performance or alignment with the ground truth.\n- If no ground truth is provided, the goal is to critically enhance the\n  predicted output.\n- If you have to optimize a variable containing code, provide a generalizable\n  algorithm.\n- Always focus on ONLY one aspect at the time.\n- If the instructions/prompt contains general information, keep it.\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.mutation_instructions","title":"<code>mutation_instructions(variables_keys)</code>","text":"<p>Instructions for the mutation program that optimizes variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables_keys</code> <code>list</code> <p>List of keys that the variable should contain</p> required Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>def mutation_instructions(variables_keys):\n    \"\"\"Instructions for the mutation program that optimizes variables.\n\n    Args:\n        variables_keys (list): List of keys that the variable should contain\n    \"\"\"\n    return f\"\"\"\nYour primary task is to creatively enhance the provided variable so that the\npredicted output aligns as closely as possible with the ground truth.\nPay close attention to the variable's description, its intended use, and the\nbroader context of the computation graph.\n\nGuidelines:\n- Ensure the new variable is generalizable and performs well across various\n  inputs of the same kind.\n- Include all specified keys: {variables_keys}.\n- Justify each change with clear reasoning, referencing the variable's purpose\n  and the desired output.\n- If no ground truth is provided, the goal is to critically enhance the\n  predicted output.\n- If you have to optimize a variable containing code, provide a generalizable\n  algorithm.\n- Always focus on ONLY one aspect at the time.\n- If the instructions/prompt contains general information, keep it.\n\"\"\".strip()\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/OMEGA/#synalinks.src.optimizers.omega.similarity_distance","title":"<code>similarity_distance(candidate1, candidate2, embedding_model=None, axis=-1)</code>  <code>async</code>","text":"<p>Compute distance between two candidates using embeddings.</p> <p>This function computes the cosine distance between the mean embeddings of two candidate JSON objects. Each field of the JSON is embedded separately, normalized to unit length, then averaged.</p> <p>Parameters:</p> Name Type Description Default <code>candidate1</code> <code>dict</code> <p>First candidate (dict or JSON-serializable object)</p> required <code>candidate2</code> <code>dict</code> <p>Second candidate (dict or JSON-serializable object)</p> required <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model for computing embeddings</p> <code>None</code> <code>axis</code> <code>int</code> <p>The axis along which to compute the similarity (default: -1)</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Cosine distance between candidates (0 = identical, 1 = orthogonal)</p> Source code in <code>synalinks/src/optimizers/omega.py</code> <pre><code>async def similarity_distance(\n    candidate1: Dict[str, Any],\n    candidate2: Dict[str, Any],\n    embedding_model: Optional[\"EmbeddingModel\"] = None,\n    axis: int = -1,\n) -&gt; float:\n    \"\"\"Compute distance between two candidates using embeddings.\n\n    This function computes the cosine distance between the mean embeddings\n    of two candidate JSON objects. Each field of the JSON is embedded\n    separately, normalized to unit length, then averaged.\n\n    Args:\n        candidate1 (dict): First candidate (dict or JSON-serializable object)\n        candidate2 (dict): Second candidate (dict or JSON-serializable object)\n        embedding_model (EmbeddingModel): The embedding model for computing embeddings\n        axis (int): The axis along which to compute the similarity (default: -1)\n\n    Returns:\n        float: Cosine distance between candidates (0 = identical, 1 = orthogonal)\n    \"\"\"\n    embeddings1 = await embedding_model(tree.flatten(candidate1))\n    embeddings2 = await embedding_model(tree.flatten(candidate2))\n    embeddings1 = embeddings1[\"embeddings\"]\n    embeddings2 = embeddings2[\"embeddings\"]\n    embeddings1 = np.convert_to_tensor(embeddings1)\n    embeddings2 = np.convert_to_tensor(embeddings2)\n    embeddings1, embeddings2 = squeeze_or_expand_to_same_rank(embeddings1, embeddings2)\n    embeddings1 = np.normalize(embeddings1, axis=axis)\n    embeddings2 = np.normalize(embeddings2, axis=axis)\n    embeddings1 = np.mean(embeddings1, axis=0)\n    embeddings2 = np.mean(embeddings2, axis=0)\n    similarity = (np.sum(embeddings1 * embeddings2, axis=axis) + 1) / 2\n    return 1 - similarity\n</code></pre>"},{"location":"Synalinks%20API/Optimizers%20API/RandomFewShot/","title":"RandomFewShot","text":""},{"location":"Synalinks%20API/Optimizers%20API/RandomFewShot/#synalinks.src.optimizers.random_few_shot.RandomFewShot","title":"<code>RandomFewShot</code>","text":"<p>               Bases: <code>GreedyOptimizer</code></p> <p>Sample among the best predictions to populate the LM's prompt.</p> <p>Makes the model learn using Few Shot Learning by selecting predictions as examples based on their rewards using the configured sampling strategy.</p> <p>Example:</p> <pre><code>import synalinks\nimport asyncio\n\nasync def main():\n    # ... your program definition\n\n    program.compile(\n        reward=synalinks.rewards.ExactMatch(),\n        optimizer=synalinks.optimizers.RandomFewShot(\n            nb_min_examples=1,\n            nb_max_examples=3,\n            sampling=\"softmax\",\n            sampling_temperature=1.0,\n        ),\n    )\n\n    history = await program.fit(...)\n</code></pre> References <ul> <li>DSPy: Compiling Declarative Language Model Calls into   Self-Improving Pipelines (https://arxiv.org/pdf/2310.03714)</li> <li>Language Models are Few-Shot Learners   (https://arxiv.org/abs/2005.14165)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nb_min_examples</code> <code>int</code> <p>The min number of examples for few-shot learning (Default to 1).</p> <code>1</code> <code>nb_max_examples</code> <code>int</code> <p>The max number of examples for few-shot learning (Default to 3).</p> <code>3</code> <code>sampling</code> <code>str</code> <p>The method to sample predictions between ['random', 'best', 'softmax']. (Default to 'softmax').</p> <code>'softmax'</code> <code>sampling_temperature</code> <code>float</code> <p>The temperature for softmax sampling. Used only when <code>sampling='softmax'</code>. Lower values concentrate sampling on high-reward predictions, higher values make sampling more uniform (Default 0.3).</p> <code>0.3</code> <code>population_size</code> <code>int</code> <p>The maximum number of best candidates to keep during the optimization process.</p> <code>10</code> <code>name</code> <code>str</code> <p>Optional name for the optimizer instance.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional description of the optimizer instance.</p> <code>None</code> Source code in <code>synalinks/src/optimizers/random_few_shot.py</code> <pre><code>@synalinks_export(\"synalinks.optimizers.RandomFewShot\")\nclass RandomFewShot(GreedyOptimizer):\n    \"\"\"Sample among the best predictions to populate the LM's prompt.\n\n    Makes the model learn using Few Shot Learning by selecting predictions\n    as examples based on their rewards using the configured sampling strategy.\n\n    Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    async def main():\n        # ... your program definition\n\n        program.compile(\n            reward=synalinks.rewards.ExactMatch(),\n            optimizer=synalinks.optimizers.RandomFewShot(\n                nb_min_examples=1,\n                nb_max_examples=3,\n                sampling=\"softmax\",\n                sampling_temperature=1.0,\n            ),\n        )\n\n        history = await program.fit(...)\n    ```\n\n    References:\n        - DSPy: Compiling Declarative Language Model Calls into\n          Self-Improving Pipelines (https://arxiv.org/pdf/2310.03714)\n        - Language Models are Few-Shot Learners\n          (https://arxiv.org/abs/2005.14165)\n\n    Args:\n        nb_min_examples (int): The min number of examples for few-shot\n            learning (Default to 1).\n        nb_max_examples (int): The max number of examples for few-shot\n            learning (Default to 3).\n        sampling (str): The method to sample predictions between\n            ['random', 'best', 'softmax']. (Default to 'softmax').\n        sampling_temperature (float): The temperature for softmax sampling.\n            Used only when `sampling='softmax'`. Lower values concentrate\n            sampling on high-reward predictions, higher values make sampling\n            more uniform (Default 0.3).\n        population_size (int): The maximum number of best candidates to keep\n            during the optimization process.\n        name (str): Optional name for the optimizer instance.\n        description (str): Optional description of the optimizer instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        nb_min_examples=1,\n        nb_max_examples=3,\n        sampling=\"softmax\",\n        sampling_temperature=0.3,\n        population_size=10,\n        name=None,\n        description=None,\n    ):\n        super().__init__(\n            nb_min_examples=nb_min_examples,\n            nb_max_examples=nb_max_examples,\n            sampling=sampling,\n            sampling_temperature=sampling_temperature,\n            population_size=population_size,\n            name=name,\n            description=description,\n        )\n\n    async def build(self, _):\n        self.built = True\n\n    async def propose_new_candidates(\n        self,\n        step,\n        trainable_variables,\n        x=None,\n        y=None,\n        y_pred=None,\n        training=False,\n    ):\n        variable_name_to_update = await self.select_variable_name_to_update(\n            trainable_variables,\n        )\n\n        for trainable_variable in trainable_variables:\n            if trainable_variable.name == variable_name_to_update:\n                examples = await self.sample_best_predictions(\n                    trainable_variable,\n                )\n                await self.assign_candidate(\n                    trainable_variable,\n                    examples=examples,\n                )\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/","title":"Programs API","text":""},{"location":"Synalinks%20API/Programs%20API/#programs-api","title":"Programs API","text":"<p>Synalinks offers 4 methods to create programs, each tailored to different levels of complexity and use cases:</p> <ul> <li> <p>Sequential Program: This is the simplest method, involving a straightforward list of modules. Ideal for single-input, single-output stacks of modules. However, it is limited in flexibility compared to other methods.</p> </li> <li> <p>Functional Program: This is a fully-featured API that supports arbitrary program architectures. Easy to use and suitable for most users, offering greater flexibility than the Sequential program.</p> </li> <li> <p>Mixing Subclassing and Functional API: This make easier to encapsulate your programs made with the functional API into bigger systems.</p> </li> <li> <p>Program Subclassing: This method allows you to implement everything from scratch. Ideal for complex or research use cases. It is also the preferred method for contributing.</p> </li> </ul>"},{"location":"Synalinks%20API/Programs%20API/#programs-api-overview","title":"Programs API Overview","text":""},{"location":"Synalinks%20API/Programs%20API/#the-program-class","title":"The Program class","text":"<ul> <li>Program class</li> <li>summary method</li> <li>get_module method</li> </ul>"},{"location":"Synalinks%20API/Programs%20API/#the-sequential-class","title":"The Sequential class","text":"<ul> <li>Sequential class</li> <li>add method</li> <li>pop method</li> </ul>"},{"location":"Synalinks%20API/Programs%20API/#program-training-apis","title":"Program training APIs","text":"<ul> <li>compile method</li> <li>fit method</li> <li>evaluate method</li> <li>predict method</li> <li>train_on_batch method</li> <li>test_on_batch method</li> <li>predict_on_batch method</li> </ul>"},{"location":"Synalinks%20API/Programs%20API/#saving-serialization","title":"Saving &amp; Serialization","text":"<ul> <li>Whole program saving and loading</li> <li>Variables-only saving and loading</li> </ul>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/","title":"Program training API","text":"Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>class Trainer:\n    def __init__(self):\n        self._lock = False\n        self._run_eagerly = False\n        self.compiled = False\n        self.reward = None\n        self.steps_per_execution = 1\n        # Can be set by callbacks in on_train_begin\n        self._initial_epoch = None\n        self._compute_reward_has_training_arg = (\n            \"training\" in inspect.signature(self.compute_reward).parameters\n        )\n        # Placeholders used in `compile`\n        self._optimizer = None\n        self._compile_reward = None\n        self._compile_metrics = None\n        self._reward_tracker = None\n\n    @tracking.no_automatic_dependency_tracking\n    def compile(\n        self,\n        optimizer=None,\n        reward=None,\n        reward_weights=None,\n        metrics=None,\n        run_eagerly=False,\n        steps_per_execution=1,\n    ):\n        \"\"\"Configures the program for training.\n\n        Example:\n\n        ```python\n        program.compile(\n            optimizer=synalinks.optimizers.RandomFewShot(),\n            reward=synalinks.rewards.ExactMatch(),\n            metrics=[\n                synalinks.metrics.MeanMetricWrapper(synalinks.rewards.exact_match),\n            ],\n        )\n        ```\n\n        Args:\n            optimizer (Optimizer): Optimizer instance. See `synalinks.optimizers`.\n            reward (Reward): Reward function. A `synalinks.rewards.Reward`\n                instance. See `synalinks.rewards`. A reward function is\n                any callable with the signature `reward = fn(y_true, y_pred)`,\n                where `y_true` are the ground truth values, and `y_pred`\n                are the program's predictions.\n                `y_true` should be a list of batch size length `[d0, .. dN]`.\n                `y_pred` should be a list of batch size length `[d0, .. dN]`.\n                The reward function should return a float.\n            reward_weights (list): Optional list specifying scalar coefficients\n                (Python floats) to weight the reward contributions of\n                different program outputs. The reward value that will be maximized\n                by the program will then be the *weighted sum* of all individual\n                rewards, weighted by the `reward_weights` coefficients. It is\n                expected to have a 1:1 mapping to the program's outputs.\n            metrics (list): List of metrics to be evaluated by the program during\n                training and testing. Each of it is a `synalinks.metrics.Metric`\n                instance. See `synalinks.metrics`. A function is any callable with the\n                signature `result = fn(y_true, y_pred)`.\n            run_eagerly (bool): If `True`, this program's forward pass\n                will never be compiled. It is recommended to leave this\n                as `False` when training (for best performance),\n                and to set it to `True` when debugging.\n            steps_per_execution (int): The number of batches to run\n                during each a single compiled function call. Running multiple\n                batches inside a single compiled function call can\n                greatly improve performance on TPUs or small programs with a large\n                Python overhead. At most, one full epoch will be run each\n                execution. If a number larger than the size of the epoch is\n                passed, the execution will be truncated to the size of the\n                epoch. Note that if `steps_per_execution` is set to `N`,\n                `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n                will only be called every `N` batches (i.e. before/after\n                each compiled function execution).\n        \"\"\"\n        self._clear_previous_trainer_metrics()\n        self._optimizer = optimizer\n        self._optimizer.set_program(self)\n\n        if hasattr(self, \"output_names\"):\n            output_names = self.output_names\n        else:\n            output_names = None\n        if reward is not None:\n            self._compile_reward = CompileReward(\n                reward, reward_weights, output_names=output_names\n            )\n            self.reward = reward\n        if metrics is not None:\n            self._compile_metrics = CompileMetrics(metrics, output_names=output_names)\n        self.run_eagerly = run_eagerly\n        self.stop_training = False\n        self.compiled = True\n        self._reward_tracker = metrics_module.Mean(name=\"reward\")\n        self.steps_per_execution = steps_per_execution\n\n        self._compile_config = serialization_lib.SerializableDict(\n            optimizer=optimizer,\n            reward=reward,\n            reward_weights=reward_weights,\n            metrics=metrics,\n            run_eagerly=run_eagerly,\n            steps_per_execution=steps_per_execution,\n        )\n\n    @property\n    def optimizer(self):\n        return self._optimizer\n\n    @property\n    def metrics(self):\n        # Order: reward tracker, individual reward trackers, compiled metrics,\n        # custom metrcis, submodule metrics.\n        metrics = []\n        if self.compiled:\n            if self._reward_tracker is not None:\n                metrics.append(self._reward_tracker)\n            if self._compile_metrics is not None:\n                metrics.append(self._compile_metrics)\n            if self._compile_reward is not None:\n                metrics.extend(self._compile_reward.metrics)\n        metrics.extend(self._metrics)\n        for module in self._flatten_modules(include_self=False):\n            if isinstance(module, Trainer):\n                # All Trainer-related metrics in submodules should be ignored\n                # because a new Trainer has been instantiated.\n                continue\n            metrics.extend(module.metrics)\n        return metrics\n\n    @property\n    def metrics_names(self):\n        return [m.name for m in self.metrics]\n\n    def reset_metrics(self):\n        for m in self.metrics:\n            m.reset_state()\n\n    def _get_own_metrics(self):\n        metrics = []\n        if self._reward_tracker is not None:\n            metrics.append(self._reward_tracker)\n        if self._compile_metrics is not None:\n            metrics.append(self._compile_metrics)\n        if self._compile_reward is not None:\n            metrics.extend(self._compile_reward.metrics)\n        metrics.extend(self._metrics)\n        return metrics\n\n    def _clear_previous_trainer_metrics(self):\n        for module in self._flatten_modules(include_self=False):\n            if not isinstance(module, Trainer):\n                continue\n            # A submodule might be a Trainer. In that case, we need to clear\n            # the Trainer-related metrics, as they are not usable when a\n            # new Trainer is instantiated.\n            for m in self._get_own_metrics():\n                module._tracker.untrack(m)\n            module._reward_tracker = None\n            module._compile_metrics = None\n            if module._compile_reward is not None:\n                module._compile_reward._metrics.clear()\n            module._metrics.clear()\n\n    @property\n    def run_eagerly(self):\n        return self._run_eagerly\n\n    @run_eagerly.setter\n    def run_eagerly(self, value):\n        self._run_eagerly = value\n\n    async def compute_reward(\n        self,\n        x=None,\n        y=None,\n        y_pred=None,\n        training=True,\n    ):\n        \"\"\"Compute the total reward, validate it, and return it.\n\n        Subclasses can optionally override this method to provide custom reward\n        computation logic.\n\n        Args:\n            x (list): Input data.\n            y (list): Target data.\n            y_pred (list): Predictions returned by the program (output of `program(x)`).\n            training (bool): Whether we are training or evaluating the program.\n\n        Returns:\n            (float | None): The total reward as a scalar, or `None` if no reward results\n                (which is the case when called by `Program.test_step`).\n        \"\"\"\n        # The default implementation does not use `x` or `training`.\n        del x\n        del training\n        rewards = []\n        if self._compile_reward is not None:\n            for y_t, y_p in zip(y, y_pred):\n                reward = await self._compile_reward(y_t, y_p)\n                if reward is not None:\n                    rewards.append(reward)\n        for reward in self.rewards:\n            rewards.append(numpy.sum(reward))\n        if len(rewards) == 1:\n            total_reward = rewards[0]\n        elif len(rewards) == 0:\n            total_reward = numpy.zeros(())\n        else:\n            total_reward = numpy.mean(rewards)\n        return float(total_reward)\n\n    def stateless_compute_reward(\n        self,\n        trainable_variables,\n        non_trainable_variables,\n        metrics_variables,\n        x=None,\n        y=None,\n        y_pred=None,\n        training=True,\n    ):\n        var_mapping = list(zip(self.trainable_variables, trainable_variables))\n        var_mapping.extend(zip(self.non_trainable_variables, non_trainable_variables))\n        var_mapping.extend(zip(self.metrics_variables, metrics_variables))\n        with backend.StatelessScope(state_mapping=var_mapping) as scope:\n            # Note that this is needed for the regularization reward, which need\n            # the latest value of train/non-trainable variables.\n            reward = self._compute_reward(\n                x,\n                y,\n                y_pred,\n                training=training,\n            )\n\n        # Update non trainable vars (may have been updated in compute_reward)\n        non_trainable_variables = []\n        for v in self.non_trainable_variables:\n            new_v = scope.get_current_value(v)\n            non_trainable_variables.append(new_v)\n\n        # Update metrics vars (may have been updated in compute_reward)\n        metrics_variables = []\n        for v in self.metrics_variables:\n            new_v = scope.get_current_value(v)\n            metrics_variables.append(new_v)\n        return reward, (\n            trainable_variables,\n            non_trainable_variables,\n            metrics_variables,\n        )\n\n    async def compute_metrics(self, x, y, y_pred):\n        \"\"\"Update metric states and collect all metrics to be returned.\n\n        Subclasses can optionally override this method to provide custom metric\n        updating and collection logic. Custom metrics are not passed in\n        `compile()`, they can be created in `__init__` or `build`. They are\n        automatically tracked and returned by `self.metrics`.\n        ```\n\n        Args:\n            x: Input data.\n            y: Target data.\n            y_pred: Predictions returned by the program output of `program.call(x)`.\n\n        Returns:\n            A `dict` containing values that will be passed to\n                `synalinks.callbacks.CallbackList.on_train_batch_end()`. Typically,\n                the values of the metrics listed in `self.metrics` are returned.\n                Example: `{'reward': 0.2, 'accuracy': 0.7}`.\n        \"\"\"\n        del x  # The default implementation does not use `x`.\n        if self._compile_metrics is not None:\n            for y_t, y_p in zip(y, y_pred):\n                await self._compile_metrics.update_state(y_t, y_p)\n        return self.get_metrics_result()\n\n    def get_metrics_result(self):\n        \"\"\"Returns the program's metrics values as a dict.\n\n        If any of the metric result is a dict (containing multiple metrics),\n        each of them gets added to the top level returned dict of this method.\n\n        Returns:\n            (dict): A `dict` containing values of the metrics listed in `self.metrics`.\n                Example: `{'reward': 0.2, 'accuracy': 0.7}`.\n        \"\"\"\n        return_metrics = {}\n        for metric in self.metrics:\n            result = metric.result()\n            if isinstance(result, dict):\n                return_metrics.update(result)\n            else:\n                return_metrics[metric.name] = result\n        return python_utils.pythonify_logs(return_metrics)\n\n    async def fit(\n        self,\n        x=None,\n        y=None,\n        batch_size=1,\n        minibatch_size=4,\n        epochs=1,\n        verbose=\"auto\",\n        callbacks=None,\n        validation_split=0.1,\n        validation_data=None,\n        shuffle=True,\n        initial_epoch=0,\n        steps_per_epoch=None,\n        validation_steps=None,\n        validation_batch_size=32,\n        validation_freq=1,\n    ):\n        \"\"\"Trains the program for a fixed number of epochs (dataset iterations).\n\n        Args:\n            x (np.ndarray | generator): Input data. It can be:\n                - A NumPy array (or array-like), or a list of `DataModel` arrays\n                    (in case the model has multiple inputs).\n                - A list of dict mapping input names to the corresponding `DataModel`s,\n                    if the program has named inputs.\n                - A Python generator function yielding `(inputs, targets)`.\n            y (np.ndarray): Target data. Like the input data `x`, it can be either NumPy\n                array(s) of `DataModel`(s). If `x` is a Python generator function,\n                `y` should not be specified since targets will be obtained from\n                `x`.\n            batch_size (int): Integer or `None`.\n                Number of samples per batch of computation.\n                If unspecified, `batch_size` will default to 32.\n                Do not specify the `batch_size` if your input data `x` is a\n                Python generator function since they generate batches.\n            minibatch_size (int): Integer or `None`.\n                Number of randomly selected samples per batch validation.\n                If unspecified, `minibatch_size` will default to 4.\n                If `None`, the whole validation set will be used.\n            epochs (int): Integer. Number of epochs to train the program.\n                An epoch is an iteration over the entire `x` and `y`\n                data provided (unless the `steps_per_epoch` flag is set to\n                something other than None).\n                Note that in conjunction with `initial_epoch`,\n                `epochs` is to be understood as \"final epoch\".\n                The program is not trained for a number of iterations\n                given by `epochs`, but merely until the epoch\n                of index `epochs` is reached.\n            verbose (int): `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = one line per epoch.\n                \"auto\" becomes 1 for most cases.\n                Note that the progress bar is not\n                particularly useful when logged to a file,\n                so `verbose=2` is recommended when not running interactively\n                (e.g., in a production environment). Defaults to `\"auto\"`.\n            callbacks (list): List of `synalinks.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n                See `synalinks.callbacks`. Note\n                `synalinks.callbacks.ProgbarLogger` and\n                `synalinks.callbacks.History` callbacks are created\n                automatically and need not be passed to `program.fit()`.\n                `synalinks.callbacks.ProgbarLogger` is created\n                or not based on the `verbose` argument in `program.fit()`.\n            validation_split (float): Float between 0 and 1.\n                Fraction of the training data to be used as validation data.\n                The program will set apart this fraction of the training data,\n                will not train on it, and will evaluate the reward and any program\n                metrics on this data at the end of each epoch. The validation\n                data is selected from the last samples in the `x` and `y` data\n                provided, before shuffling.\n                This argument is only supported when `x` and `y` are made of\n                data_models.\n                If both `validation_data` and `validation_split` are provided,\n                `validation_data` will override `validation_split`.\n            validation_data (tuple | iterator): Data on which to evaluate\n                the reward and any program metrics at the end of each epoch.\n                The program will not be trained on this data.\n                `validation_data` will override `validation_split`.\n                It can be:\n                - A tuple `(x_val, y_val)` of `DataModel`s lists.\n            shuffle (bool): Whether to shuffle the training data before each\n                epoch. This argument is ignored when `x` is a Python generator function.\n            initial_epoch (int): Integer.\n                Epoch at which to start training\n                (useful for resuming a previous training run).\n            steps_per_epoch (int): Integer or `None`.\n                Total number of steps (batches of samples) before declaring one\n                epoch finished and starting the next epoch. When training with\n                input data_models arrays, the default `None` means that the\n                value used is the number of samples in your dataset divided by\n                the batch size, or 1 if that cannot be determined.\n                If `x` is a Python generator function, the\n                epoch will run until the input dataset is exhausted. When\n                passing an infinitely repeating dataset, you must specify the\n                `steps_per_epoch` argument, otherwise the training will run\n                indefinitely.\n            validation_steps (int): Integer or `None`.\n                Only relevant if `validation_data` is provided.\n                Total number of steps (batches of samples) to draw before\n                stopping when performing validation at the end of every epoch.\n                If `validation_steps` is `None`, validation will run until the\n                `validation_data` dataset is exhausted. In the case of an\n                infinitely repeating dataset, it will run indefinitely. If\n                `validation_steps` is specified and only part of the dataset\n                is consumed, the evaluation will start from the beginning of the\n                dataset at each epoch. This ensures that the same validation\n                samples are used every time.\n            validation_batch_size (int): Integer or `None`.\n                Number of samples per validation batch.\n                If unspecified, will default to `batch_size`.\n                Do not specify the `validation_batch_size` if your data is a\n                `synalinks.utils.PyDataset`, `tf.data.Dataset`,\n                `torch.utils.data.DataLoader` or Python generator function\n                since they generate batches.\n            validation_freq (int): Only relevant if validation data is provided.\n                Specifies how many training epochs to run\n                before a new validation run is performed,\n                e.g. `validation_freq=2` runs validation every 2 epochs.\n\n        Returns:\n            (History): A `History` object. Its `History.history` attribute is\n                a record of training reward values and metrics values\n                at successive epochs, as well as validation reward values\n                and validation metrics values (if applicable).\n        \"\"\"\n        self._assert_compile_called(\"fit\")\n        self._eval_epoch_iterator = None\n        val_y, val_y = None, None\n\n        if validation_split and validation_data is None:\n            # Create the validation data using the training data. Only supported\n            # for numpy arrays.\n            (x, y), validation_data = array_slicing.train_validation_split(\n                (x, y), validation_split=validation_split\n            )\n\n        if validation_data is not None:\n            val_x, val_y = data_adapter_utils.unpack_x_y(validation_data)\n        # Create an iterator that yields batches of input/target data.\n        epoch_iterator = EpochIterator(\n            x=x,\n            y=y,\n            batch_size=batch_size,\n            steps_per_epoch=steps_per_epoch,\n            shuffle=False,\n            steps_per_execution=self.steps_per_execution,\n        )\n\n        if not all(module.built for module in self._flatten_modules()):\n            # Build the model on one batch of data.\n            for _, data in epoch_iterator:\n                data_batch = data[0]\n                self._auto_build(\n                    iterator=epoch_iterator,\n                    data_batch=data_batch,\n                )\n                break\n        epoch_iterator.reset()\n\n        # Container that configures and calls callbacks.\n        if not isinstance(callbacks, callbacks_module.CallbackList):\n            # Get optimizer name for logging\n            optimizer_name = None\n            if self._optimizer is not None:\n                optimizer_name = self._optimizer.__class__.__name__\n\n            callbacks = callbacks_module.CallbackList(\n                callbacks,\n                add_history=True,\n                add_progbar=verbose != 0,\n                verbose=verbose,\n                epochs=epochs,\n                steps=steps_per_epoch,\n                batch_size=batch_size,\n                optimizer=optimizer_name,\n                program=self,\n            )\n\n        self.stop_training = False\n        callbacks.on_train_begin()\n        training_logs = None\n        logs = {}\n        initial_epoch = self._initial_epoch or initial_epoch\n\n        if self.trainable_variables and isinstance(\n            self.optimizer, optimizers_module.Optimizer\n        ):\n            await self.optimizer.on_train_begin(\n                self.trainable_variables,\n            )\n\n        for epoch in range(initial_epoch, epochs):\n            self.reset_metrics()\n\n            if self.trainable_variables and isinstance(\n                self.optimizer, optimizers_module.Optimizer\n            ):\n                await self.optimizer.on_epoch_begin(\n                    epoch,\n                    self.trainable_variables,\n                )\n\n            callbacks.on_epoch_begin(epoch)\n            with epoch_iterator.catch_stop_iteration():\n                for step, iterator in epoch_iterator:\n                    data = iterator[0]\n                    x_batch, y_batch = data_adapter_utils.unpack_x_y(data)\n\n                    if self.trainable_variables and isinstance(\n                        self.optimizer, optimizers_module.Optimizer\n                    ):\n                        await self.optimizer.on_batch_begin(\n                            step,\n                            epoch,\n                            self.trainable_variables,\n                        )\n\n                    callbacks.on_train_batch_begin(step)\n\n                    mini_val_x = None\n                    mini_val_y = None\n                    if minibatch_size:\n                        if len(val_x) &gt; minibatch_size:\n                            indices = np.random.choice(\n                                len(val_x),\n                                size=minibatch_size,\n                                replace=False,\n                            )\n                            mini_val_x = val_x[indices]\n                            mini_val_y = val_y[indices]\n\n                    logs = await self.train_on_batch(\n                        step=step,\n                        x=x_batch,\n                        y=y_batch,\n                        val_x=mini_val_x if mini_val_x is not None else val_x,\n                        val_y=mini_val_y if mini_val_y is not None else val_y,\n                        return_dict=True,\n                    )\n\n                    val_logs = await self.evaluate(\n                        x=val_x,\n                        y=val_y,\n                        batch_size=validation_batch_size or batch_size,\n                        steps=validation_steps,\n                        callbacks=callbacks,\n                        _use_cached_eval_dataset=False,\n                    )\n\n                    if self.trainable_variables and isinstance(\n                        self.optimizer, optimizers_module.Optimizer\n                    ):\n                        await self.optimizer.on_batch_end(\n                            step,\n                            epoch,\n                            self.trainable_variables,\n                        )\n\n                    callbacks.on_train_batch_end(step, logs)\n                    if self.stop_training:\n                        break\n\n            # Override with model metrics instead of last step logs if needed.\n            epoch_logs = dict(self._get_metrics_result_or_logs(logs))\n\n            # Run validation.\n            if validation_data is not None and self._should_eval(epoch, validation_freq):\n                # Create EpochIterator for evaluation and cache it.\n                if getattr(self, \"_eval_epoch_iterator\", None) is None:\n                    self._eval_epoch_iterator = EpochIterator(\n                        x=val_x,\n                        y=val_y,\n                        batch_size=validation_batch_size or batch_size,\n                        steps_per_execution=self.steps_per_execution,\n                        steps_per_epoch=validation_steps,\n                        shuffle=False,\n                    )\n\n                val_logs = await self.evaluate(\n                    x=val_x,\n                    y=val_y,\n                    batch_size=validation_batch_size or batch_size,\n                    steps=validation_steps,\n                    callbacks=callbacks,\n                    _use_cached_eval_dataset=True,\n                )\n                val_logs = {\"val_\" + name: val for name, val in val_logs.items()}\n                epoch_logs.update(val_logs)\n\n            if self.trainable_variables and isinstance(\n                self.optimizer, optimizers_module.Optimizer\n            ):\n                await self.optimizer.on_epoch_end(\n                    epoch,\n                    self.trainable_variables,\n                )\n\n            callbacks.on_epoch_end(epoch, epoch_logs)\n            training_logs = epoch_logs\n            if self.stop_training:\n                break\n\n        # If _eval_epoch_iterator exists, delete it after all epochs are done.\n        if getattr(self, \"_eval_epoch_iterator\", None) is not None:\n            del self._eval_epoch_iterator\n\n        if self.trainable_variables and isinstance(\n            self.optimizer, optimizers_module.Optimizer\n        ):\n            await self.optimizer.on_train_end(self.trainable_variables)\n\n        callbacks.on_train_end(logs=training_logs)\n        return self.history\n\n    async def evaluate(\n        self,\n        x=None,\n        y=None,\n        batch_size=32,\n        verbose=\"auto\",\n        steps=None,\n        callbacks=None,\n        return_dict=True,\n        **kwargs,\n    ):\n        \"\"\"Returns the reward value &amp; metrics values for the program in test mode.\n\n        Computation is done in batches (see the `batch_size` arg.)\n\n        Args:\n            x (np.ndarray | generator): Input data. It can be:\n                - A NumPy array (or array-like), or a list of `DataModel` arrays\n                    (in case the model has multiple inputs).\n                - A list of dict mapping input names to the corresponding `DataModel`s,\n                    if the program has named inputs.\n                - A Python generator function yielding `(inputs, targets)`.\n            y (np.ndarray): Target data. Like the input data `x`, it can be either NumPy\n                array(s) of `DataModel`(s). If `x` is a Python generator function,\n                `y` should not be specified since targets will be obtained from\n                `x`.\n            batch_size (int): Integer or `None`.\n                Number of samples per batch of computation.\n                If unspecified, `batch_size` will default to 32.\n                Do not specify the `batch_size` if your input data `x` is a\n                Python generator function since they generate batches.\n            verbose (int | str): `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = single line.\n                `\"auto\"` becomes 1 for most cases.\n                Note that the progress bar is not\n                particularly useful when logged to a file, so `verbose=2` is\n                recommended when not running interactively\n                (e.g. in a production environment). Defaults to `\"auto\"`.\n            steps (int): Integer or `None`.\n                Total number of steps (batches of samples) to draw before\n                declaring the evaluation round finished. If `steps` is `None`,\n                it will run until `x` is exhausted. In the case of an infinitely\n                repeating dataset, it will run indefinitely.\n            callbacks (list): List of `synalinks.callbacks.Callback` instances.\n                List of callbacks to apply during evaluation.\n            return_dict (bool): If `True`, reward and metric results are returned as a\n                dict, with each key being the name of the metric.\n                If `False`, they are returned as a list.\n\n        Returns:\n            (float | list | dict): Scalar test reward\n                (if the program has a single output and no metrics)\n                or list of scalars (if the program has multiple outputs\n                and/or metrics). The attribute `program.metrics_names` will give you\n                the display labels for the scalar outputs.\n        \"\"\"\n        self._assert_compile_called(\"evaluate\")\n        use_cached_eval_dataset = kwargs.pop(\"_use_cached_eval_dataset\", False)\n        if kwargs:\n            raise ValueError(f\"Arguments not recognized: {kwargs}\")\n        # Create an iterator that yields batches of input/target data.\n        if use_cached_eval_dataset:\n            epoch_iterator = self._eval_epoch_iterator\n        else:\n            epoch_iterator = EpochIterator(\n                x=x,\n                y=y,\n                batch_size=batch_size,\n                steps_per_epoch=steps,\n                shuffle=False,\n                steps_per_execution=self.steps_per_execution,\n            )\n\n        if not all(module.built for module in self._flatten_modules()):\n            # Build the model on one batch of data.\n            for _, data in epoch_iterator:\n                data_batch = data[0]\n                self._auto_build(\n                    iterator=epoch_iterator,\n                    data_batch=data_batch,\n                )\n                break\n        epoch_iterator.reset()\n\n        # Container that configures and calls callbacks.\n        if not isinstance(callbacks, callbacks_module.CallbackList):\n            callbacks = callbacks_module.CallbackList(\n                callbacks,\n                add_history=False,\n                add_progbar=verbose != 0,\n                verbose=verbose,\n                epochs=1,\n                steps=epoch_iterator.num_batches,\n                program=self,\n            )\n\n        self.stop_evaluating = False\n        callbacks.on_test_begin()\n        logs = {}\n        self.reset_metrics()\n        for step, iterator in epoch_iterator:\n            callbacks.on_test_batch_begin(step)\n            data = iterator[0]\n            x_batch, y_batch = data_adapter_utils.unpack_x_y(data)\n            logs = await self.test_on_batch(\n                x=x_batch,\n                y=y_batch,\n                return_dict=True,\n            )\n            callbacks.on_test_batch_end(step, logs)\n            if self.stop_evaluating:\n                break\n        logs = self.get_metrics_result()\n        callbacks.on_test_end(logs)\n\n        if return_dict:\n            return logs\n        return self._flatten_metrics_in_order(logs)\n\n    async def predict(\n        self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n    ):\n        \"\"\"Generates output predictions for the input samples.\n\n        Computation is done in batches. This method is designed for batch\n        processing of large numbers of inputs. It is not intended for use inside\n        of loops that iterate over your data and process small numbers of inputs\n        at a time.\n\n        For small numbers of inputs that fit in one batch,\n        directly use `__call__()` for faster execution, e.g.,\n        `program(x)`, or `program(x, training=False)` if you have modules\n        that behave differently during inference.\n\n        Args:\n            x (np.ndarray | generator): Input data. It can be:\n                - A NumPy array (or array-like), or a list of `DataModel` arrays\n                    (in case the model has multiple inputs).\n                - A list of dict mapping input names to the corresponding `DataModel`s,\n                    if the program has named inputs.\n                - A Python generator function yielding `(inputs, targets)`.\n            batch_size (int): Integer or `None`.\n                Number of samples per batch of computation.\n                If unspecified, `batch_size` will default to 32.\n                Do not specify the `batch_size` if your input data `x` is a\n                `synalinks.utils.PyDataset`, `tf.data.Dataset`,\n                `torch.utils.data.DataLoader` or Python generator function\n                since they generate batches.\n            verbose (int): `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = single line.\n                `\"auto\"` becomes 1 for most cases. Note that the progress bar\n                is not particularly useful when logged to a file,\n                so `verbose=2` is recommended when not running interactively\n                (e.g. in a production environment). Defaults to `\"auto\"`.\n            steps (int): Total number of steps (batches of samples) to draw before\n                declaring the prediction round finished. If `steps` is `None`,\n                it will run until `x` is exhausted. In the case of an infinitely\n                repeating dataset, it will run indefinitely.\n            callbacks (list): List of `synalinks.callbacks.Callback` instances.\n                List of callbacks to apply during prediction.\n\n        Returns:\n            (list): `JsonDataModel` array(s) of predictions.\n                If the pipeline failed, a None is added to the predictions.\n        \"\"\"\n        # Create an iterator that yields batches of input data.\n        epoch_iterator = EpochIterator(\n            x=x,\n            batch_size=batch_size,\n            steps_per_epoch=steps,\n            shuffle=False,\n            steps_per_execution=self.steps_per_execution,\n        )\n\n        # Container that configures and calls callbacks.\n        if not isinstance(callbacks, callbacks_module.CallbackList):\n            callbacks = callbacks_module.CallbackList(\n                callbacks,\n                add_history=True,\n                add_progbar=verbose != 0,\n                verbose=verbose,\n                epochs=1,\n                steps=epoch_iterator.num_batches,\n                model=self,\n            )\n\n        self.stop_predicting = False\n        callbacks.on_test_begin()\n        outputs = []\n        for step, iterator in epoch_iterator:\n            callbacks.on_predict_batch_begin(step)\n            data = iterator[0]\n            x_batch, _ = data_adapter_utils.unpack_x_y(data)\n            batch_outputs = await self.predict_on_batch(x_batch)\n            outputs.extend(batch_outputs)\n            callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n            if self.stop_predicting:\n                break\n        callbacks.on_predict_end()\n        return np.array(outputs, dtype=\"object\")\n\n    async def train_on_batch(\n        self,\n        step,\n        x,\n        y=None,\n        val_x=None,\n        val_y=None,\n        return_dict=False,\n    ):\n        \"\"\"Runs a single optimization step on a single batch of data.\n\n        Args:\n            step (int): The training step.\n            x (np.ndarray): Input data. Must be array-like.\n            y (np.ndarray): Target data. Must be array-like.\n            val_x (np.ndarray): Input validation data. Must be array-like.\n            val_y (np.ndarray): Target validation data. Must be array-like.\n            return_dict (bool): If `True`, reward and metric results are returned as a\n                dict, with each key being the name of the metric. If `False`,\n                they are returned as a list.\n\n        Returns:\n            (float | list | dict): A scalar reward value\n                (when no metrics and `return_dict=False`), a list of reward\n                and metric values (if there are metrics and `return_dict=False`),\n                or a dict of metric and reward values (if `return_dict=True`).\n        \"\"\"\n        if self.trainable_variables and isinstance(\n            self.optimizer, optimizers_module.Optimizer\n        ):\n            metrics = await self.optimizer.optimize(\n                step,\n                self.trainable_variables,\n                x=x,\n                y=y,\n                val_x=val_x,\n                val_y=val_y,\n            )\n        else:\n            warnings.warn(\"The program does not have any trainable variables.\")\n            y_pred = await self.predict_on_batch(val_x)\n            reward = await self.compute_reward(\n                x=val_x,\n                y=val_y,\n                y_pred=y_pred,\n            )\n            await self._reward_tracker.update_state(reward)\n            metrics = await self.compute_metrics(val_x, val_y, y_pred)\n\n        if return_dict:\n            return metrics\n        return self._flatten_metrics_in_order(metrics)\n\n    async def test_on_batch(\n        self,\n        x,\n        y=None,\n        return_dict=False,\n    ):\n        \"\"\"Test the program on a single batch of samples.\n\n        Args:\n            x (np.ndarray): Input data. Must be array-like.\n            y (np.ndarray): Target data. Must be array-like.\n            return_dict (bool): If `True`, reward and metric results are returned as a\n                dict, with each key being the name of the metric. If `False`,\n                they are returned as a list.\n\n        Returns:\n            (float | list | dict): A scalar reward value\n                (when no metrics and `return_dict=False`), a list of reward\n                and metric values (if there are metrics and `return_dict=False`),\n                or a dict of metric and reward values (if `return_dict=True`).\n        \"\"\"\n        y_pred = await self.predict_on_batch(x)\n\n        reward = await self.compute_reward(\n            x=x,\n            y=y,\n            y_pred=y_pred,\n            training=False,\n        )\n        await self._reward_tracker.update_state(reward)\n\n        metrics = await self.compute_metrics(x, y, y_pred)\n\n        if return_dict:\n            return metrics\n        return self._flatten_metrics_in_order(metrics)\n\n    async def predict_on_batch(self, x, training=False):\n        \"\"\"Returns predictions for a single batch of samples.\n\n        Args:\n            x (np.ndarray): Input data. Must be array-like.\n            training (bool): Boolean. True if training.\n\n        Returns:\n            (list): list(s) of JsonDataModel predictions.\n        \"\"\"\n        tasks = []\n        for inputs in x:\n            tasks.append(self(inputs, training=training))\n        y_pred = await asyncio.gather(*tasks)\n        return y_pred\n\n    def get_compile_config(self):\n        \"\"\"Returns a serialized config with information for compiling the program.\n\n        This method returns a config dictionary containing all the information\n        (optimizer, reward, metrics, etc.) with which the program was compiled.\n\n        Returns:\n            (dict): A dict containing information for compiling the program.\n        \"\"\"\n        if self.compiled and hasattr(self, \"_compile_config\"):\n            return self._compile_config.serialize()\n\n    def compile_from_config(self, config):\n        \"\"\"Compiles the program with the information given in config.\n\n        This method uses the information in the config (optimizer, reward,\n        metrics, etc.) to compile the program.\n\n        Args:\n            config (dict): Dict containing information for compiling the program.\n        \"\"\"\n        has_overridden_compile = self.__class__.compile != Trainer.compile\n        if has_overridden_compile:\n            warnings.warn(\n                \"`compile()` was not called as part of program loading \"\n                \"because the program's `compile()` method is custom. \"\n                \"All subclassed Models that have `compile()` \"\n                \"overridden should also override \"\n                \"`get_compile_config()` and `compile_from_config(config)`. \"\n                \"Alternatively, you can \"\n                \"call `compile()` manually after loading.\",\n                stacklevel=2,\n            )\n            return\n        config = serialization_lib.deserialize_synalinks_object(config)\n        self.compile(**config)\n        if hasattr(self, \"optimizer\") and self.built:\n            # Create optimizer variables/programs.\n            if not self.optimizer.built:\n                run_maybe_nested(self.optimizer.build(self.trainable_variables))\n\n    def _should_reward(self, epoch, validation_freq):\n        epoch = epoch + 1  # one-index the user-facing epoch.\n        if isinstance(validation_freq, int):\n            return epoch % validation_freq == 0\n        elif isinstance(validation_freq, list):\n            return epoch in validation_freq\n        else:\n            raise ValueError(\n                \"Expected `validation_freq` to be a list or int. \"\n                f\"Received: validation_freq={validation_freq} of the \"\n                f\"type {type(validation_freq)}.\"\n            )\n\n    def _get_metrics_result_or_logs(self, logs):\n        \"\"\"Returns program metrics as a dict if the keys match with input logs.\n\n        When the training / evaluation is performed with an asynchronous steps,\n        the last scheduled `train / test_step` may not give the latest metrics\n        because it is not guaranteed to be executed the last. This method gets\n        metrics from the program directly instead of relying on the return from\n        last step function.\n\n        When the user has custom train / test step functions, the metrics\n        returned may be different from `Program.metrics`. In those instances,\n        this function will be no-op and return the logs passed in.\n\n        Args:\n            logs (dict): A `dict` of metrics returned by train / test step function.\n\n        Returns:\n            (dict): A `dict` containing values of the metrics listed in `self.metrics`\n                when logs and program metrics keys match. Otherwise it returns input\n                `logs`.\n        \"\"\"\n        metric_logs = self.get_metrics_result()\n        # Verify that train / test step logs passed and metric logs have\n        # matching keys. It could be different when using custom step functions,\n        # in which case we return the logs from the last step.\n        if isinstance(logs, dict) and set(logs.keys()) == set(metric_logs.keys()):\n            return metric_logs\n        return logs\n\n    def _flatten_metrics_in_order(self, logs):\n        \"\"\"Turns `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n        metric_names = []\n        for metric in self.metrics:\n            if isinstance(metric, CompileMetrics):\n                metric_names += [sub_metric.name for sub_metric in metric.metrics]\n            else:\n                metric_names.append(metric.name)\n        results = []\n        for name in metric_names:\n            if name in logs:\n                results.append(logs[name])\n        for key in sorted(logs.keys()):\n            if key not in metric_names:\n                results.append(logs[key])\n        if len(results) == 1:\n            return results[0]\n        return results\n\n    def _assert_compile_called(self, method_name=None):\n        if not self.compiled:\n            msg = \"You must call `compile()` before \"\n            if metrics_module:\n                msg += \"using the program.\"\n            else:\n                msg += f\"calling `{method_name}()`.\"\n            raise ValueError(msg)\n\n    def _auto_build(self, iterator=None, data_batch=None):\n        program_unbuilt = not all(module.built for module in self._flatten_modules())\n        compile_metrics_unbuilt = (\n            self._compile_metrics is not None and not self._compile_metrics.built\n        )\n        compile_reward_unbuilt = (\n            self._compile_reward is not None and not self._compile_reward.built\n        )\n        optimizer_unbuilt = self.optimizer is not None and not self.optimizer.built\n        if program_unbuilt or compile_metrics_unbuilt or compile_reward_unbuilt:\n            if data_batch is None:\n                for _, data_or_iterator in iterator:\n                    if isinstance(data_or_iterator, (list, tuple)):\n                        data_batch = data_or_iterator[0]\n                    else:\n                        data_batch = next(data_or_iterator)\n                    break\n            x, y = data_batch\n            try:\n                y_pred = run_maybe_nested(self.predict_on_batch(x))\n            except Exception as e:\n                raise RuntimeError(\n                    \"Unable to automatically build the program. \"\n                    \"Please build it yourself before calling \"\n                    \"fit/evaluate/predict. \"\n                    \"A program is 'built' when its variables have \"\n                    \"been created and its `self.built` attribute \"\n                    \"is True. Usually, calling the program on a batch \"\n                    \"of data is the right way to build it.\\n\"\n                    \"Exception encountered:\\n\"\n                    f\"'{e}'\"\n                )\n            if compile_metrics_unbuilt:\n                # Build all metric state with `backend.compute_output_spec`.\n                run_maybe_nested(\n                    backend.compute_output_spec(\n                        self.compute_metrics,\n                        x,\n                        y,\n                        y_pred,\n                    )\n                )\n            if compile_reward_unbuilt:\n                # Build `CompileReward` state with `backend.compute_output_spec`.\n                run_maybe_nested(\n                    backend.compute_output_spec(\n                        self.compute_reward,\n                        x,\n                        y,\n                        y_pred,\n                        training=False,\n                    )\n                )\n        if optimizer_unbuilt:\n            # Build optimizer\n            run_maybe_nested(self.optimizer.build(self.trainable_variables))\n        self._post_build()\n\n    def _assert_compile_called(self, method_name=None):\n        if not self.compiled:\n            msg = \"You must call `compile()` before \"\n            if metrics_module:\n                msg += \"using the model.\"\n            else:\n                msg += f\"calling `{method_name}()`.\"\n            raise ValueError(msg)\n\n    def _should_eval(self, epoch, validation_freq):\n        epoch = epoch + 1  # one-index the user-facing epoch.\n        if isinstance(validation_freq, int):\n            return epoch % validation_freq == 0\n        elif isinstance(validation_freq, list):\n            return epoch in validation_freq\n        else:\n            raise ValueError(\n                \"Expected `validation_freq` to be a list or int. \"\n                f\"Received: validation_freq={validation_freq} of the \"\n                f\"type {type(validation_freq)}.\"\n            )\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.compile","title":"<code>compile(optimizer=None, reward=None, reward_weights=None, metrics=None, run_eagerly=False, steps_per_execution=1)</code>","text":"<p>Configures the program for training.</p> <p>Example:</p> <pre><code>program.compile(\n    optimizer=synalinks.optimizers.RandomFewShot(),\n    reward=synalinks.rewards.ExactMatch(),\n    metrics=[\n        synalinks.metrics.MeanMetricWrapper(synalinks.rewards.exact_match),\n    ],\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>Optimizer instance. See <code>synalinks.optimizers</code>.</p> <code>None</code> <code>reward</code> <code>Reward</code> <p>Reward function. A <code>synalinks.rewards.Reward</code> instance. See <code>synalinks.rewards</code>. A reward function is any callable with the signature <code>reward = fn(y_true, y_pred)</code>, where <code>y_true</code> are the ground truth values, and <code>y_pred</code> are the program's predictions. <code>y_true</code> should be a list of batch size length <code>[d0, .. dN]</code>. <code>y_pred</code> should be a list of batch size length <code>[d0, .. dN]</code>. The reward function should return a float.</p> <code>None</code> <code>reward_weights</code> <code>list</code> <p>Optional list specifying scalar coefficients (Python floats) to weight the reward contributions of different program outputs. The reward value that will be maximized by the program will then be the weighted sum of all individual rewards, weighted by the <code>reward_weights</code> coefficients. It is expected to have a 1:1 mapping to the program's outputs.</p> <code>None</code> <code>metrics</code> <code>list</code> <p>List of metrics to be evaluated by the program during training and testing. Each of it is a <code>synalinks.metrics.Metric</code> instance. See <code>synalinks.metrics</code>. A function is any callable with the signature <code>result = fn(y_true, y_pred)</code>.</p> <code>None</code> <code>run_eagerly</code> <code>bool</code> <p>If <code>True</code>, this program's forward pass will never be compiled. It is recommended to leave this as <code>False</code> when training (for best performance), and to set it to <code>True</code> when debugging.</p> <code>False</code> <code>steps_per_execution</code> <code>int</code> <p>The number of batches to run during each a single compiled function call. Running multiple batches inside a single compiled function call can greatly improve performance on TPUs or small programs with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if <code>steps_per_execution</code> is set to <code>N</code>, <code>Callback.on_batch_begin</code> and <code>Callback.on_batch_end</code> methods will only be called every <code>N</code> batches (i.e. before/after each compiled function execution).</p> <code>1</code> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>@tracking.no_automatic_dependency_tracking\ndef compile(\n    self,\n    optimizer=None,\n    reward=None,\n    reward_weights=None,\n    metrics=None,\n    run_eagerly=False,\n    steps_per_execution=1,\n):\n    \"\"\"Configures the program for training.\n\n    Example:\n\n    ```python\n    program.compile(\n        optimizer=synalinks.optimizers.RandomFewShot(),\n        reward=synalinks.rewards.ExactMatch(),\n        metrics=[\n            synalinks.metrics.MeanMetricWrapper(synalinks.rewards.exact_match),\n        ],\n    )\n    ```\n\n    Args:\n        optimizer (Optimizer): Optimizer instance. See `synalinks.optimizers`.\n        reward (Reward): Reward function. A `synalinks.rewards.Reward`\n            instance. See `synalinks.rewards`. A reward function is\n            any callable with the signature `reward = fn(y_true, y_pred)`,\n            where `y_true` are the ground truth values, and `y_pred`\n            are the program's predictions.\n            `y_true` should be a list of batch size length `[d0, .. dN]`.\n            `y_pred` should be a list of batch size length `[d0, .. dN]`.\n            The reward function should return a float.\n        reward_weights (list): Optional list specifying scalar coefficients\n            (Python floats) to weight the reward contributions of\n            different program outputs. The reward value that will be maximized\n            by the program will then be the *weighted sum* of all individual\n            rewards, weighted by the `reward_weights` coefficients. It is\n            expected to have a 1:1 mapping to the program's outputs.\n        metrics (list): List of metrics to be evaluated by the program during\n            training and testing. Each of it is a `synalinks.metrics.Metric`\n            instance. See `synalinks.metrics`. A function is any callable with the\n            signature `result = fn(y_true, y_pred)`.\n        run_eagerly (bool): If `True`, this program's forward pass\n            will never be compiled. It is recommended to leave this\n            as `False` when training (for best performance),\n            and to set it to `True` when debugging.\n        steps_per_execution (int): The number of batches to run\n            during each a single compiled function call. Running multiple\n            batches inside a single compiled function call can\n            greatly improve performance on TPUs or small programs with a large\n            Python overhead. At most, one full epoch will be run each\n            execution. If a number larger than the size of the epoch is\n            passed, the execution will be truncated to the size of the\n            epoch. Note that if `steps_per_execution` is set to `N`,\n            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n            will only be called every `N` batches (i.e. before/after\n            each compiled function execution).\n    \"\"\"\n    self._clear_previous_trainer_metrics()\n    self._optimizer = optimizer\n    self._optimizer.set_program(self)\n\n    if hasattr(self, \"output_names\"):\n        output_names = self.output_names\n    else:\n        output_names = None\n    if reward is not None:\n        self._compile_reward = CompileReward(\n            reward, reward_weights, output_names=output_names\n        )\n        self.reward = reward\n    if metrics is not None:\n        self._compile_metrics = CompileMetrics(metrics, output_names=output_names)\n    self.run_eagerly = run_eagerly\n    self.stop_training = False\n    self.compiled = True\n    self._reward_tracker = metrics_module.Mean(name=\"reward\")\n    self.steps_per_execution = steps_per_execution\n\n    self._compile_config = serialization_lib.SerializableDict(\n        optimizer=optimizer,\n        reward=reward,\n        reward_weights=reward_weights,\n        metrics=metrics,\n        run_eagerly=run_eagerly,\n        steps_per_execution=steps_per_execution,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.compile_from_config","title":"<code>compile_from_config(config)</code>","text":"<p>Compiles the program with the information given in config.</p> <p>This method uses the information in the config (optimizer, reward, metrics, etc.) to compile the program.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dict containing information for compiling the program.</p> required Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>def compile_from_config(self, config):\n    \"\"\"Compiles the program with the information given in config.\n\n    This method uses the information in the config (optimizer, reward,\n    metrics, etc.) to compile the program.\n\n    Args:\n        config (dict): Dict containing information for compiling the program.\n    \"\"\"\n    has_overridden_compile = self.__class__.compile != Trainer.compile\n    if has_overridden_compile:\n        warnings.warn(\n            \"`compile()` was not called as part of program loading \"\n            \"because the program's `compile()` method is custom. \"\n            \"All subclassed Models that have `compile()` \"\n            \"overridden should also override \"\n            \"`get_compile_config()` and `compile_from_config(config)`. \"\n            \"Alternatively, you can \"\n            \"call `compile()` manually after loading.\",\n            stacklevel=2,\n        )\n        return\n    config = serialization_lib.deserialize_synalinks_object(config)\n    self.compile(**config)\n    if hasattr(self, \"optimizer\") and self.built:\n        # Create optimizer variables/programs.\n        if not self.optimizer.built:\n            run_maybe_nested(self.optimizer.build(self.trainable_variables))\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.compute_metrics","title":"<code>compute_metrics(x, y, y_pred)</code>  <code>async</code>","text":"<p>Update metric states and collect all metrics to be returned.</p> <p>Subclasses can optionally override this method to provide custom metric updating and collection logic. Custom metrics are not passed in <code>compile()</code>, they can be created in <code>__init__</code> or <code>build</code>. They are automatically tracked and returned by <code>self.metrics</code>. ```</p> <p>Args:     x: Input data.     y: Target data.     y_pred: Predictions returned by the program output of <code>program.call(x)</code>.</p> <p>Returns:     A <code>dict</code> containing values that will be passed to         <code>synalinks.callbacks.CallbackList.on_train_batch_end()</code>. Typically,         the values of the metrics listed in <code>self.metrics</code> are returned.         Example: <code>{'reward': 0.2, 'accuracy': 0.7}</code>.</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def compute_metrics(self, x, y, y_pred):\n    \"\"\"Update metric states and collect all metrics to be returned.\n\n    Subclasses can optionally override this method to provide custom metric\n    updating and collection logic. Custom metrics are not passed in\n    `compile()`, they can be created in `__init__` or `build`. They are\n    automatically tracked and returned by `self.metrics`.\n    ```\n\n    Args:\n        x: Input data.\n        y: Target data.\n        y_pred: Predictions returned by the program output of `program.call(x)`.\n\n    Returns:\n        A `dict` containing values that will be passed to\n            `synalinks.callbacks.CallbackList.on_train_batch_end()`. Typically,\n            the values of the metrics listed in `self.metrics` are returned.\n            Example: `{'reward': 0.2, 'accuracy': 0.7}`.\n    \"\"\"\n    del x  # The default implementation does not use `x`.\n    if self._compile_metrics is not None:\n        for y_t, y_p in zip(y, y_pred):\n            await self._compile_metrics.update_state(y_t, y_p)\n    return self.get_metrics_result()\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.compute_reward","title":"<code>compute_reward(x=None, y=None, y_pred=None, training=True)</code>  <code>async</code>","text":"<p>Compute the total reward, validate it, and return it.</p> <p>Subclasses can optionally override this method to provide custom reward computation logic.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list</code> <p>Input data.</p> <code>None</code> <code>y</code> <code>list</code> <p>Target data.</p> <code>None</code> <code>y_pred</code> <code>list</code> <p>Predictions returned by the program (output of <code>program(x)</code>).</p> <code>None</code> <code>training</code> <code>bool</code> <p>Whether we are training or evaluating the program.</p> <code>True</code> <p>Returns:</p> Type Description <code>float | None</code> <p>The total reward as a scalar, or <code>None</code> if no reward results (which is the case when called by <code>Program.test_step</code>).</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def compute_reward(\n    self,\n    x=None,\n    y=None,\n    y_pred=None,\n    training=True,\n):\n    \"\"\"Compute the total reward, validate it, and return it.\n\n    Subclasses can optionally override this method to provide custom reward\n    computation logic.\n\n    Args:\n        x (list): Input data.\n        y (list): Target data.\n        y_pred (list): Predictions returned by the program (output of `program(x)`).\n        training (bool): Whether we are training or evaluating the program.\n\n    Returns:\n        (float | None): The total reward as a scalar, or `None` if no reward results\n            (which is the case when called by `Program.test_step`).\n    \"\"\"\n    # The default implementation does not use `x` or `training`.\n    del x\n    del training\n    rewards = []\n    if self._compile_reward is not None:\n        for y_t, y_p in zip(y, y_pred):\n            reward = await self._compile_reward(y_t, y_p)\n            if reward is not None:\n                rewards.append(reward)\n    for reward in self.rewards:\n        rewards.append(numpy.sum(reward))\n    if len(rewards) == 1:\n        total_reward = rewards[0]\n    elif len(rewards) == 0:\n        total_reward = numpy.zeros(())\n    else:\n        total_reward = numpy.mean(rewards)\n    return float(total_reward)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.evaluate","title":"<code>evaluate(x=None, y=None, batch_size=32, verbose='auto', steps=None, callbacks=None, return_dict=True, **kwargs)</code>  <code>async</code>","text":"<p>Returns the reward value &amp; metrics values for the program in test mode.</p> <p>Computation is done in batches (see the <code>batch_size</code> arg.)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | generator</code> <p>Input data. It can be: - A NumPy array (or array-like), or a list of <code>DataModel</code> arrays     (in case the model has multiple inputs). - A list of dict mapping input names to the corresponding <code>DataModel</code>s,     if the program has named inputs. - A Python generator function yielding <code>(inputs, targets)</code>.</p> <code>None</code> <code>y</code> <code>ndarray</code> <p>Target data. Like the input data <code>x</code>, it can be either NumPy array(s) of <code>DataModel</code>(s). If <code>x</code> is a Python generator function, <code>y</code> should not be specified since targets will be obtained from <code>x</code>.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Integer or <code>None</code>. Number of samples per batch of computation. If unspecified, <code>batch_size</code> will default to 32. Do not specify the <code>batch_size</code> if your input data <code>x</code> is a Python generator function since they generate batches.</p> <code>32</code> <code>verbose</code> <code>int | str</code> <p><code>\"auto\"</code>, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. <code>\"auto\"</code> becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so <code>verbose=2</code> is recommended when not running interactively (e.g. in a production environment). Defaults to <code>\"auto\"</code>.</p> <code>'auto'</code> <code>steps</code> <code>int</code> <p>Integer or <code>None</code>. Total number of steps (batches of samples) to draw before declaring the evaluation round finished. If <code>steps</code> is <code>None</code>, it will run until <code>x</code> is exhausted. In the case of an infinitely repeating dataset, it will run indefinitely.</p> <code>None</code> <code>callbacks</code> <code>list</code> <p>List of <code>synalinks.callbacks.Callback</code> instances. List of callbacks to apply during evaluation.</p> <code>None</code> <code>return_dict</code> <code>bool</code> <p>If <code>True</code>, reward and metric results are returned as a dict, with each key being the name of the metric. If <code>False</code>, they are returned as a list.</p> <code>True</code> <p>Returns:</p> Type Description <code>float | list | dict</code> <p>Scalar test reward (if the program has a single output and no metrics) or list of scalars (if the program has multiple outputs and/or metrics). The attribute <code>program.metrics_names</code> will give you the display labels for the scalar outputs.</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def evaluate(\n    self,\n    x=None,\n    y=None,\n    batch_size=32,\n    verbose=\"auto\",\n    steps=None,\n    callbacks=None,\n    return_dict=True,\n    **kwargs,\n):\n    \"\"\"Returns the reward value &amp; metrics values for the program in test mode.\n\n    Computation is done in batches (see the `batch_size` arg.)\n\n    Args:\n        x (np.ndarray | generator): Input data. It can be:\n            - A NumPy array (or array-like), or a list of `DataModel` arrays\n                (in case the model has multiple inputs).\n            - A list of dict mapping input names to the corresponding `DataModel`s,\n                if the program has named inputs.\n            - A Python generator function yielding `(inputs, targets)`.\n        y (np.ndarray): Target data. Like the input data `x`, it can be either NumPy\n            array(s) of `DataModel`(s). If `x` is a Python generator function,\n            `y` should not be specified since targets will be obtained from\n            `x`.\n        batch_size (int): Integer or `None`.\n            Number of samples per batch of computation.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your input data `x` is a\n            Python generator function since they generate batches.\n        verbose (int | str): `\"auto\"`, 0, 1, or 2. Verbosity mode.\n            0 = silent, 1 = progress bar, 2 = single line.\n            `\"auto\"` becomes 1 for most cases.\n            Note that the progress bar is not\n            particularly useful when logged to a file, so `verbose=2` is\n            recommended when not running interactively\n            (e.g. in a production environment). Defaults to `\"auto\"`.\n        steps (int): Integer or `None`.\n            Total number of steps (batches of samples) to draw before\n            declaring the evaluation round finished. If `steps` is `None`,\n            it will run until `x` is exhausted. In the case of an infinitely\n            repeating dataset, it will run indefinitely.\n        callbacks (list): List of `synalinks.callbacks.Callback` instances.\n            List of callbacks to apply during evaluation.\n        return_dict (bool): If `True`, reward and metric results are returned as a\n            dict, with each key being the name of the metric.\n            If `False`, they are returned as a list.\n\n    Returns:\n        (float | list | dict): Scalar test reward\n            (if the program has a single output and no metrics)\n            or list of scalars (if the program has multiple outputs\n            and/or metrics). The attribute `program.metrics_names` will give you\n            the display labels for the scalar outputs.\n    \"\"\"\n    self._assert_compile_called(\"evaluate\")\n    use_cached_eval_dataset = kwargs.pop(\"_use_cached_eval_dataset\", False)\n    if kwargs:\n        raise ValueError(f\"Arguments not recognized: {kwargs}\")\n    # Create an iterator that yields batches of input/target data.\n    if use_cached_eval_dataset:\n        epoch_iterator = self._eval_epoch_iterator\n    else:\n        epoch_iterator = EpochIterator(\n            x=x,\n            y=y,\n            batch_size=batch_size,\n            steps_per_epoch=steps,\n            shuffle=False,\n            steps_per_execution=self.steps_per_execution,\n        )\n\n    if not all(module.built for module in self._flatten_modules()):\n        # Build the model on one batch of data.\n        for _, data in epoch_iterator:\n            data_batch = data[0]\n            self._auto_build(\n                iterator=epoch_iterator,\n                data_batch=data_batch,\n            )\n            break\n    epoch_iterator.reset()\n\n    # Container that configures and calls callbacks.\n    if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=False,\n            add_progbar=verbose != 0,\n            verbose=verbose,\n            epochs=1,\n            steps=epoch_iterator.num_batches,\n            program=self,\n        )\n\n    self.stop_evaluating = False\n    callbacks.on_test_begin()\n    logs = {}\n    self.reset_metrics()\n    for step, iterator in epoch_iterator:\n        callbacks.on_test_batch_begin(step)\n        data = iterator[0]\n        x_batch, y_batch = data_adapter_utils.unpack_x_y(data)\n        logs = await self.test_on_batch(\n            x=x_batch,\n            y=y_batch,\n            return_dict=True,\n        )\n        callbacks.on_test_batch_end(step, logs)\n        if self.stop_evaluating:\n            break\n    logs = self.get_metrics_result()\n    callbacks.on_test_end(logs)\n\n    if return_dict:\n        return logs\n    return self._flatten_metrics_in_order(logs)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.fit","title":"<code>fit(x=None, y=None, batch_size=1, minibatch_size=4, epochs=1, verbose='auto', callbacks=None, validation_split=0.1, validation_data=None, shuffle=True, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=32, validation_freq=1)</code>  <code>async</code>","text":"<p>Trains the program for a fixed number of epochs (dataset iterations).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | generator</code> <p>Input data. It can be: - A NumPy array (or array-like), or a list of <code>DataModel</code> arrays     (in case the model has multiple inputs). - A list of dict mapping input names to the corresponding <code>DataModel</code>s,     if the program has named inputs. - A Python generator function yielding <code>(inputs, targets)</code>.</p> <code>None</code> <code>y</code> <code>ndarray</code> <p>Target data. Like the input data <code>x</code>, it can be either NumPy array(s) of <code>DataModel</code>(s). If <code>x</code> is a Python generator function, <code>y</code> should not be specified since targets will be obtained from <code>x</code>.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Integer or <code>None</code>. Number of samples per batch of computation. If unspecified, <code>batch_size</code> will default to 32. Do not specify the <code>batch_size</code> if your input data <code>x</code> is a Python generator function since they generate batches.</p> <code>1</code> <code>minibatch_size</code> <code>int</code> <p>Integer or <code>None</code>. Number of randomly selected samples per batch validation. If unspecified, <code>minibatch_size</code> will default to 4. If <code>None</code>, the whole validation set will be used.</p> <code>4</code> <code>epochs</code> <code>int</code> <p>Integer. Number of epochs to train the program. An epoch is an iteration over the entire <code>x</code> and <code>y</code> data provided (unless the <code>steps_per_epoch</code> flag is set to something other than None). Note that in conjunction with <code>initial_epoch</code>, <code>epochs</code> is to be understood as \"final epoch\". The program is not trained for a number of iterations given by <code>epochs</code>, but merely until the epoch of index <code>epochs</code> is reached.</p> <code>1</code> <code>verbose</code> <code>int</code> <p><code>\"auto\"</code>, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. \"auto\" becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so <code>verbose=2</code> is recommended when not running interactively (e.g., in a production environment). Defaults to <code>\"auto\"</code>.</p> <code>'auto'</code> <code>callbacks</code> <code>list</code> <p>List of <code>synalinks.callbacks.Callback</code> instances. List of callbacks to apply during training. See <code>synalinks.callbacks</code>. Note <code>synalinks.callbacks.ProgbarLogger</code> and <code>synalinks.callbacks.History</code> callbacks are created automatically and need not be passed to <code>program.fit()</code>. <code>synalinks.callbacks.ProgbarLogger</code> is created or not based on the <code>verbose</code> argument in <code>program.fit()</code>.</p> <code>None</code> <code>validation_split</code> <code>float</code> <p>Float between 0 and 1. Fraction of the training data to be used as validation data. The program will set apart this fraction of the training data, will not train on it, and will evaluate the reward and any program metrics on this data at the end of each epoch. The validation data is selected from the last samples in the <code>x</code> and <code>y</code> data provided, before shuffling. This argument is only supported when <code>x</code> and <code>y</code> are made of data_models. If both <code>validation_data</code> and <code>validation_split</code> are provided, <code>validation_data</code> will override <code>validation_split</code>.</p> <code>0.1</code> <code>validation_data</code> <code>tuple | iterator</code> <p>Data on which to evaluate the reward and any program metrics at the end of each epoch. The program will not be trained on this data. <code>validation_data</code> will override <code>validation_split</code>. It can be: - A tuple <code>(x_val, y_val)</code> of <code>DataModel</code>s lists.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the training data before each epoch. This argument is ignored when <code>x</code> is a Python generator function.</p> <code>True</code> <code>initial_epoch</code> <code>int</code> <p>Integer. Epoch at which to start training (useful for resuming a previous training run).</p> <code>0</code> <code>steps_per_epoch</code> <code>int</code> <p>Integer or <code>None</code>. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input data_models arrays, the default <code>None</code> means that the value used is the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If <code>x</code> is a Python generator function, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the <code>steps_per_epoch</code> argument, otherwise the training will run indefinitely.</p> <code>None</code> <code>validation_steps</code> <code>int</code> <p>Integer or <code>None</code>. Only relevant if <code>validation_data</code> is provided. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If <code>validation_steps</code> is <code>None</code>, validation will run until the <code>validation_data</code> dataset is exhausted. In the case of an infinitely repeating dataset, it will run indefinitely. If <code>validation_steps</code> is specified and only part of the dataset is consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time.</p> <code>None</code> <code>validation_batch_size</code> <code>int</code> <p>Integer or <code>None</code>. Number of samples per validation batch. If unspecified, will default to <code>batch_size</code>. Do not specify the <code>validation_batch_size</code> if your data is a <code>synalinks.utils.PyDataset</code>, <code>tf.data.Dataset</code>, <code>torch.utils.data.DataLoader</code> or Python generator function since they generate batches.</p> <code>32</code> <code>validation_freq</code> <code>int</code> <p>Only relevant if validation data is provided. Specifies how many training epochs to run before a new validation run is performed, e.g. <code>validation_freq=2</code> runs validation every 2 epochs.</p> <code>1</code> <p>Returns:</p> Type Description <code>History</code> <p>A <code>History</code> object. Its <code>History.history</code> attribute is a record of training reward values and metrics values at successive epochs, as well as validation reward values and validation metrics values (if applicable).</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def fit(\n    self,\n    x=None,\n    y=None,\n    batch_size=1,\n    minibatch_size=4,\n    epochs=1,\n    verbose=\"auto\",\n    callbacks=None,\n    validation_split=0.1,\n    validation_data=None,\n    shuffle=True,\n    initial_epoch=0,\n    steps_per_epoch=None,\n    validation_steps=None,\n    validation_batch_size=32,\n    validation_freq=1,\n):\n    \"\"\"Trains the program for a fixed number of epochs (dataset iterations).\n\n    Args:\n        x (np.ndarray | generator): Input data. It can be:\n            - A NumPy array (or array-like), or a list of `DataModel` arrays\n                (in case the model has multiple inputs).\n            - A list of dict mapping input names to the corresponding `DataModel`s,\n                if the program has named inputs.\n            - A Python generator function yielding `(inputs, targets)`.\n        y (np.ndarray): Target data. Like the input data `x`, it can be either NumPy\n            array(s) of `DataModel`(s). If `x` is a Python generator function,\n            `y` should not be specified since targets will be obtained from\n            `x`.\n        batch_size (int): Integer or `None`.\n            Number of samples per batch of computation.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your input data `x` is a\n            Python generator function since they generate batches.\n        minibatch_size (int): Integer or `None`.\n            Number of randomly selected samples per batch validation.\n            If unspecified, `minibatch_size` will default to 4.\n            If `None`, the whole validation set will be used.\n        epochs (int): Integer. Number of epochs to train the program.\n            An epoch is an iteration over the entire `x` and `y`\n            data provided (unless the `steps_per_epoch` flag is set to\n            something other than None).\n            Note that in conjunction with `initial_epoch`,\n            `epochs` is to be understood as \"final epoch\".\n            The program is not trained for a number of iterations\n            given by `epochs`, but merely until the epoch\n            of index `epochs` is reached.\n        verbose (int): `\"auto\"`, 0, 1, or 2. Verbosity mode.\n            0 = silent, 1 = progress bar, 2 = one line per epoch.\n            \"auto\" becomes 1 for most cases.\n            Note that the progress bar is not\n            particularly useful when logged to a file,\n            so `verbose=2` is recommended when not running interactively\n            (e.g., in a production environment). Defaults to `\"auto\"`.\n        callbacks (list): List of `synalinks.callbacks.Callback` instances.\n            List of callbacks to apply during training.\n            See `synalinks.callbacks`. Note\n            `synalinks.callbacks.ProgbarLogger` and\n            `synalinks.callbacks.History` callbacks are created\n            automatically and need not be passed to `program.fit()`.\n            `synalinks.callbacks.ProgbarLogger` is created\n            or not based on the `verbose` argument in `program.fit()`.\n        validation_split (float): Float between 0 and 1.\n            Fraction of the training data to be used as validation data.\n            The program will set apart this fraction of the training data,\n            will not train on it, and will evaluate the reward and any program\n            metrics on this data at the end of each epoch. The validation\n            data is selected from the last samples in the `x` and `y` data\n            provided, before shuffling.\n            This argument is only supported when `x` and `y` are made of\n            data_models.\n            If both `validation_data` and `validation_split` are provided,\n            `validation_data` will override `validation_split`.\n        validation_data (tuple | iterator): Data on which to evaluate\n            the reward and any program metrics at the end of each epoch.\n            The program will not be trained on this data.\n            `validation_data` will override `validation_split`.\n            It can be:\n            - A tuple `(x_val, y_val)` of `DataModel`s lists.\n        shuffle (bool): Whether to shuffle the training data before each\n            epoch. This argument is ignored when `x` is a Python generator function.\n        initial_epoch (int): Integer.\n            Epoch at which to start training\n            (useful for resuming a previous training run).\n        steps_per_epoch (int): Integer or `None`.\n            Total number of steps (batches of samples) before declaring one\n            epoch finished and starting the next epoch. When training with\n            input data_models arrays, the default `None` means that the\n            value used is the number of samples in your dataset divided by\n            the batch size, or 1 if that cannot be determined.\n            If `x` is a Python generator function, the\n            epoch will run until the input dataset is exhausted. When\n            passing an infinitely repeating dataset, you must specify the\n            `steps_per_epoch` argument, otherwise the training will run\n            indefinitely.\n        validation_steps (int): Integer or `None`.\n            Only relevant if `validation_data` is provided.\n            Total number of steps (batches of samples) to draw before\n            stopping when performing validation at the end of every epoch.\n            If `validation_steps` is `None`, validation will run until the\n            `validation_data` dataset is exhausted. In the case of an\n            infinitely repeating dataset, it will run indefinitely. If\n            `validation_steps` is specified and only part of the dataset\n            is consumed, the evaluation will start from the beginning of the\n            dataset at each epoch. This ensures that the same validation\n            samples are used every time.\n        validation_batch_size (int): Integer or `None`.\n            Number of samples per validation batch.\n            If unspecified, will default to `batch_size`.\n            Do not specify the `validation_batch_size` if your data is a\n            `synalinks.utils.PyDataset`, `tf.data.Dataset`,\n            `torch.utils.data.DataLoader` or Python generator function\n            since they generate batches.\n        validation_freq (int): Only relevant if validation data is provided.\n            Specifies how many training epochs to run\n            before a new validation run is performed,\n            e.g. `validation_freq=2` runs validation every 2 epochs.\n\n    Returns:\n        (History): A `History` object. Its `History.history` attribute is\n            a record of training reward values and metrics values\n            at successive epochs, as well as validation reward values\n            and validation metrics values (if applicable).\n    \"\"\"\n    self._assert_compile_called(\"fit\")\n    self._eval_epoch_iterator = None\n    val_y, val_y = None, None\n\n    if validation_split and validation_data is None:\n        # Create the validation data using the training data. Only supported\n        # for numpy arrays.\n        (x, y), validation_data = array_slicing.train_validation_split(\n            (x, y), validation_split=validation_split\n        )\n\n    if validation_data is not None:\n        val_x, val_y = data_adapter_utils.unpack_x_y(validation_data)\n    # Create an iterator that yields batches of input/target data.\n    epoch_iterator = EpochIterator(\n        x=x,\n        y=y,\n        batch_size=batch_size,\n        steps_per_epoch=steps_per_epoch,\n        shuffle=False,\n        steps_per_execution=self.steps_per_execution,\n    )\n\n    if not all(module.built for module in self._flatten_modules()):\n        # Build the model on one batch of data.\n        for _, data in epoch_iterator:\n            data_batch = data[0]\n            self._auto_build(\n                iterator=epoch_iterator,\n                data_batch=data_batch,\n            )\n            break\n    epoch_iterator.reset()\n\n    # Container that configures and calls callbacks.\n    if not isinstance(callbacks, callbacks_module.CallbackList):\n        # Get optimizer name for logging\n        optimizer_name = None\n        if self._optimizer is not None:\n            optimizer_name = self._optimizer.__class__.__name__\n\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            verbose=verbose,\n            epochs=epochs,\n            steps=steps_per_epoch,\n            batch_size=batch_size,\n            optimizer=optimizer_name,\n            program=self,\n        )\n\n    self.stop_training = False\n    callbacks.on_train_begin()\n    training_logs = None\n    logs = {}\n    initial_epoch = self._initial_epoch or initial_epoch\n\n    if self.trainable_variables and isinstance(\n        self.optimizer, optimizers_module.Optimizer\n    ):\n        await self.optimizer.on_train_begin(\n            self.trainable_variables,\n        )\n\n    for epoch in range(initial_epoch, epochs):\n        self.reset_metrics()\n\n        if self.trainable_variables and isinstance(\n            self.optimizer, optimizers_module.Optimizer\n        ):\n            await self.optimizer.on_epoch_begin(\n                epoch,\n                self.trainable_variables,\n            )\n\n        callbacks.on_epoch_begin(epoch)\n        with epoch_iterator.catch_stop_iteration():\n            for step, iterator in epoch_iterator:\n                data = iterator[0]\n                x_batch, y_batch = data_adapter_utils.unpack_x_y(data)\n\n                if self.trainable_variables and isinstance(\n                    self.optimizer, optimizers_module.Optimizer\n                ):\n                    await self.optimizer.on_batch_begin(\n                        step,\n                        epoch,\n                        self.trainable_variables,\n                    )\n\n                callbacks.on_train_batch_begin(step)\n\n                mini_val_x = None\n                mini_val_y = None\n                if minibatch_size:\n                    if len(val_x) &gt; minibatch_size:\n                        indices = np.random.choice(\n                            len(val_x),\n                            size=minibatch_size,\n                            replace=False,\n                        )\n                        mini_val_x = val_x[indices]\n                        mini_val_y = val_y[indices]\n\n                logs = await self.train_on_batch(\n                    step=step,\n                    x=x_batch,\n                    y=y_batch,\n                    val_x=mini_val_x if mini_val_x is not None else val_x,\n                    val_y=mini_val_y if mini_val_y is not None else val_y,\n                    return_dict=True,\n                )\n\n                val_logs = await self.evaluate(\n                    x=val_x,\n                    y=val_y,\n                    batch_size=validation_batch_size or batch_size,\n                    steps=validation_steps,\n                    callbacks=callbacks,\n                    _use_cached_eval_dataset=False,\n                )\n\n                if self.trainable_variables and isinstance(\n                    self.optimizer, optimizers_module.Optimizer\n                ):\n                    await self.optimizer.on_batch_end(\n                        step,\n                        epoch,\n                        self.trainable_variables,\n                    )\n\n                callbacks.on_train_batch_end(step, logs)\n                if self.stop_training:\n                    break\n\n        # Override with model metrics instead of last step logs if needed.\n        epoch_logs = dict(self._get_metrics_result_or_logs(logs))\n\n        # Run validation.\n        if validation_data is not None and self._should_eval(epoch, validation_freq):\n            # Create EpochIterator for evaluation and cache it.\n            if getattr(self, \"_eval_epoch_iterator\", None) is None:\n                self._eval_epoch_iterator = EpochIterator(\n                    x=val_x,\n                    y=val_y,\n                    batch_size=validation_batch_size or batch_size,\n                    steps_per_execution=self.steps_per_execution,\n                    steps_per_epoch=validation_steps,\n                    shuffle=False,\n                )\n\n            val_logs = await self.evaluate(\n                x=val_x,\n                y=val_y,\n                batch_size=validation_batch_size or batch_size,\n                steps=validation_steps,\n                callbacks=callbacks,\n                _use_cached_eval_dataset=True,\n            )\n            val_logs = {\"val_\" + name: val for name, val in val_logs.items()}\n            epoch_logs.update(val_logs)\n\n        if self.trainable_variables and isinstance(\n            self.optimizer, optimizers_module.Optimizer\n        ):\n            await self.optimizer.on_epoch_end(\n                epoch,\n                self.trainable_variables,\n            )\n\n        callbacks.on_epoch_end(epoch, epoch_logs)\n        training_logs = epoch_logs\n        if self.stop_training:\n            break\n\n    # If _eval_epoch_iterator exists, delete it after all epochs are done.\n    if getattr(self, \"_eval_epoch_iterator\", None) is not None:\n        del self._eval_epoch_iterator\n\n    if self.trainable_variables and isinstance(\n        self.optimizer, optimizers_module.Optimizer\n    ):\n        await self.optimizer.on_train_end(self.trainable_variables)\n\n    callbacks.on_train_end(logs=training_logs)\n    return self.history\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.get_compile_config","title":"<code>get_compile_config()</code>","text":"<p>Returns a serialized config with information for compiling the program.</p> <p>This method returns a config dictionary containing all the information (optimizer, reward, metrics, etc.) with which the program was compiled.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dict containing information for compiling the program.</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>def get_compile_config(self):\n    \"\"\"Returns a serialized config with information for compiling the program.\n\n    This method returns a config dictionary containing all the information\n    (optimizer, reward, metrics, etc.) with which the program was compiled.\n\n    Returns:\n        (dict): A dict containing information for compiling the program.\n    \"\"\"\n    if self.compiled and hasattr(self, \"_compile_config\"):\n        return self._compile_config.serialize()\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.get_metrics_result","title":"<code>get_metrics_result()</code>","text":"<p>Returns the program's metrics values as a dict.</p> <p>If any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A <code>dict</code> containing values of the metrics listed in <code>self.metrics</code>. Example: <code>{'reward': 0.2, 'accuracy': 0.7}</code>.</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>def get_metrics_result(self):\n    \"\"\"Returns the program's metrics values as a dict.\n\n    If any of the metric result is a dict (containing multiple metrics),\n    each of them gets added to the top level returned dict of this method.\n\n    Returns:\n        (dict): A `dict` containing values of the metrics listed in `self.metrics`.\n            Example: `{'reward': 0.2, 'accuracy': 0.7}`.\n    \"\"\"\n    return_metrics = {}\n    for metric in self.metrics:\n        result = metric.result()\n        if isinstance(result, dict):\n            return_metrics.update(result)\n        else:\n            return_metrics[metric.name] = result\n    return python_utils.pythonify_logs(return_metrics)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.predict","title":"<code>predict(x, batch_size=None, verbose='auto', steps=None, callbacks=None)</code>  <code>async</code>","text":"<p>Generates output predictions for the input samples.</p> <p>Computation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.</p> <p>For small numbers of inputs that fit in one batch, directly use <code>__call__()</code> for faster execution, e.g., <code>program(x)</code>, or <code>program(x, training=False)</code> if you have modules that behave differently during inference.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | generator</code> <p>Input data. It can be: - A NumPy array (or array-like), or a list of <code>DataModel</code> arrays     (in case the model has multiple inputs). - A list of dict mapping input names to the corresponding <code>DataModel</code>s,     if the program has named inputs. - A Python generator function yielding <code>(inputs, targets)</code>.</p> required <code>batch_size</code> <code>int</code> <p>Integer or <code>None</code>. Number of samples per batch of computation. If unspecified, <code>batch_size</code> will default to 32. Do not specify the <code>batch_size</code> if your input data <code>x</code> is a <code>synalinks.utils.PyDataset</code>, <code>tf.data.Dataset</code>, <code>torch.utils.data.DataLoader</code> or Python generator function since they generate batches.</p> <code>None</code> <code>verbose</code> <code>int</code> <p><code>\"auto\"</code>, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. <code>\"auto\"</code> becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so <code>verbose=2</code> is recommended when not running interactively (e.g. in a production environment). Defaults to <code>\"auto\"</code>.</p> <code>'auto'</code> <code>steps</code> <code>int</code> <p>Total number of steps (batches of samples) to draw before declaring the prediction round finished. If <code>steps</code> is <code>None</code>, it will run until <code>x</code> is exhausted. In the case of an infinitely repeating dataset, it will run indefinitely.</p> <code>None</code> <code>callbacks</code> <code>list</code> <p>List of <code>synalinks.callbacks.Callback</code> instances. List of callbacks to apply during prediction.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p><code>JsonDataModel</code> array(s) of predictions. If the pipeline failed, a None is added to the predictions.</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def predict(\n    self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n):\n    \"\"\"Generates output predictions for the input samples.\n\n    Computation is done in batches. This method is designed for batch\n    processing of large numbers of inputs. It is not intended for use inside\n    of loops that iterate over your data and process small numbers of inputs\n    at a time.\n\n    For small numbers of inputs that fit in one batch,\n    directly use `__call__()` for faster execution, e.g.,\n    `program(x)`, or `program(x, training=False)` if you have modules\n    that behave differently during inference.\n\n    Args:\n        x (np.ndarray | generator): Input data. It can be:\n            - A NumPy array (or array-like), or a list of `DataModel` arrays\n                (in case the model has multiple inputs).\n            - A list of dict mapping input names to the corresponding `DataModel`s,\n                if the program has named inputs.\n            - A Python generator function yielding `(inputs, targets)`.\n        batch_size (int): Integer or `None`.\n            Number of samples per batch of computation.\n            If unspecified, `batch_size` will default to 32.\n            Do not specify the `batch_size` if your input data `x` is a\n            `synalinks.utils.PyDataset`, `tf.data.Dataset`,\n            `torch.utils.data.DataLoader` or Python generator function\n            since they generate batches.\n        verbose (int): `\"auto\"`, 0, 1, or 2. Verbosity mode.\n            0 = silent, 1 = progress bar, 2 = single line.\n            `\"auto\"` becomes 1 for most cases. Note that the progress bar\n            is not particularly useful when logged to a file,\n            so `verbose=2` is recommended when not running interactively\n            (e.g. in a production environment). Defaults to `\"auto\"`.\n        steps (int): Total number of steps (batches of samples) to draw before\n            declaring the prediction round finished. If `steps` is `None`,\n            it will run until `x` is exhausted. In the case of an infinitely\n            repeating dataset, it will run indefinitely.\n        callbacks (list): List of `synalinks.callbacks.Callback` instances.\n            List of callbacks to apply during prediction.\n\n    Returns:\n        (list): `JsonDataModel` array(s) of predictions.\n            If the pipeline failed, a None is added to the predictions.\n    \"\"\"\n    # Create an iterator that yields batches of input data.\n    epoch_iterator = EpochIterator(\n        x=x,\n        batch_size=batch_size,\n        steps_per_epoch=steps,\n        shuffle=False,\n        steps_per_execution=self.steps_per_execution,\n    )\n\n    # Container that configures and calls callbacks.\n    if not isinstance(callbacks, callbacks_module.CallbackList):\n        callbacks = callbacks_module.CallbackList(\n            callbacks,\n            add_history=True,\n            add_progbar=verbose != 0,\n            verbose=verbose,\n            epochs=1,\n            steps=epoch_iterator.num_batches,\n            model=self,\n        )\n\n    self.stop_predicting = False\n    callbacks.on_test_begin()\n    outputs = []\n    for step, iterator in epoch_iterator:\n        callbacks.on_predict_batch_begin(step)\n        data = iterator[0]\n        x_batch, _ = data_adapter_utils.unpack_x_y(data)\n        batch_outputs = await self.predict_on_batch(x_batch)\n        outputs.extend(batch_outputs)\n        callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n        if self.stop_predicting:\n            break\n    callbacks.on_predict_end()\n    return np.array(outputs, dtype=\"object\")\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.predict_on_batch","title":"<code>predict_on_batch(x, training=False)</code>  <code>async</code>","text":"<p>Returns predictions for a single batch of samples.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data. Must be array-like.</p> required <code>training</code> <code>bool</code> <p>Boolean. True if training.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>list(s) of JsonDataModel predictions.</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def predict_on_batch(self, x, training=False):\n    \"\"\"Returns predictions for a single batch of samples.\n\n    Args:\n        x (np.ndarray): Input data. Must be array-like.\n        training (bool): Boolean. True if training.\n\n    Returns:\n        (list): list(s) of JsonDataModel predictions.\n    \"\"\"\n    tasks = []\n    for inputs in x:\n        tasks.append(self(inputs, training=training))\n    y_pred = await asyncio.gather(*tasks)\n    return y_pred\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.test_on_batch","title":"<code>test_on_batch(x, y=None, return_dict=False)</code>  <code>async</code>","text":"<p>Test the program on a single batch of samples.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data. Must be array-like.</p> required <code>y</code> <code>ndarray</code> <p>Target data. Must be array-like.</p> <code>None</code> <code>return_dict</code> <code>bool</code> <p>If <code>True</code>, reward and metric results are returned as a dict, with each key being the name of the metric. If <code>False</code>, they are returned as a list.</p> <code>False</code> <p>Returns:</p> Type Description <code>float | list | dict</code> <p>A scalar reward value (when no metrics and <code>return_dict=False</code>), a list of reward and metric values (if there are metrics and <code>return_dict=False</code>), or a dict of metric and reward values (if <code>return_dict=True</code>).</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def test_on_batch(\n    self,\n    x,\n    y=None,\n    return_dict=False,\n):\n    \"\"\"Test the program on a single batch of samples.\n\n    Args:\n        x (np.ndarray): Input data. Must be array-like.\n        y (np.ndarray): Target data. Must be array-like.\n        return_dict (bool): If `True`, reward and metric results are returned as a\n            dict, with each key being the name of the metric. If `False`,\n            they are returned as a list.\n\n    Returns:\n        (float | list | dict): A scalar reward value\n            (when no metrics and `return_dict=False`), a list of reward\n            and metric values (if there are metrics and `return_dict=False`),\n            or a dict of metric and reward values (if `return_dict=True`).\n    \"\"\"\n    y_pred = await self.predict_on_batch(x)\n\n    reward = await self.compute_reward(\n        x=x,\n        y=y,\n        y_pred=y_pred,\n        training=False,\n    )\n    await self._reward_tracker.update_state(reward)\n\n    metrics = await self.compute_metrics(x, y, y_pred)\n\n    if return_dict:\n        return metrics\n    return self._flatten_metrics_in_order(metrics)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20training%20API/#synalinks.src.trainers.trainer.Trainer.train_on_batch","title":"<code>train_on_batch(step, x, y=None, val_x=None, val_y=None, return_dict=False)</code>  <code>async</code>","text":"<p>Runs a single optimization step on a single batch of data.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The training step.</p> required <code>x</code> <code>ndarray</code> <p>Input data. Must be array-like.</p> required <code>y</code> <code>ndarray</code> <p>Target data. Must be array-like.</p> <code>None</code> <code>val_x</code> <code>ndarray</code> <p>Input validation data. Must be array-like.</p> <code>None</code> <code>val_y</code> <code>ndarray</code> <p>Target validation data. Must be array-like.</p> <code>None</code> <code>return_dict</code> <code>bool</code> <p>If <code>True</code>, reward and metric results are returned as a dict, with each key being the name of the metric. If <code>False</code>, they are returned as a list.</p> <code>False</code> <p>Returns:</p> Type Description <code>float | list | dict</code> <p>A scalar reward value (when no metrics and <code>return_dict=False</code>), a list of reward and metric values (if there are metrics and <code>return_dict=False</code>), or a dict of metric and reward values (if <code>return_dict=True</code>).</p> Source code in <code>synalinks/src/trainers/trainer.py</code> <pre><code>async def train_on_batch(\n    self,\n    step,\n    x,\n    y=None,\n    val_x=None,\n    val_y=None,\n    return_dict=False,\n):\n    \"\"\"Runs a single optimization step on a single batch of data.\n\n    Args:\n        step (int): The training step.\n        x (np.ndarray): Input data. Must be array-like.\n        y (np.ndarray): Target data. Must be array-like.\n        val_x (np.ndarray): Input validation data. Must be array-like.\n        val_y (np.ndarray): Target validation data. Must be array-like.\n        return_dict (bool): If `True`, reward and metric results are returned as a\n            dict, with each key being the name of the metric. If `False`,\n            they are returned as a list.\n\n    Returns:\n        (float | list | dict): A scalar reward value\n            (when no metrics and `return_dict=False`), a list of reward\n            and metric values (if there are metrics and `return_dict=False`),\n            or a dict of metric and reward values (if `return_dict=True`).\n    \"\"\"\n    if self.trainable_variables and isinstance(\n        self.optimizer, optimizers_module.Optimizer\n    ):\n        metrics = await self.optimizer.optimize(\n            step,\n            self.trainable_variables,\n            x=x,\n            y=y,\n            val_x=val_x,\n            val_y=val_y,\n        )\n    else:\n        warnings.warn(\"The program does not have any trainable variables.\")\n        y_pred = await self.predict_on_batch(val_x)\n        reward = await self.compute_reward(\n            x=val_x,\n            y=val_y,\n            y_pred=y_pred,\n        )\n        await self._reward_tracker.update_state(reward)\n        metrics = await self.compute_metrics(val_x, val_y, y_pred)\n\n    if return_dict:\n        return metrics\n    return self._flatten_metrics_in_order(metrics)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/","title":"The Program class","text":""},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#the-program-class","title":"The Program class","text":"<p>               Bases: <code>Trainer</code>, <code>Module</code></p> <p>A program grouping modules into an object with training/inference features.</p> <p>There is four ways to instantiate a <code>Program</code>:</p>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program--with-the-functional-api","title":"With the \"Functional API\"","text":"<p>You start from <code>Input</code>, you chain modules calls to specify the program's structure, and finally, you create your program from inputs and outputs:</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking process\",\n    )\n    answer: float = synalinks.Field(\n        description=\"The correct numerical answer\",\n    )\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    x0 = synalinks.Input(data_model=Query)\n    x1 = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=language_model,\n    )(x0)\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=x1,\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Note: Only dicts, lists, and tuples of input data models are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).</p>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program--by-subclassing-the-program-class","title":"By subclassing the <code>Program</code> class","text":"<p>In that case, you should define your modules in <code>__init__()</code> and you should implement the program's structure in <code>call()</code> .</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking process\",\n    )\n    answer: float = synalinks.Field(\n        description=\"The correct numerical answer\",\n    )\n\nclass ChainOfThought(synalinks.Program):\n    \"\"\"Useful to answer in a step by step manner.\n\n    The first line of the docstring is provided as description\n    for the program if not provided in the `super().__init__()`.\n    In a similar way the name is automatically infered based on\n    the class name if not provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model=None,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.answer = synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=language_model,\n            name=\"generator_\"+self.name,\n        )\n\n    async def call(self, inputs, training=False):\n        if not inputs:\n            return None\n        x = await self.answer(inputs, training=training)\n        return x\n\n    def get_config(self):\n        config = {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config =             {\n            \"language_model\": synalinks.saving.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n        return {**config, **language_model_config}\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = synalinks.saving.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(language_model=language_model, **config)\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    program = ChainOfThought(\n        language_model=language_model,\n    )\n</code></pre> <p>If you subclass <code>Program</code>, you can optionally have a <code>training</code> argument (boolean) in <code>call()</code>, which you can use to specify a different behavior in training and inference.</p> <p>Once the program is created, you can config the program with rewards and metrics with <code>program.compile()</code>, train the program with <code>program.fit()</code>, or use the program to do prediction with <code>program.predict()</code> or <code>program()</code>.</p> <p>To understand the difference between <code>program.predict()</code> or <code>program()</code>, read the FAQ.</p>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program--mixing-the-subclassing-and-the-functional-api","title":"Mixing the subclassing and the <code>Functional</code> API","text":"<p>This way of programming is recommended to encapsulate your application while  providing an easy to use setup. It is the recommended way for most users as  it avoid making your program/agents from scratch. In that case, you should implement only the <code>__init__()</code> and <code>build()</code> methods.</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking process\",\n    )\n    answer: float = synalinks.Field(\n        description=\"The correct numerical answer\",\n    )\n\nasync def main():\n\n    class ChainOfThought(synalinks.Program):\n        \"\"\"Useful to answer in a step by step manner.\"\"\"\n\n        def __init__(\n            self,\n            language_model=None,\n            name=None,\n            description=None,\n            trainable=True,\n        ):\n            super().__init__(\n                name=name,\n                description=description,\n                trainable=trainable,\n            )\n\n            self.language_model = language_model\n\n        async def build(self, inputs):\n            outputs = await synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=self.language_model,\n            )(inputs)\n\n            # Create your program using the functional API\n            super().__init__(\n                inputs=inputs,\n                outputs=outputs,\n                name=self.name,\n                description=self.description,\n                trainable=self.trainable,\n            )\n\n    language_model = synalinks.LanguageModel(\n        model=\"ollama/mistral\",\n    )\n\n    program = ChainOfThought(\n        language_model=language_model,\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This allows you to not have to implement the <code>call()</code> and serialization methods (<code>get_config()</code> and <code>from_config()</code>). The program will be built for any inputs  the first time called.</p>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program--with-the-sequential-class","title":"With the <code>Sequential</code> class","text":"<p>In addition, <code>synalinks.Sequential</code> is a special case of program where the program is purely a stack of single-input, single-output modules.</p> <pre><code>import synalinks\nimport asyncio\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(\n        description=\"The user query\",\n    )\n\nclass AnswerWithThinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(\n        description=\"Your step by step thinking process\",\n    )\n    answer: float = synalinks.Field(\n        description=\"The correct numerical answer\",\n    )\n\nasync def main():\n\n    language_model = synalinks.LanguageModel(model=\"ollama/mistral\")\n\n    program = synalinks.Sequential(\n        [\n            synalinks.Input(\n                data_model=Query,\n            ),\n            synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=language_model,\n            ),\n        ],\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\",\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>@synalinks_export([\"synalinks.Program\", \"synalinks.programs.Program\"])\nclass Program(Trainer, Module):\n    \"\"\"A program grouping modules into an object with training/inference features.\n\n    There is four ways to instantiate a `Program`:\n\n    ## With the \"Functional API\"\n\n    You start from `Input`, you chain modules calls to specify the program's structure,\n    and finally, you create your program from inputs and outputs:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        x0 = synalinks.Input(data_model=Query)\n        x1 = await synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=language_model,\n        )(x0)\n\n        program = synalinks.Program(\n            inputs=x0,\n            outputs=x1,\n            name=\"chain_of_thought\",\n            description=\"Useful to answer in a step by step manner.\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    Note: Only dicts, lists, and tuples of input data models are supported. Nested\n    inputs are not supported (e.g. lists of list or dicts of dict).\n\n    ## By subclassing the `Program` class\n\n    In that case, you should define your\n    modules in `__init__()` and you should implement the program's structure\n    in `call()` .\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    class ChainOfThought(synalinks.Program):\n        \\\"\\\"\\\"Useful to answer in a step by step manner.\n\n        The first line of the docstring is provided as description\n        for the program if not provided in the `super().__init__()`.\n        In a similar way the name is automatically infered based on\n        the class name if not provided.\n        \\\"\\\"\\\"\n\n        def __init__(\n            self,\n            language_model=None,\n            name=None,\n            description=None,\n            trainable=True,\n        ):\n            super().__init__(\n                name=name,\n                description=description,\n                trainable=trainable,\n            )\n            self.answer = synalinks.Generator(\n                data_model=AnswerWithThinking,\n                language_model=language_model,\n                name=\"generator_\"+self.name,\n            )\n\n        async def call(self, inputs, training=False):\n            if not inputs:\n                return None\n            x = await self.answer(inputs, training=training)\n            return x\n\n        def get_config(self):\n            config = {\n                \"name\": self.name,\n                \"description\": self.description,\n                \"trainable\": self.trainable,\n            }\n            language_model_config = \\\n            {\n                \"language_model\": synalinks.saving.serialize_synalinks_object(\n                    self.language_model\n                )\n            }\n            return {**config, **language_model_config}\n\n        @classmethod\n        def from_config(cls, config):\n            language_model = synalinks.saving.deserialize_synalinks_object(\n                config.pop(\"language_model\")\n            )\n            return cls(language_model=language_model, **config)\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        program = ChainOfThought(\n            language_model=language_model,\n        )\n    ```\n\n    If you subclass `Program`, you can optionally have\n    a `training` argument (boolean) in `call()`, which you can use to specify\n    a different behavior in training and inference.\n\n    Once the program is created, you can config the program with rewards and metrics\n    with `program.compile()`, train the program with `program.fit()`, or use the program\n    to do prediction with `program.predict()` or `program()`.\n\n    To understand the difference between `program.predict()` or `program()`, read the\n    [FAQ](https://synalinks.github.io/synalinks/FAQ/#whats-the-difference-between-program-methods-predict-and-__call__).\n\n    ## Mixing the subclassing and the `Functional` API\n\n    This way of programming is recommended to encapsulate your application while \n    providing an easy to use setup. It is the recommended way for most users as \n    it avoid making your program/agents from scratch.\n    In that case, you should implement only the `__init__()` and `build()` methods.\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    async def main():\n\n        class ChainOfThought(synalinks.Program):\n            \\\"\\\"\\\"Useful to answer in a step by step manner.\\\"\\\"\\\"\n\n            def __init__(\n                self,\n                language_model=None,\n                name=None,\n                description=None,\n                trainable=True,\n            ):\n                super().__init__(\n                    name=name,\n                    description=description,\n                    trainable=trainable,\n                )\n\n                self.language_model = language_model\n\n            async def build(self, inputs):\n                outputs = await synalinks.Generator(\n                    data_model=AnswerWithThinking,\n                    language_model=self.language_model,\n                )(inputs)\n\n                # Create your program using the functional API\n                super().__init__(\n                    inputs=inputs,\n                    outputs=outputs,\n                    name=self.name,\n                    description=self.description,\n                    trainable=self.trainable,\n                )\n\n        language_model = synalinks.LanguageModel(\n            model=\"ollama/mistral\",\n        )\n\n        program = ChainOfThought(\n            language_model=language_model,\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n\n    This allows you to not have to implement the `call()` and serialization methods\n    (`get_config()` and `from_config()`). The program will be built for any inputs \n    the first time called.\n\n    ## With the `Sequential` class\n\n    In addition, `synalinks.Sequential` is a special case of program where\n    the program is purely a stack of single-input, single-output modules.\n\n    ```python\n    import synalinks\n    import asyncio\n\n    class Query(synalinks.DataModel):\n        query: str = synalinks.Field(\n            description=\"The user query\",\n        )\n\n    class AnswerWithThinking(synalinks.DataModel):\n        thinking: str = synalinks.Field(\n            description=\"Your step by step thinking process\",\n        )\n        answer: float = synalinks.Field(\n            description=\"The correct numerical answer\",\n        )\n\n    async def main():\n\n        language_model = synalinks.LanguageModel(model=\"ollama/mistral\")\n\n        program = synalinks.Sequential(\n            [\n                synalinks.Input(\n                    data_model=Query,\n                ),\n                synalinks.Generator(\n                    data_model=AnswerWithThinking,\n                    language_model=language_model,\n                ),\n            ],\n            name=\"chain_of_thought\",\n            description=\"Useful to answer in a step by step manner.\",\n        )\n\n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    ```\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        # Signature detection for usage of `Program` as a `Functional`\n        if functional_init_arguments(args, kwargs) and cls == Program:\n            from synalinks.src.programs.functional import Functional\n\n            return Functional.__new__(Functional, *args, **kwargs)\n        return typing.cast(cls, super().__new__(cls))\n\n    def __init__(self, *args, **kwargs):\n        Trainer.__init__(self)\n        from synalinks.src.programs import functional\n\n        # Signature detection for usage of a `Program` subclass\n        # as a `Functional` subclass\n        if functional_init_arguments(args, kwargs):\n            inject_functional_program_class(self.__class__)\n            functional.Functional.__init__(self, *args, **kwargs)\n        else:\n            Module.__init__(self, *args, **kwargs)\n\n    async def call(self, *args, **kwargs):\n        raise NotImplementedError(\n            f\"Program {self.__class__.__name__} does not have a `call()` \"\n            \"method implemented.\"\n        )\n\n    @property\n    def modules(self):\n        return list(self._flatten_modules(include_self=False, recursive=False))\n\n    @modules.setter\n    def modules(self, _):\n        raise AttributeError(\n            \"`Program.modules` attribute is reserved and should not be used. \"\n            \"Please use another name.\"\n        )\n\n    def get_module(self, name=None, index=None):\n        \"\"\"Retrieves a module based on either its name (unique) or index.\n\n        If `name` and `index` are both provided, `index` will take precedence.\n        Indices are based on order of horizontal graph traversal (bottom-up).\n\n        Args:\n            name (str): String, name of module.\n            index (int): Integer, index of module.\n\n        Returns:\n            (Module): A module instance.\n        \"\"\"\n        if index is not None and name is not None:\n            raise ValueError(\n                \"Provide only a module name or a module index. Received: \"\n                f\"index={index}, name={name}.\"\n            )\n        if index is not None:\n            if len(self.modules) &lt;= index:\n                raise ValueError(\n                    f\"Was asked to retrieve module at index {index}\"\n                    f\" but program only has {len(self.modules)}\"\n                    \" modules.\"\n                )\n            else:\n                return self.modules[index]\n\n        if name is not None:\n            for module in self.modules:\n                if module.name == name:\n                    return module\n            raise ValueError(\n                f\"No such module: {name}. Existing modules are: \"\n                f\"{list(module.name for module in self.modules)}.\"\n            )\n        raise ValueError(\"Provide either a module name or module index at `get_module`.\")\n\n    def summary(\n        self,\n        line_length=None,\n        positions=None,\n        print_fn=None,\n        expand_nested=False,\n        show_trainable=False,\n        module_range=None,\n    ):\n        \"\"\"Prints a string summary of the program.\n\n        Args:\n            line_length (int): Total length of printed lines\n                (e.g. set this to adapt the display to different\n                terminal window sizes).\n            positions (list): Relative or absolute positions of log elements\n                in each line. If not provided, becomes\n                `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.\n            print_fn (Callable): Print function to use. By default, prints to `stdout`.\n                If `stdout` doesn't work in your environment, change to `print`.\n                It will be called on each line of the summary.\n                You can set it to a custom function\n                in order to capture the string summary.\n            expand_nested (bool): Whether to expand the nested models.\n                Defaults to `False`.\n            show_trainable (bool): Whether to show if a module is trainable.\n                Defaults to `False`.\n            module_range (list | tuple): a list or tuple of 2 strings,\n                which is the starting module name and ending module name\n                (both inclusive) indicating the range of modules to be printed\n                in summary. It also accepts regex patterns instead of exact\n                names. In this case, the start predicate will be\n                the first element that matches `module_range[0]`\n                and the end predicate will be the last element\n                that matches `module_range[1]`.\n                By default `None` considers all modules of the model.\n\n        Raises:\n            ValueError: if `summary()` is called before the model is built.\n        \"\"\"\n        summary_utils.print_summary(\n            self,\n            line_length=line_length,\n            positions=positions,\n            print_fn=print_fn,\n            expand_nested=expand_nested,\n            show_trainable=show_trainable,\n            module_range=module_range,\n        )\n\n    def save(self, filepath, overwrite=True, **kwargs):\n        \"\"\"Saves a program as a `.json` file.\n\n        Example:\n\n        ```python\n        import synalinks\n\n        class Query(synalinks.DataModel):\n            query: str\n\n        class AnswerWithRationale(synalinks.DataModel):\n            rationale: str\n            answer: str\n\n        language_model = LanguageModel(\"ollama/mistral\")\n\n        program = synalinks.Sequential(\n            [\n                synalinks.Input(data_model=Query),\n                synalinks.Generator(\n                    data_model=AnswerWithRationale,\n                    language_model=language_model,\n                ),\n            ],\n        )\n\n        program.save(\"program.json\")\n        loaded_program = synalinks.programs.program_from_json(\"program.json\")\n        ```\n\n        The saved `.json` file contains:\n\n        - The program's configuration (architecture)\n        - The program's variables\n        - The program's optimizer's state (if any)\n        - The program's reward's state (if any)\n\n        Thus programs can be reinstantiated in the exact same state.\n\n        Args:\n            filepath (str | os.PathLike): `str` or `os.PathLike` object.\n                The path where to save the model. Must end in `.json`.\n            overwrite (bool): Whether we should overwrite any existing program at\n                the target location, or instead ask the user via\n                an interactive prompt. Default to `True`.\n        \"\"\"\n        from synalinks.src.saving import serialization_lib\n\n        filepath = file_utils.path_to_string(filepath)\n        if not filepath.endswith(\".json\"):\n            raise ValueError(\n                f\"The filepath should ends with '.json', received filepath={filepath}\"\n            )\n        program_config = serialization_lib.serialize_synalinks_object(self)\n        variables_config = self.get_state_tree()\n        program_config.update({\"variables\": variables_config})\n        program_config_string = orjson.dumps(\n            program_config, option=orjson.OPT_INDENT_2\n        ).decode()\n        if file_utils.exists(filepath) and not overwrite:\n            io_utils.ask_to_proceed_with_overwrite(filepath)\n        with open(filepath, \"w\") as f:\n            f.write(program_config_string)\n\n    async def build_from_config(self, config):\n        if not config:\n            return\n        status = False\n        if \"input_schema\" in config:\n            # Case: all inputs are in the first arg (possibly nested).\n            if utils.is_default(self.build):\n                status = self._build_by_run_for_single_pos_arg(config[\"input_schema\"])\n            else:\n                try:\n                    await self.build(config[\"input_schema\"])\n                    status = True\n                except:\n                    pass\n            self._build_schemas_dict = config\n\n        elif \"schemas_dict\" in config:\n            # Case: inputs were recorded as multiple keyword arguments.\n            if utils.is_default(self.build):\n                status = self._build_by_run_for_kwargs(config[\"schemas_dict\"])\n            else:\n                try:\n                    await self.build(**config[\"schemas_dict\"])\n                    status = True\n                except:\n                    pass\n            self._build_schemas_dict = config[\"schemas_dict\"]\n\n        if not status:\n            warnings.warn(\n                f\"Program '{self.name}' had a build config, but the program \"\n                \"cannot be built automatically in \"\n                \"`build_from_config(config)`. \"\n                \"You should implement \"\n                \"`def build_from_config(self, config)`, \"\n                \"and you might also want to implement the method \"\n                \" that generates the config at saving time, \"\n                \"`def get_build_config(self)`. \"\n                \"The method `build_from_config()` is meant to \"\n                \"create the state of the model (i.e. its variables) \"\n                \"upon deserialization.\",\n                stacklevel=2,\n            )\n\n    def to_json(self, **kwargs):\n        \"\"\"Returns a JSON string containing the network configuration.\n\n        ```python\n        json_string = program.to_json()\n        ```\n\n        To load a network from a JSON save file, use\n        `synalinks.programs.program_from_json(json_string, custom_objects={...})`.\n\n        Args:\n            **kwargs (keyword arguments): Additional keyword arguments to be passed to\n                `orjson.dumps()`.\n\n        Returns:\n            (str): A JSON string.\n        \"\"\"\n        from synalinks.src.saving import serialization_lib\n\n        program_config = serialization_lib.serialize_synalinks_object(self)\n        return orjson.dumps(program_config, **kwargs).decode()\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        from synalinks.src.programs.functional import Functional\n\n        functional_config_keys = [\n            \"name\",\n            \"modules\",\n            \"input_modules\",\n            \"output_modules\",\n        ]\n        is_functional_config = all(key in config for key in functional_config_keys)\n        argspec = inspect.getfullargspec(cls.__init__)\n        functional_init_args = inspect.getfullargspec(Functional.__init__).args[1:]\n        revivable_as_functional = (\n            cls in {Functional, Program}\n            or argspec.args[1:] == functional_init_args\n            or (argspec.varargs == \"args\" and argspec.varkw == \"kwargs\")\n        )\n        if is_functional_config and revivable_as_functional:\n            # Revive Functional model\n            # (but not Functional subclasses with a custom __init__)\n            from synalinks.src.programs.functional import functional_from_config\n\n            return functional_from_config(cls, config, custom_objects=custom_objects)\n\n        # Either the model has a custom __init__, or the config\n        # does not contain all the information necessary to\n        # revive a Functional model. This happens when the user creates\n        # subclassed models where `get_config()` is returning\n        # insufficient information to be considered a Functional model.\n        # In this case, we fall back to provide all config into the\n        # constructor of the class.\n        try:\n            return cls(**config)\n        except TypeError as e:\n            raise TypeError(\n                \"Unable to revive program from config. When overriding \"\n                \"the `get_config()` method, make sure that the \"\n                \"returned config contains all items used as arguments \"\n                f\"in the  constructor to {cls}, \"\n                \"which is the default behavior. \"\n                \"You can override this default behavior by defining a \"\n                \"`from_config(cls, config)` class method to specify \"\n                \"how to create an \"\n                f\"instance of {cls.__name__} from its config.\\n\\n\"\n                f\"Received config={config}\\n\\n\"\n                f\"Error encountered during deserialization: {e}\"\n            )\n\n    def get_state_tree(self):\n        \"\"\"Retrieves tree-like structure of program variables.\n\n        This method allows retrieval of different program variables (trainable,\n        non-trainable, optimizer, and metrics). The variables are returned in a\n        nested dictionary format, where the keys correspond to the variable\n        names and the values are the nested representations of the variables.\n\n        Example:\n\n        ```python\n        program.compile(\n            optimizer=synalinks.optimizers.RandomFewShot(),\n            reward=synalinks.rewards.ExactMatch(),\n        )\n        program.fit(x=x_train, y=y_train)\n        state_tree = program.get_state_tree()\n        ```\n\n        Returns:\n            (dict): A dictionary containing the nested representations of the\n                requested variables. The keys are the variable names, and the\n                values are the corresponding nested dictionaries.\n        \"\"\"\n        variables = {}\n        variables[\"trainable_variables\"] = self._create_nested_dict(\n            self.trainable_variables\n        )\n        variables[\"non_trainable_variables\"] = self._create_nested_dict(\n            self.non_trainable_variables\n        )\n        if self.optimizer:\n            variables[\"optimizer_trainable_variables\"] = self._create_nested_dict(\n                self.optimizer.trainable_variables\n            )\n            variables[\"optimizer_non_trainable_variables\"] = self._create_nested_dict(\n                self.optimizer.non_trainable_variables\n            )\n        variables[\"metrics_variables\"] = self._create_nested_dict(self.metrics_variables)\n        return variables\n\n    def _create_nested_dict(self, variables):\n        flat_dict = {}\n        for v in variables:\n            if v.path in flat_dict:\n                raise ValueError(\n                    \"The following variable path is found twice in the program: \"\n                    f\"'{v.path}'. `get_state_tree()` can only be called when \"\n                    \"all variable paths are unique. Make sure to give unique \"\n                    \"names to your modules (and other objects).\"\n                )\n            flat_dict[v.path] = v.get_json()\n\n        nested_dict = {}\n        for path, value in flat_dict.items():\n            parts = path.split(\"/\")\n            current_dict = nested_dict\n            for part in parts[:-1]:\n                if part not in current_dict:\n                    current_dict[part] = {}\n                current_dict = current_dict[part]\n            current_dict[parts[-1]] = value\n\n        return nested_dict\n\n    def set_state_tree(self, state_tree):\n        \"\"\"Assigns values to variables of the program.\n\n        This method takes a dictionary of nested variable values, which\n        represents the state tree of the program, and assigns them to the\n        corresponding variables of the program. The dictionary keys represent the\n        variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),\n        and the values are nested dictionaries containing the variable\n        paths and their corresponding values.\n\n        Args:\n            state_tree (dict): A dictionary representing the state tree of the program.\n                The keys are the variable names, and the values are nested\n                dictionaries representing the variable paths and their values.\n        \"\"\"\n        for k, v in state_tree.items():\n            path_value_dict = self._flatten_nested_dict(v)\n            if k == \"trainable_variables\":\n                self._assign_variable_values(self.trainable_variables, path_value_dict)\n            elif k == \"non_trainable_variables\":\n                self._assign_variable_values(\n                    self.non_trainable_variables, path_value_dict\n                )\n            elif k == \"optimizer_trainable_variables\":\n                if self.optimizer:\n                    self._assign_variable_values(\n                        self.optimizer.trainable_variables, path_value_dict\n                    )\n            elif k == \"optimizer_non_trainable_variables\":\n                if self.optimizer:\n                    self._assign_variable_values(\n                        self.optimizer.non_trainable_variables, path_value_dict\n                    )\n            elif k == \"metrics_variables\":\n                self._assign_variable_values(self.metrics_variables, path_value_dict)\n            else:\n                raise ValueError(f\"Unknown variable name: {k}\")\n\n    def _assign_variable_values(self, variables, path_value_dict):\n        for full_path, value in path_value_dict.items():\n            path = \"/\".join(full_path.split(\"/\")[:-1])\n            field_name = full_path.split(\"/\")[-1]\n            for variable in variables:\n                if variable.path == path:\n                    variable.get_json()[field_name] = value\n\n    def _flatten_nested_dict(self, nested_dict):\n        flat_dict = {}\n\n        def _flatten(current_dict, prefix=\"\"):\n            for key, value in current_dict.items():\n                if isinstance(value, dict):\n                    _flatten(value, prefix + key + \"/\")\n                else:\n                    flat_dict[prefix + key] = value\n\n        _flatten(nested_dict)\n        return flat_dict\n\n    def save_variables(self, filepath, overwrite=True):\n        \"\"\"Saves all module variables to a `.variables.json` file.\n\n        Args:\n            filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n                Path where to save the program. Must end in `.variables.json`.\n            overwrite (bool): Whether we should overwrite any existing program\n                at the target location, or instead ask the user\n                via an interactive prompt.\n        \"\"\"\n        filepath = file_utils.path_to_string(filepath)\n        if not filepath.endswith(\".variables.json\"):\n            raise ValueError(\n                \"The filepath should ends with '.variables.json', \"\n                f\"received filepath={filepath}\"\n            )\n        config = self.get_state_tree()\n        config_string = orjson.dumps(config, option=orjson.OPT_INDENT_2).decode()\n        if file_utils.exists(filepath) and not overwrite:\n            io_utils.ask_to_proceed_with_overwrite(filepath)\n        with open(filepath, \"w\") as f:\n            f.write(config_string)\n\n    def load_variables(self, filepath):\n        \"\"\"Load all module variables from a `.variable.json` file.\n\n        Args:\n            filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n                Path to load the program's variables from.\n                Must end in `.variables.json`.\n        \"\"\"\n        filepath = file_utils.path_to_string(filepath)\n        if not filepath.endswith(\".variables.json\"):\n            raise ValueError(\n                \"The filepath should ends with '.variables.json', \"\n                f\"received filepath={filepath}\"\n            )\n        with open(filepath, \"rb\") as f:\n            state_tree_config = orjson.loads(f.read())\n        self.set_state_tree(state_tree_config)\n\n    @classmethod\n    def load(cls, filepath, custom_objects=None):\n        \"\"\"Load a program from a JSON file.\n\n        Example:\n\n        ```python\n        import synalinks\n\n        loaded_program = synalinks.Program.load(\"program.json\")\n        ```\n\n        Args:\n            filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n                Path to load the program's variables from.\n                Must end in `.variables.json`.\n            custom_objects (dict): Optional dictionary mapping names\n                (strings) to custom classes or functions to be\n                considered during deserialization.\n\n        Returns:\n            (Program): A Synalinks program instance (uncompiled).\n        \"\"\"\n        filepath = file_utils.path_to_string(filepath)\n        if not filepath.endswith(\".json\"):\n            raise ValueError(\n                f\"The filepath should ends with '.json', received filepath={filepath}\"\n            )\n        with open(filepath, \"r\") as f:\n            json_config = f.read()\n        return program_from_json(json_config, custom_objects=custom_objects)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.get_module","title":"<code>get_module(name=None, index=None)</code>","text":"<p>Retrieves a module based on either its name (unique) or index.</p> <p>If <code>name</code> and <code>index</code> are both provided, <code>index</code> will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>String, name of module.</p> <code>None</code> <code>index</code> <code>int</code> <p>Integer, index of module.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>A module instance.</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def get_module(self, name=None, index=None):\n    \"\"\"Retrieves a module based on either its name (unique) or index.\n\n    If `name` and `index` are both provided, `index` will take precedence.\n    Indices are based on order of horizontal graph traversal (bottom-up).\n\n    Args:\n        name (str): String, name of module.\n        index (int): Integer, index of module.\n\n    Returns:\n        (Module): A module instance.\n    \"\"\"\n    if index is not None and name is not None:\n        raise ValueError(\n            \"Provide only a module name or a module index. Received: \"\n            f\"index={index}, name={name}.\"\n        )\n    if index is not None:\n        if len(self.modules) &lt;= index:\n            raise ValueError(\n                f\"Was asked to retrieve module at index {index}\"\n                f\" but program only has {len(self.modules)}\"\n                \" modules.\"\n            )\n        else:\n            return self.modules[index]\n\n    if name is not None:\n        for module in self.modules:\n            if module.name == name:\n                return module\n        raise ValueError(\n            f\"No such module: {name}. Existing modules are: \"\n            f\"{list(module.name for module in self.modules)}.\"\n        )\n    raise ValueError(\"Provide either a module name or module index at `get_module`.\")\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.get_state_tree","title":"<code>get_state_tree()</code>","text":"<p>Retrieves tree-like structure of program variables.</p> <p>This method allows retrieval of different program variables (trainable, non-trainable, optimizer, and metrics). The variables are returned in a nested dictionary format, where the keys correspond to the variable names and the values are the nested representations of the variables.</p> <p>Example:</p> <pre><code>program.compile(\n    optimizer=synalinks.optimizers.RandomFewShot(),\n    reward=synalinks.rewards.ExactMatch(),\n)\nprogram.fit(x=x_train, y=y_train)\nstate_tree = program.get_state_tree()\n</code></pre> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the nested representations of the requested variables. The keys are the variable names, and the values are the corresponding nested dictionaries.</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def get_state_tree(self):\n    \"\"\"Retrieves tree-like structure of program variables.\n\n    This method allows retrieval of different program variables (trainable,\n    non-trainable, optimizer, and metrics). The variables are returned in a\n    nested dictionary format, where the keys correspond to the variable\n    names and the values are the nested representations of the variables.\n\n    Example:\n\n    ```python\n    program.compile(\n        optimizer=synalinks.optimizers.RandomFewShot(),\n        reward=synalinks.rewards.ExactMatch(),\n    )\n    program.fit(x=x_train, y=y_train)\n    state_tree = program.get_state_tree()\n    ```\n\n    Returns:\n        (dict): A dictionary containing the nested representations of the\n            requested variables. The keys are the variable names, and the\n            values are the corresponding nested dictionaries.\n    \"\"\"\n    variables = {}\n    variables[\"trainable_variables\"] = self._create_nested_dict(\n        self.trainable_variables\n    )\n    variables[\"non_trainable_variables\"] = self._create_nested_dict(\n        self.non_trainable_variables\n    )\n    if self.optimizer:\n        variables[\"optimizer_trainable_variables\"] = self._create_nested_dict(\n            self.optimizer.trainable_variables\n        )\n        variables[\"optimizer_non_trainable_variables\"] = self._create_nested_dict(\n            self.optimizer.non_trainable_variables\n        )\n    variables[\"metrics_variables\"] = self._create_nested_dict(self.metrics_variables)\n    return variables\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.load","title":"<code>load(filepath, custom_objects=None)</code>  <code>classmethod</code>","text":"<p>Load a program from a JSON file.</p> <p>Example:</p> <pre><code>import synalinks\n\nloaded_program = synalinks.Program.load(\"program.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p><code>str</code> or <code>pathlib.Path</code> object. Path to load the program's variables from. Must end in <code>.variables.json</code>.</p> required <code>custom_objects</code> <code>dict</code> <p>Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Program</code> <p>A Synalinks program instance (uncompiled).</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>@classmethod\ndef load(cls, filepath, custom_objects=None):\n    \"\"\"Load a program from a JSON file.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    loaded_program = synalinks.Program.load(\"program.json\")\n    ```\n\n    Args:\n        filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n            Path to load the program's variables from.\n            Must end in `.variables.json`.\n        custom_objects (dict): Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    Returns:\n        (Program): A Synalinks program instance (uncompiled).\n    \"\"\"\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".json\"):\n        raise ValueError(\n            f\"The filepath should ends with '.json', received filepath={filepath}\"\n        )\n    with open(filepath, \"r\") as f:\n        json_config = f.read()\n    return program_from_json(json_config, custom_objects=custom_objects)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.load_variables","title":"<code>load_variables(filepath)</code>","text":"<p>Load all module variables from a <code>.variable.json</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p><code>str</code> or <code>pathlib.Path</code> object. Path to load the program's variables from. Must end in <code>.variables.json</code>.</p> required Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def load_variables(self, filepath):\n    \"\"\"Load all module variables from a `.variable.json` file.\n\n    Args:\n        filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n            Path to load the program's variables from.\n            Must end in `.variables.json`.\n    \"\"\"\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".variables.json\"):\n        raise ValueError(\n            \"The filepath should ends with '.variables.json', \"\n            f\"received filepath={filepath}\"\n        )\n    with open(filepath, \"rb\") as f:\n        state_tree_config = orjson.loads(f.read())\n    self.set_state_tree(state_tree_config)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.save","title":"<code>save(filepath, overwrite=True, **kwargs)</code>","text":"<p>Saves a program as a <code>.json</code> file.</p> <p>Example:</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str\n\nclass AnswerWithRationale(synalinks.DataModel):\n    rationale: str\n    answer: str\n\nlanguage_model = LanguageModel(\"ollama/mistral\")\n\nprogram = synalinks.Sequential(\n    [\n        synalinks.Input(data_model=Query),\n        synalinks.Generator(\n            data_model=AnswerWithRationale,\n            language_model=language_model,\n        ),\n    ],\n)\n\nprogram.save(\"program.json\")\nloaded_program = synalinks.programs.program_from_json(\"program.json\")\n</code></pre> <p>The saved <code>.json</code> file contains:</p> <ul> <li>The program's configuration (architecture)</li> <li>The program's variables</li> <li>The program's optimizer's state (if any)</li> <li>The program's reward's state (if any)</li> </ul> <p>Thus programs can be reinstantiated in the exact same state.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | PathLike</code> <p><code>str</code> or <code>os.PathLike</code> object. The path where to save the model. Must end in <code>.json</code>.</p> required <code>overwrite</code> <code>bool</code> <p>Whether we should overwrite any existing program at the target location, or instead ask the user via an interactive prompt. Default to <code>True</code>.</p> <code>True</code> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def save(self, filepath, overwrite=True, **kwargs):\n    \"\"\"Saves a program as a `.json` file.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    class Query(synalinks.DataModel):\n        query: str\n\n    class AnswerWithRationale(synalinks.DataModel):\n        rationale: str\n        answer: str\n\n    language_model = LanguageModel(\"ollama/mistral\")\n\n    program = synalinks.Sequential(\n        [\n            synalinks.Input(data_model=Query),\n            synalinks.Generator(\n                data_model=AnswerWithRationale,\n                language_model=language_model,\n            ),\n        ],\n    )\n\n    program.save(\"program.json\")\n    loaded_program = synalinks.programs.program_from_json(\"program.json\")\n    ```\n\n    The saved `.json` file contains:\n\n    - The program's configuration (architecture)\n    - The program's variables\n    - The program's optimizer's state (if any)\n    - The program's reward's state (if any)\n\n    Thus programs can be reinstantiated in the exact same state.\n\n    Args:\n        filepath (str | os.PathLike): `str` or `os.PathLike` object.\n            The path where to save the model. Must end in `.json`.\n        overwrite (bool): Whether we should overwrite any existing program at\n            the target location, or instead ask the user via\n            an interactive prompt. Default to `True`.\n    \"\"\"\n    from synalinks.src.saving import serialization_lib\n\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".json\"):\n        raise ValueError(\n            f\"The filepath should ends with '.json', received filepath={filepath}\"\n        )\n    program_config = serialization_lib.serialize_synalinks_object(self)\n    variables_config = self.get_state_tree()\n    program_config.update({\"variables\": variables_config})\n    program_config_string = orjson.dumps(\n        program_config, option=orjson.OPT_INDENT_2\n    ).decode()\n    if file_utils.exists(filepath) and not overwrite:\n        io_utils.ask_to_proceed_with_overwrite(filepath)\n    with open(filepath, \"w\") as f:\n        f.write(program_config_string)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.save_variables","title":"<code>save_variables(filepath, overwrite=True)</code>","text":"<p>Saves all module variables to a <code>.variables.json</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p><code>str</code> or <code>pathlib.Path</code> object. Path where to save the program. Must end in <code>.variables.json</code>.</p> required <code>overwrite</code> <code>bool</code> <p>Whether we should overwrite any existing program at the target location, or instead ask the user via an interactive prompt.</p> <code>True</code> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def save_variables(self, filepath, overwrite=True):\n    \"\"\"Saves all module variables to a `.variables.json` file.\n\n    Args:\n        filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n            Path where to save the program. Must end in `.variables.json`.\n        overwrite (bool): Whether we should overwrite any existing program\n            at the target location, or instead ask the user\n            via an interactive prompt.\n    \"\"\"\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".variables.json\"):\n        raise ValueError(\n            \"The filepath should ends with '.variables.json', \"\n            f\"received filepath={filepath}\"\n        )\n    config = self.get_state_tree()\n    config_string = orjson.dumps(config, option=orjson.OPT_INDENT_2).decode()\n    if file_utils.exists(filepath) and not overwrite:\n        io_utils.ask_to_proceed_with_overwrite(filepath)\n    with open(filepath, \"w\") as f:\n        f.write(config_string)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.set_state_tree","title":"<code>set_state_tree(state_tree)</code>","text":"<p>Assigns values to variables of the program.</p> <p>This method takes a dictionary of nested variable values, which represents the state tree of the program, and assigns them to the corresponding variables of the program. The dictionary keys represent the variable names (e.g., <code>'trainable_variables'</code>, <code>'optimizer_variables'</code>), and the values are nested dictionaries containing the variable paths and their corresponding values.</p> <p>Parameters:</p> Name Type Description Default <code>state_tree</code> <code>dict</code> <p>A dictionary representing the state tree of the program. The keys are the variable names, and the values are nested dictionaries representing the variable paths and their values.</p> required Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def set_state_tree(self, state_tree):\n    \"\"\"Assigns values to variables of the program.\n\n    This method takes a dictionary of nested variable values, which\n    represents the state tree of the program, and assigns them to the\n    corresponding variables of the program. The dictionary keys represent the\n    variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),\n    and the values are nested dictionaries containing the variable\n    paths and their corresponding values.\n\n    Args:\n        state_tree (dict): A dictionary representing the state tree of the program.\n            The keys are the variable names, and the values are nested\n            dictionaries representing the variable paths and their values.\n    \"\"\"\n    for k, v in state_tree.items():\n        path_value_dict = self._flatten_nested_dict(v)\n        if k == \"trainable_variables\":\n            self._assign_variable_values(self.trainable_variables, path_value_dict)\n        elif k == \"non_trainable_variables\":\n            self._assign_variable_values(\n                self.non_trainable_variables, path_value_dict\n            )\n        elif k == \"optimizer_trainable_variables\":\n            if self.optimizer:\n                self._assign_variable_values(\n                    self.optimizer.trainable_variables, path_value_dict\n                )\n        elif k == \"optimizer_non_trainable_variables\":\n            if self.optimizer:\n                self._assign_variable_values(\n                    self.optimizer.non_trainable_variables, path_value_dict\n                )\n        elif k == \"metrics_variables\":\n            self._assign_variable_values(self.metrics_variables, path_value_dict)\n        else:\n            raise ValueError(f\"Unknown variable name: {k}\")\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.summary","title":"<code>summary(line_length=None, positions=None, print_fn=None, expand_nested=False, show_trainable=False, module_range=None)</code>","text":"<p>Prints a string summary of the program.</p> <p>Parameters:</p> Name Type Description Default <code>line_length</code> <code>int</code> <p>Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes).</p> <code>None</code> <code>positions</code> <code>list</code> <p>Relative or absolute positions of log elements in each line. If not provided, becomes <code>[0.3, 0.6, 0.70, 1.]</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>print_fn</code> <code>Callable</code> <p>Print function to use. By default, prints to <code>stdout</code>. If <code>stdout</code> doesn't work in your environment, change to <code>print</code>. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary.</p> <code>None</code> <code>expand_nested</code> <code>bool</code> <p>Whether to expand the nested models. Defaults to <code>False</code>.</p> <code>False</code> <code>show_trainable</code> <code>bool</code> <p>Whether to show if a module is trainable. Defaults to <code>False</code>.</p> <code>False</code> <code>module_range</code> <code>list | tuple</code> <p>a list or tuple of 2 strings, which is the starting module name and ending module name (both inclusive) indicating the range of modules to be printed in summary. It also accepts regex patterns instead of exact names. In this case, the start predicate will be the first element that matches <code>module_range[0]</code> and the end predicate will be the last element that matches <code>module_range[1]</code>. By default <code>None</code> considers all modules of the model.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>summary()</code> is called before the model is built.</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def summary(\n    self,\n    line_length=None,\n    positions=None,\n    print_fn=None,\n    expand_nested=False,\n    show_trainable=False,\n    module_range=None,\n):\n    \"\"\"Prints a string summary of the program.\n\n    Args:\n        line_length (int): Total length of printed lines\n            (e.g. set this to adapt the display to different\n            terminal window sizes).\n        positions (list): Relative or absolute positions of log elements\n            in each line. If not provided, becomes\n            `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.\n        print_fn (Callable): Print function to use. By default, prints to `stdout`.\n            If `stdout` doesn't work in your environment, change to `print`.\n            It will be called on each line of the summary.\n            You can set it to a custom function\n            in order to capture the string summary.\n        expand_nested (bool): Whether to expand the nested models.\n            Defaults to `False`.\n        show_trainable (bool): Whether to show if a module is trainable.\n            Defaults to `False`.\n        module_range (list | tuple): a list or tuple of 2 strings,\n            which is the starting module name and ending module name\n            (both inclusive) indicating the range of modules to be printed\n            in summary. It also accepts regex patterns instead of exact\n            names. In this case, the start predicate will be\n            the first element that matches `module_range[0]`\n            and the end predicate will be the last element\n            that matches `module_range[1]`.\n            By default `None` considers all modules of the model.\n\n    Raises:\n        ValueError: if `summary()` is called before the model is built.\n    \"\"\"\n    summary_utils.print_summary(\n        self,\n        line_length=line_length,\n        positions=positions,\n        print_fn=print_fn,\n        expand_nested=expand_nested,\n        show_trainable=show_trainable,\n        module_range=module_range,\n    )\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Program%20class/#synalinks.src.programs.program.Program.to_json","title":"<code>to_json(**kwargs)</code>","text":"<p>Returns a JSON string containing the network configuration.</p> <pre><code>json_string = program.to_json()\n</code></pre> <p>To load a network from a JSON save file, use <code>synalinks.programs.program_from_json(json_string, custom_objects={...})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments to be passed to <code>orjson.dumps()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string.</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def to_json(self, **kwargs):\n    \"\"\"Returns a JSON string containing the network configuration.\n\n    ```python\n    json_string = program.to_json()\n    ```\n\n    To load a network from a JSON save file, use\n    `synalinks.programs.program_from_json(json_string, custom_objects={...})`.\n\n    Args:\n        **kwargs (keyword arguments): Additional keyword arguments to be passed to\n            `orjson.dumps()`.\n\n    Returns:\n        (str): A JSON string.\n    \"\"\"\n    from synalinks.src.saving import serialization_lib\n\n    program_config = serialization_lib.serialize_synalinks_object(self)\n    return orjson.dumps(program_config, **kwargs).decode()\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Sequential%20class/","title":"The Sequential class","text":""},{"location":"Synalinks%20API/Programs%20API/The%20Sequential%20class/#the-sequential-class","title":"The Sequential class","text":"<p>               Bases: <code>Program</code></p> <p><code>Sequential</code> groups a linear stack of modules into a <code>Program</code>.</p> <p>Examples:</p> <pre><code>program = synalinks.Sequential(\n    name=\"chain_of_thought\",\n    description=\"Useful to answer in a step by step manner.\"\n)\nprogram.add(\n    synalinks.Input(\n            data_program=Query,\n        )\n)\nprogram.add(\n    synalinks.Generator(\n        data_program=AnswerWithRationale,\n        language_program=language_program,\n    )\n)\n\n# Note that you can also omit the initial `Input`.\n# In that case the program doesn't have any variables until the first call\n# to a training/evaluation method (since it isn't yet built):\n\nprogram = synalinks.Sequential(\n    name=\"chain_of_thought\",\n    description=\"Useful to answer in a step by step manner.\"\n)\nprogram.add(\n    synalinks.Generator(\n        data_program=AnswerWithRationale,\n        language_program=language_program,\n    )\n)\n# program.variables not created yet\n\n# Whereas if you specify an `Input`, the program gets built\n# continuously as you are adding modules:\n\nprogram = synalinks.Sequential(\n    name=\"chain_of_thought\",\n    description=\"Useful to answer in a step by step manner.\"\n)\nprogram.add(\n    synalinks.Input(\n        data_program=Query,\n    )\n)\nprogram.add(\n    synalinks.Generator(\n        data_program=AnswerWithRationale,\n        language_program=language_program,\n    )\n)\n\n# Note that when using the delayed-build pattern (no input specified),\n# the program gets built the first time you call `fit`, `eval`, or `predict`,\n# or the first time you call the program on some input data.\n</code></pre> Source code in <code>synalinks/src/programs/sequential.py</code> <pre><code>@synalinks_export([\"synalinks.Sequential\", \"synalinks.programs.Sequential\"])\nclass Sequential(Program):\n    \"\"\"`Sequential` groups a linear stack of modules into a `Program`.\n\n    Examples:\n\n    ```python\n    program = synalinks.Sequential(\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\"\n    )\n    program.add(\n        synalinks.Input(\n                data_program=Query,\n            )\n    )\n    program.add(\n        synalinks.Generator(\n            data_program=AnswerWithRationale,\n            language_program=language_program,\n        )\n    )\n\n    # Note that you can also omit the initial `Input`.\n    # In that case the program doesn't have any variables until the first call\n    # to a training/evaluation method (since it isn't yet built):\n\n    program = synalinks.Sequential(\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\"\n    )\n    program.add(\n        synalinks.Generator(\n            data_program=AnswerWithRationale,\n            language_program=language_program,\n        )\n    )\n    # program.variables not created yet\n\n    # Whereas if you specify an `Input`, the program gets built\n    # continuously as you are adding modules:\n\n    program = synalinks.Sequential(\n        name=\"chain_of_thought\",\n        description=\"Useful to answer in a step by step manner.\"\n    )\n    program.add(\n        synalinks.Input(\n            data_program=Query,\n        )\n    )\n    program.add(\n        synalinks.Generator(\n            data_program=AnswerWithRationale,\n            language_program=language_program,\n        )\n    )\n\n    # Note that when using the delayed-build pattern (no input specified),\n    # the program gets built the first time you call `fit`, `eval`, or `predict`,\n    # or the first time you call the program on some input data.\n\n    ```\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        return typing.cast(cls, super().__new__(cls))\n\n    def __init__(self, modules=None, trainable=True, name=None, description=None):\n        if description is None:\n            raise ValueError(\n                \"All Sequential programs must have a `description`, \"\n                \"please add it to the constructor arguments\"\n            )\n        super().__init__(trainable=trainable, name=name, description=description)\n        self._functional = None\n        self._modules = []\n        if modules:\n            for module in modules:\n                self.add(module, rebuild=False)\n            run_maybe_nested(self._maybe_rebuild())\n\n    def add(self, module, rebuild=True):\n        \"\"\"Adds a module instance on top of the module stack.\n\n        Args:\n            module (Module): Module instance.\n            rebuild (bool): If `True` rebuild the program.\n        \"\"\"\n        # If we are passed a SymbolicDataModel created by synalinks.Input(), we\n        # extract the input module from its synalinks history and use that.\n        if hasattr(module, \"_synalinks_history\"):\n            origin_module = module._synalinks_history[0]\n            if isinstance(origin_module, InputModule):\n                module = origin_module\n        if not isinstance(module, Module):\n            raise ValueError(\n                \"Only instances of `synalinks.Module` can be \"\n                f\"added to a Sequential program. Received: {module} \"\n                f\"(of type {type(module)})\"\n            )\n        if not self._is_module_name_unique(module):\n            raise ValueError(\n                \"All modules added to a Sequential program \"\n                f\"should have unique names. Name '{module.name}' is already \"\n                \"the name of a module in this program. Update the `name` argument \"\n                \"to pass a unique name.\"\n            )\n        if (\n            isinstance(module, InputModule)\n            and self._modules\n            and isinstance(self._modules[0], InputModule)\n        ):\n            raise ValueError(\n                f\"Sequential program '{self.name}' has already been configured \"\n                f\"to use input schema {self._modules[0].input_schema}. You cannot \"\n                f\"add a different Input module to it.\"\n            )\n\n        self._modules.append(module)\n        if rebuild:\n            run_maybe_nested(self._maybe_rebuild())\n        else:\n            self.built = False\n            self._functional = None\n\n    def pop(self, rebuild=True):\n        \"\"\"Removes the last module in the program.\n\n        Args:\n            rebuild (bool): If `True` rebuild the program.\n        \"\"\"\n        module = self._modules.pop()\n        self.built = False\n        self._functional = None\n        if rebuild:\n            run_maybe_nested(self._maybe_rebuild())\n        return module\n\n    async def _maybe_rebuild(self):\n        self.built = False\n        self._functional = None\n        if isinstance(self._modules[0], InputModule) and len(self._modules) &gt; 1:\n            input_schema = self._modules[0].get_schema()\n            await self.build(Input(schema=input_schema))\n        elif hasattr(self._modules[0], \"input_schema\") and len(self._modules) &gt; 1:\n            # We can build the Sequential program if the first module has the\n            # `input_schema` property. This is most commonly found in Functional\n            # program.\n            input_schema = self._modules[0].input_schema\n            await self.build(Input(schema=input_schema))\n\n    def _lock_state(self):\n        # Unlike other modules, Sequential is mutable after build.\n        pass\n\n    def _obj_type(self):\n        return \"Sequential\"\n\n    async def build(self, inputs):\n        try:\n            input_schema = standardize_schema(inputs.get_schema())\n        except Exception:\n            # Do not attempt to build if the program does not have a single\n            # input.\n            return\n        if not self._modules:\n            raise ValueError(\n                f\"Sequential program {self.name} cannot be built because it has \"\n                \"no modules. Call `program.add(module)`.\"\n            )\n        if isinstance(self._modules[0], InputModule):\n            if self._modules[0].get_schema() != input_schema:\n                raise ValueError(\n                    f\"Sequential program '{self.name}' has already been \"\n                    \"configured to use input schema \"\n                    f\"{self._modules[0].get_schema()}. You cannot build it \"\n                    f\"with input_schema {input_schema}\"\n                )\n        else:\n            self._modules = [InputModule(schema=input_schema)] + self._modules\n\n        # Build functional program\n        inputs = self._modules[0].output\n        x = inputs\n        for module in self._modules[1:]:\n            try:\n                x = await module(x)\n            except NotImplementedError:\n                # Can happen if spec inference is not implemented.\n                # TODO: consider reverting inbound nodes on modules processed.\n                return\n            except TypeError as e:\n                signature = inspect.signature(module.call)\n                positional_args = [\n                    param\n                    for param in signature.parameters.values()\n                    if param.default == inspect.Parameter.empty\n                ]\n                if len(positional_args) != 1:\n                    raise ValueError(\n                        \"Modules added to a Sequential program \"\n                        \"can only have a single positional argument, \"\n                        f\"the input data model. Module {module.__class__.__name__} \"\n                        f\"has multiple positional arguments: {positional_args}\"\n                    )\n                raise e\n        outputs = x\n        self._functional = Functional(inputs=inputs, outputs=outputs)\n        self.built = True\n\n    async def call(self, inputs, training=None):\n        if self._functional:\n            return await self._functional.call(inputs, training=training)\n        # Fallback: Just apply the module sequence.\n        # This typically happens if `inputs` is a nested struct.\n        for module in self.modules:\n            # During each iteration, `inputs` are the inputs to `module`, and\n            # `outputs` are the outputs of `module` applied to `inputs`. At the\n            # end of each iteration `inputs` is set to `outputs` to prepare for\n            # the next module.\n            kwargs = {}\n            if module._call_has_training_arg and training is not None:\n                kwargs[\"training\"] = training\n            outputs = await module(inputs, **kwargs)\n            inputs = outputs\n        return outputs\n\n    @property\n    def modules(self):\n        \"\"\"Unlike Keras, also output the potentially auto-generated `InputModule`\"\"\"\n        return self._modules\n\n    @modules.setter\n    def modules(self, _):\n        raise AttributeError(\n            \"`Sequential.modules` attribute is reserved and should not be used. \"\n            \"Use `add()` and `pop()` to change the modules in this program.\"\n        )\n\n    async def compute_output_spec(self, inputs, training=None):\n        if self._functional:\n            return await self._functional.compute_output_spec(\n                inputs,\n                training=training,\n            )\n        # Direct application\n        for module in self.modules:\n            outputs = await module.compute_output_spec(inputs, training=training)\n            inputs = outputs\n        return outputs\n\n    @property\n    def input_schema(self):\n        if self._functional:\n            return self._functional.input_schema\n        raise AttributeError(\n            f\"Sequential program '{self.name}' has no defined input schema yet.\"\n        )\n\n    @property\n    def output_schema(self):\n        if self._functional:\n            return self._functional.output_schema\n        raise AttributeError(\n            f\"Sequential program '{self.name}' has no defined output schema yet.\"\n        )\n\n    @property\n    def inputs(self):\n        if self._functional:\n            return self._functional.inputs\n        raise AttributeError(\n            f\"Sequential program '{self.name}' has no defined inputs yet.\"\n        )\n\n    @property\n    def outputs(self):\n        if self._functional:\n            return self._functional.outputs\n        raise AttributeError(\n            f\"Sequential program '{self.name}' has no defined outputs yet.\"\n        )\n\n    def _is_module_name_unique(self, module):\n        for ref_module in self._modules:\n            if module.name == ref_module.name and ref_module is not module:\n                return False\n        return True\n\n    def get_config(self):\n        serialize_fn = serialization_lib.serialize_synalinks_object\n        module_configs = []\n        for module in self.modules:\n            module_configs.append(serialize_fn(module))\n        config = Program.get_config(self)\n        config[\"name\"] = self.name\n        config[\"description\"] = self.description\n        config[\"modules\"] = copy.deepcopy(module_configs)\n        if self._functional is not None:\n            config[\"build_input_schema\"] = self._modules[0].input_schema\n        return config\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        if \"name\" in config:\n            name = config[\"name\"]\n            build_input_schema = config.get(\"build_input_schema\")\n            module_configs = config[\"modules\"]\n        else:\n            name = None\n            module_configs = config\n        if \"description\" in config:\n            description = config[\"description\"]\n        else:\n            description = None\n        program = cls(name=name, description=description)\n        for module_config in module_configs:\n            module = serialization_lib.deserialize_synalinks_object(\n                module_config,\n                custom_objects=custom_objects,\n            )\n            program.add(module)\n        if (\n            not program._functional\n            and \"build_input_schema\" in locals()\n            and build_input_schema\n            and isinstance(build_input_schema, (tuple, list))\n        ):\n            program.build(build_input_schema)\n        return program\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Sequential%20class/#synalinks.src.programs.sequential.Sequential.modules","title":"<code>modules</code>  <code>property</code> <code>writable</code>","text":"<p>Unlike Keras, also output the potentially auto-generated <code>InputModule</code></p>"},{"location":"Synalinks%20API/Programs%20API/The%20Sequential%20class/#synalinks.src.programs.sequential.Sequential.add","title":"<code>add(module, rebuild=True)</code>","text":"<p>Adds a module instance on top of the module stack.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Module instance.</p> required <code>rebuild</code> <code>bool</code> <p>If <code>True</code> rebuild the program.</p> <code>True</code> Source code in <code>synalinks/src/programs/sequential.py</code> <pre><code>def add(self, module, rebuild=True):\n    \"\"\"Adds a module instance on top of the module stack.\n\n    Args:\n        module (Module): Module instance.\n        rebuild (bool): If `True` rebuild the program.\n    \"\"\"\n    # If we are passed a SymbolicDataModel created by synalinks.Input(), we\n    # extract the input module from its synalinks history and use that.\n    if hasattr(module, \"_synalinks_history\"):\n        origin_module = module._synalinks_history[0]\n        if isinstance(origin_module, InputModule):\n            module = origin_module\n    if not isinstance(module, Module):\n        raise ValueError(\n            \"Only instances of `synalinks.Module` can be \"\n            f\"added to a Sequential program. Received: {module} \"\n            f\"(of type {type(module)})\"\n        )\n    if not self._is_module_name_unique(module):\n        raise ValueError(\n            \"All modules added to a Sequential program \"\n            f\"should have unique names. Name '{module.name}' is already \"\n            \"the name of a module in this program. Update the `name` argument \"\n            \"to pass a unique name.\"\n        )\n    if (\n        isinstance(module, InputModule)\n        and self._modules\n        and isinstance(self._modules[0], InputModule)\n    ):\n        raise ValueError(\n            f\"Sequential program '{self.name}' has already been configured \"\n            f\"to use input schema {self._modules[0].input_schema}. You cannot \"\n            f\"add a different Input module to it.\"\n        )\n\n    self._modules.append(module)\n    if rebuild:\n        run_maybe_nested(self._maybe_rebuild())\n    else:\n        self.built = False\n        self._functional = None\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/The%20Sequential%20class/#synalinks.src.programs.sequential.Sequential.pop","title":"<code>pop(rebuild=True)</code>","text":"<p>Removes the last module in the program.</p> <p>Parameters:</p> Name Type Description Default <code>rebuild</code> <code>bool</code> <p>If <code>True</code> rebuild the program.</p> <code>True</code> Source code in <code>synalinks/src/programs/sequential.py</code> <pre><code>def pop(self, rebuild=True):\n    \"\"\"Removes the last module in the program.\n\n    Args:\n        rebuild (bool): If `True` rebuild the program.\n    \"\"\"\n    module = self._modules.pop()\n    self.built = False\n    self._functional = None\n    if rebuild:\n        run_maybe_nested(self._maybe_rebuild())\n    return module\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Program%20saving%20and%20loading/","title":"Program saving and loading","text":""},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Program%20saving%20and%20loading/#saving-programs-into-a-json-file","title":"Saving programs into a JSON file","text":"<p>Saves a program as a <code>.json</code> file.</p> <p>Example:</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str\n\nclass AnswerWithRationale(synalinks.DataModel):\n    rationale: str\n    answer: str\n\nlanguage_model = LanguageModel(\"ollama/mistral\")\n\nprogram = synalinks.Sequential(\n    [\n        synalinks.Input(data_model=Query),\n        synalinks.Generator(\n            data_model=AnswerWithRationale,\n            language_model=language_model,\n        ),\n    ],\n)\n\nprogram.save(\"program.json\")\nloaded_program = synalinks.programs.program_from_json(\"program.json\")\n</code></pre> <p>The saved <code>.json</code> file contains:</p> <ul> <li>The program's configuration (architecture)</li> <li>The program's variables</li> <li>The program's optimizer's state (if any)</li> <li>The program's reward's state (if any)</li> </ul> <p>Thus programs can be reinstantiated in the exact same state.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | PathLike</code> <p><code>str</code> or <code>os.PathLike</code> object. The path where to save the model. Must end in <code>.json</code>.</p> required <code>overwrite</code> <code>bool</code> <p>Whether we should overwrite any existing program at the target location, or instead ask the user via an interactive prompt. Default to <code>True</code>.</p> <code>True</code> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def save(self, filepath, overwrite=True, **kwargs):\n    \"\"\"Saves a program as a `.json` file.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    class Query(synalinks.DataModel):\n        query: str\n\n    class AnswerWithRationale(synalinks.DataModel):\n        rationale: str\n        answer: str\n\n    language_model = LanguageModel(\"ollama/mistral\")\n\n    program = synalinks.Sequential(\n        [\n            synalinks.Input(data_model=Query),\n            synalinks.Generator(\n                data_model=AnswerWithRationale,\n                language_model=language_model,\n            ),\n        ],\n    )\n\n    program.save(\"program.json\")\n    loaded_program = synalinks.programs.program_from_json(\"program.json\")\n    ```\n\n    The saved `.json` file contains:\n\n    - The program's configuration (architecture)\n    - The program's variables\n    - The program's optimizer's state (if any)\n    - The program's reward's state (if any)\n\n    Thus programs can be reinstantiated in the exact same state.\n\n    Args:\n        filepath (str | os.PathLike): `str` or `os.PathLike` object.\n            The path where to save the model. Must end in `.json`.\n        overwrite (bool): Whether we should overwrite any existing program at\n            the target location, or instead ask the user via\n            an interactive prompt. Default to `True`.\n    \"\"\"\n    from synalinks.src.saving import serialization_lib\n\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".json\"):\n        raise ValueError(\n            f\"The filepath should ends with '.json', received filepath={filepath}\"\n        )\n    program_config = serialization_lib.serialize_synalinks_object(self)\n    variables_config = self.get_state_tree()\n    program_config.update({\"variables\": variables_config})\n    program_config_string = orjson.dumps(\n        program_config, option=orjson.OPT_INDENT_2\n    ).decode()\n    if file_utils.exists(filepath) and not overwrite:\n        io_utils.ask_to_proceed_with_overwrite(filepath)\n    with open(filepath, \"w\") as f:\n        f.write(program_config_string)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Program%20saving%20and%20loading/#saving-programs-into-a-json-string","title":"Saving programs into a JSON string","text":"<p>Returns a JSON string containing the network configuration.</p> <pre><code>json_string = program.to_json()\n</code></pre> <p>To load a network from a JSON save file, use <code>synalinks.programs.program_from_json(json_string, custom_objects={...})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments to be passed to <code>orjson.dumps()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string.</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def to_json(self, **kwargs):\n    \"\"\"Returns a JSON string containing the network configuration.\n\n    ```python\n    json_string = program.to_json()\n    ```\n\n    To load a network from a JSON save file, use\n    `synalinks.programs.program_from_json(json_string, custom_objects={...})`.\n\n    Args:\n        **kwargs (keyword arguments): Additional keyword arguments to be passed to\n            `orjson.dumps()`.\n\n    Returns:\n        (str): A JSON string.\n    \"\"\"\n    from synalinks.src.saving import serialization_lib\n\n    program_config = serialization_lib.serialize_synalinks_object(self)\n    return orjson.dumps(program_config, **kwargs).decode()\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Program%20saving%20and%20loading/#loading-programs-from-a-json-file","title":"Loading programs from a JSON file","text":"<p>Load a program from a JSON file.</p> <p>Example:</p> <pre><code>import synalinks\n\nloaded_program = synalinks.Program.load(\"program.json\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p><code>str</code> or <code>pathlib.Path</code> object. Path to load the program's variables from. Must end in <code>.variables.json</code>.</p> required <code>custom_objects</code> <code>dict</code> <p>Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Program</code> <p>A Synalinks program instance (uncompiled).</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>@classmethod\ndef load(cls, filepath, custom_objects=None):\n    \"\"\"Load a program from a JSON file.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    loaded_program = synalinks.Program.load(\"program.json\")\n    ```\n\n    Args:\n        filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n            Path to load the program's variables from.\n            Must end in `.variables.json`.\n        custom_objects (dict): Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    Returns:\n        (Program): A Synalinks program instance (uncompiled).\n    \"\"\"\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".json\"):\n        raise ValueError(\n            f\"The filepath should ends with '.json', received filepath={filepath}\"\n        )\n    with open(filepath, \"r\") as f:\n        json_config = f.read()\n    return program_from_json(json_config, custom_objects=custom_objects)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Program%20saving%20and%20loading/#loading-a-program-from-a-json-string","title":"Loading a program from a JSON string","text":"<p>Parses a JSON program configuration string and returns a program instance.</p> <p>Example:</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str\n\nclass AnswerWithRationale(synalinks.DataModel):\n    rationale: str\n    answer: str\n\nlanguage_model = LanguageModel(\"ollama/mistral\")\n\nprogram = synalinks.Sequential(\n    [\n        synalinks.Input(data_model=Query),\n        synalinks.Generator(\n            data_model=AnswerWithRationale,\n            language_model=language_model,\n        ),\n    ],\n)\n\nconfig = program.to_json()\nloaded_program = synalinks.programs.program_from_json(config)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>JSON string encoding a program configuration.</p> required <code>custom_objects</code> <code>dict</code> <p>Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.</p> <code>None</code> <p>Returns:</p> Type Description <code>Program</code> <p>A Synalinks program instance (uncompiled).</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>@synalinks_export(\"synalinks.programs.program_from_json\")\ndef program_from_json(json_string, custom_objects=None):\n    \"\"\"Parses a JSON program configuration string and returns a program instance.\n\n    Example:\n\n    ```python\n    import synalinks\n\n    class Query(synalinks.DataModel):\n        query: str\n\n    class AnswerWithRationale(synalinks.DataModel):\n        rationale: str\n        answer: str\n\n    language_model = LanguageModel(\"ollama/mistral\")\n\n    program = synalinks.Sequential(\n        [\n            synalinks.Input(data_model=Query),\n            synalinks.Generator(\n                data_model=AnswerWithRationale,\n                language_model=language_model,\n            ),\n        ],\n    )\n\n    config = program.to_json()\n    loaded_program = synalinks.programs.program_from_json(config)\n    ```\n\n    Args:\n        json_string (str): JSON string encoding a program configuration.\n        custom_objects (dict): Optional dictionary mapping names\n            (strings) to custom classes or functions to be\n            considered during deserialization.\n\n    Returns:\n        (Program): A Synalinks program instance (uncompiled).\n    \"\"\"\n    from synalinks.src.saving import serialization_lib\n\n    program_config = orjson.loads(json_string)\n    variables_config = program_config.get(\"variables\")\n    program = serialization_lib.deserialize_synalinks_object(\n        program_config, custom_objects=custom_objects\n    )\n    program.set_state_tree(variables_config)\n    return program\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Variable%20saving%20and%20loading/","title":"Variable saving and loading","text":""},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Variable%20saving%20and%20loading/#saving-variables-into-a-json-file","title":"Saving variables into a JSON file","text":"<p>Saves all module variables to a <code>.variables.json</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p><code>str</code> or <code>pathlib.Path</code> object. Path where to save the program. Must end in <code>.variables.json</code>.</p> required <code>overwrite</code> <code>bool</code> <p>Whether we should overwrite any existing program at the target location, or instead ask the user via an interactive prompt.</p> <code>True</code> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def save_variables(self, filepath, overwrite=True):\n    \"\"\"Saves all module variables to a `.variables.json` file.\n\n    Args:\n        filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n            Path where to save the program. Must end in `.variables.json`.\n        overwrite (bool): Whether we should overwrite any existing program\n            at the target location, or instead ask the user\n            via an interactive prompt.\n    \"\"\"\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".variables.json\"):\n        raise ValueError(\n            \"The filepath should ends with '.variables.json', \"\n            f\"received filepath={filepath}\"\n        )\n    config = self.get_state_tree()\n    config_string = orjson.dumps(config, option=orjson.OPT_INDENT_2).decode()\n    if file_utils.exists(filepath) and not overwrite:\n        io_utils.ask_to_proceed_with_overwrite(filepath)\n    with open(filepath, \"w\") as f:\n        f.write(config_string)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Variable%20saving%20and%20loading/#saving-variables-into-json-dict","title":"Saving variables into JSON dict","text":"<p>Retrieves tree-like structure of program variables.</p> <p>This method allows retrieval of different program variables (trainable, non-trainable, optimizer, and metrics). The variables are returned in a nested dictionary format, where the keys correspond to the variable names and the values are the nested representations of the variables.</p> <p>Example:</p> <pre><code>program.compile(\n    optimizer=synalinks.optimizers.RandomFewShot(),\n    reward=synalinks.rewards.ExactMatch(),\n)\nprogram.fit(x=x_train, y=y_train)\nstate_tree = program.get_state_tree()\n</code></pre> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the nested representations of the requested variables. The keys are the variable names, and the values are the corresponding nested dictionaries.</p> Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def get_state_tree(self):\n    \"\"\"Retrieves tree-like structure of program variables.\n\n    This method allows retrieval of different program variables (trainable,\n    non-trainable, optimizer, and metrics). The variables are returned in a\n    nested dictionary format, where the keys correspond to the variable\n    names and the values are the nested representations of the variables.\n\n    Example:\n\n    ```python\n    program.compile(\n        optimizer=synalinks.optimizers.RandomFewShot(),\n        reward=synalinks.rewards.ExactMatch(),\n    )\n    program.fit(x=x_train, y=y_train)\n    state_tree = program.get_state_tree()\n    ```\n\n    Returns:\n        (dict): A dictionary containing the nested representations of the\n            requested variables. The keys are the variable names, and the\n            values are the corresponding nested dictionaries.\n    \"\"\"\n    variables = {}\n    variables[\"trainable_variables\"] = self._create_nested_dict(\n        self.trainable_variables\n    )\n    variables[\"non_trainable_variables\"] = self._create_nested_dict(\n        self.non_trainable_variables\n    )\n    if self.optimizer:\n        variables[\"optimizer_trainable_variables\"] = self._create_nested_dict(\n            self.optimizer.trainable_variables\n        )\n        variables[\"optimizer_non_trainable_variables\"] = self._create_nested_dict(\n            self.optimizer.non_trainable_variables\n        )\n    variables[\"metrics_variables\"] = self._create_nested_dict(self.metrics_variables)\n    return variables\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Variable%20saving%20and%20loading/#loading-variables-from-a-json-file","title":"Loading variables from a JSON file","text":"<p>Load all module variables from a <code>.variable.json</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p><code>str</code> or <code>pathlib.Path</code> object. Path to load the program's variables from. Must end in <code>.variables.json</code>.</p> required Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def load_variables(self, filepath):\n    \"\"\"Load all module variables from a `.variable.json` file.\n\n    Args:\n        filepath (str | pathlib.Path): `str` or `pathlib.Path` object.\n            Path to load the program's variables from.\n            Must end in `.variables.json`.\n    \"\"\"\n    filepath = file_utils.path_to_string(filepath)\n    if not filepath.endswith(\".variables.json\"):\n        raise ValueError(\n            \"The filepath should ends with '.variables.json', \"\n            f\"received filepath={filepath}\"\n        )\n    with open(filepath, \"rb\") as f:\n        state_tree_config = orjson.loads(f.read())\n    self.set_state_tree(state_tree_config)\n</code></pre>"},{"location":"Synalinks%20API/Programs%20API/Program%20Saving%20API/Variable%20saving%20and%20loading/#load-variables-from-a-json-dict","title":"Load variables from a JSON dict","text":"<p>Assigns values to variables of the program.</p> <p>This method takes a dictionary of nested variable values, which represents the state tree of the program, and assigns them to the corresponding variables of the program. The dictionary keys represent the variable names (e.g., <code>'trainable_variables'</code>, <code>'optimizer_variables'</code>), and the values are nested dictionaries containing the variable paths and their corresponding values.</p> <p>Parameters:</p> Name Type Description Default <code>state_tree</code> <code>dict</code> <p>A dictionary representing the state tree of the program. The keys are the variable names, and the values are nested dictionaries representing the variable paths and their values.</p> required Source code in <code>synalinks/src/programs/program.py</code> <pre><code>def set_state_tree(self, state_tree):\n    \"\"\"Assigns values to variables of the program.\n\n    This method takes a dictionary of nested variable values, which\n    represents the state tree of the program, and assigns them to the\n    corresponding variables of the program. The dictionary keys represent the\n    variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),\n    and the values are nested dictionaries containing the variable\n    paths and their corresponding values.\n\n    Args:\n        state_tree (dict): A dictionary representing the state tree of the program.\n            The keys are the variable names, and the values are nested\n            dictionaries representing the variable paths and their values.\n    \"\"\"\n    for k, v in state_tree.items():\n        path_value_dict = self._flatten_nested_dict(v)\n        if k == \"trainable_variables\":\n            self._assign_variable_values(self.trainable_variables, path_value_dict)\n        elif k == \"non_trainable_variables\":\n            self._assign_variable_values(\n                self.non_trainable_variables, path_value_dict\n            )\n        elif k == \"optimizer_trainable_variables\":\n            if self.optimizer:\n                self._assign_variable_values(\n                    self.optimizer.trainable_variables, path_value_dict\n                )\n        elif k == \"optimizer_non_trainable_variables\":\n            if self.optimizer:\n                self._assign_variable_values(\n                    self.optimizer.non_trainable_variables, path_value_dict\n                )\n        elif k == \"metrics_variables\":\n            self._assign_variable_values(self.metrics_variables, path_value_dict)\n        else:\n            raise ValueError(f\"Unknown variable name: {k}\")\n</code></pre>"},{"location":"Synalinks%20API/Rewards/","title":"Rewards","text":""},{"location":"Synalinks%20API/Rewards/#rewards","title":"Rewards","text":"<p><code>Reward</code>s are an essential part of reinforcement learning frameworks.  They are typically float values (usually between 0.0 and 1.0, but they can be  negative also) that guide the process into making more efficient decisions or  predictions. During training, the goal is to maximize the reward function.  The reward gives the system an indication of how well it performed for that task.</p> <p>The purpose of a reward function is to compute the quantity that the program should maximize during training.</p>"},{"location":"Synalinks%20API/Rewards/#rewards-overview","title":"Rewards Overview","text":"<ul> <li>ExactMatch reward</li> <li>CosineSimilarity reward</li> <li>LMAsJudge reward</li> <li>ProgramAsJudge reward</li> </ul>"},{"location":"Synalinks%20API/Rewards/CosineSimilarity%20reward/","title":"CosineSimilarity reward","text":""},{"location":"Synalinks%20API/Rewards/CosineSimilarity%20reward/#synalinks.src.rewards.cosine_similarity.CosineSimilarity","title":"<code>CosineSimilarity</code>","text":"<p>               Bases: <code>RewardFunctionWrapper</code></p> <p>Computes the cosine similarity between <code>y_true</code> and <code>y_pred</code>.</p> <p>Formula:</p> <pre><code>reward = (sum(l2_norm(y_true) * l2_norm(y_pred))+1) / 2\n</code></pre> <p>The formula is similar to the classic cosine similarity used in deep learning, but scaled to [0.0, 1.0] and adjusted to have a reward that tend towards 1.0 if the two objects are similar (and 0.0 otherwise).</p> <p>Example:</p> <pre><code>program.compile(\n    reward=synalinks.rewards.CosineSimilarity(\n        embedding_model=embedding_model\n    )\n    optimizer=synalinks.optimizers.RandomFewShot(),\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use to compute the cosine similarity.</p> <code>None</code> <code>axis</code> <code>int</code> <p>(Optional) Defaults to <code>-1</code>. The dimension along which the cosine similarity is computed.</p> <code>-1</code> <code>name</code> <code>str</code> <p>(Optional) string name of the reward instance.</p> <code>'cosine_similarity'</code> <code>in_mask</code> <code>list</code> <p>(Optional) list of keys to keep to compute the reward.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>(Optional) list of keys to remove to compute the reward.</p> <code>None</code> Source code in <code>synalinks/src/rewards/cosine_similarity.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.CosineSimilarity\",\n        \"synalinks.rewards.CosineSimilarity\",\n    ]\n)\nclass CosineSimilarity(RewardFunctionWrapper):\n    \"\"\"\n    Computes the cosine similarity between `y_true` and `y_pred`.\n\n    Formula:\n\n    ```\n    reward = (sum(l2_norm(y_true) * l2_norm(y_pred))+1) / 2\n    ```\n\n    The formula is similar to the classic cosine similarity used in deep learning,\n    but scaled to [0.0, 1.0] and adjusted to have a reward that tend\n    towards 1.0 if the two objects are similar (and 0.0 otherwise).\n\n    Example:\n\n    ```python\n    program.compile(\n        reward=synalinks.rewards.CosineSimilarity(\n            embedding_model=embedding_model\n        )\n        optimizer=synalinks.optimizers.RandomFewShot(),\n    )\n    ```\n\n    Args:\n        embedding_model (EmbeddingModel): The embedding model to use to compute the\n            cosine similarity.\n        axis (int): (Optional) Defaults to `-1`. The dimension along which the cosine\n            similarity is computed.\n        name (str): (Optional) string name of the reward instance.\n        in_mask (list): (Optional) list of keys to keep to compute the reward.\n        out_mask (list): (Optional) list of keys to remove to compute the reward.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_model=None,\n        axis=-1,\n        name=\"cosine_similarity\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            fn=cosine_similarity,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n            axis=axis,\n            embedding_model=embedding_model,\n        )\n\n    def get_config(self):\n        config = Reward.get_config()\n        from synalinks.src.saving.serialization_lib import serialize_synalinks_object\n\n        embedding_model_config = {\n            \"embedding_model\": serialize_synalinks_object(self.embedding_model)\n        }\n        return {**config, **embedding_model_config}\n\n    @classmethod\n    def from_config(cls, config):\n        from synalinks.saving.serialization_lib import deserialize_synalinks_object\n\n        embedding_model = deserialize_synalinks_object(config.pop(\"embedding_model\"))\n        return cls(embedding_model=embedding_model, **config)\n</code></pre>"},{"location":"Synalinks%20API/Rewards/CosineSimilarity%20reward/#synalinks.src.rewards.cosine_similarity.cosine_similarity","title":"<code>cosine_similarity(y_true, y_pred, embedding_model=None, axis=-1)</code>  <code>async</code>","text":"<p>Computes the cosine similarity between <code>y_true</code> and <code>y_pred</code>.</p> <p>Formula:</p> <pre><code>reward = (sum(l2_norm(y_true) * l2_norm(y_pred))+1) / 2\n</code></pre> <p>The formula is similar to the classic cosine similarity used in deep learning, but scaled to [0.0, 1.0] and adjusted to have a reward that tend towards 1.0 if the two objects are similar (and 0.0 otherwise).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>JsonDataModel</code> <p>The ground truth JSON data_model.</p> required <code>y_pred</code> <code>JsonDataModel</code> <p>The predicted JSON data_model.</p> required <code>embedding_model</code> <code>EmbeddingModel</code> <p>The embedding model to use to compute the cosine similarity.</p> <code>None</code> <code>axis</code> <code>int</code> <p>(Optional) Defaults to <code>-1</code>. The dimension along which the cosine similarity is computed.</p> <code>-1</code> <p>Returns:</p> Type Description <code>float</code> <p>The reward value, which tend to 1.0 if the values are similar, and towards 0.0 otherwise.</p> Source code in <code>synalinks/src/rewards/cosine_similarity.py</code> <pre><code>@synalinks_export(\"synalinks.rewards.cosine_similarity\")\nasync def cosine_similarity(y_true, y_pred, embedding_model=None, axis=-1):\n    \"\"\"\n    Computes the cosine similarity between `y_true` and `y_pred`.\n\n    Formula:\n\n    ```\n    reward = (sum(l2_norm(y_true) * l2_norm(y_pred))+1) / 2\n    ```\n\n    The formula is similar to the classic cosine similarity used in deep learning,\n    but scaled to [0.0, 1.0] and adjusted to have a reward that tend\n    towards 1.0 if the two objects are similar (and 0.0 otherwise).\n\n    Args:\n        y_true (JsonDataModel): The ground truth JSON data_model.\n        y_pred (JsonDataModel): The predicted JSON data_model.\n        embedding_model (EmbeddingModel): The embedding model to use to compute the\n            cosine similarity.\n        axis (int): (Optional) Defaults to `-1`. The dimension along which the cosine\n            similarity is computed.\n\n    Returns:\n        (float): The reward value, which tend to 1.0 if the values are similar,\n            and towards 0.0 otherwise.\n    \"\"\"\n    reward = 0.0\n    if y_pred is not None:\n        y_true = await ops.embedding(y_true, embedding_model=embedding_model)\n        y_pred = await ops.embedding(y_pred, embedding_model=embedding_model)\n        y_true = np.convert_to_tensor(y_true.get(\"embeddings\"))\n        y_pred = np.convert_to_tensor(y_pred.get(\"embeddings\"))\n        y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n        y_pred = np.normalize(y_pred, axis=axis)\n        y_true = np.normalize(y_true, axis=axis)\n        reward = (np.sum(y_true * y_pred, axis=axis) + 1) / 2\n    return reward\n</code></pre>"},{"location":"Synalinks%20API/Rewards/ExactMatch%20reward/","title":"ExactMatch reward","text":""},{"location":"Synalinks%20API/Rewards/ExactMatch%20reward/#synalinks.src.rewards.exact_match.ExactMatch","title":"<code>ExactMatch</code>","text":"<p>               Bases: <code>RewardFunctionWrapper</code></p> <p>Computes the exact match between <code>y_true</code> and <code>y_pred</code>.</p> <p>Example:</p> <pre><code>program.compile(\n    reward=synalinks.rewards.ExactMatch(),\n    optimizer=synalinks.optimizers.RandomFewShot(),\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optional. string name of the reward instance.</p> <code>'exact_match'</code> <code>in_mask</code> <code>list</code> <p>Optional. list of keys to keep to compute the reward.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>Optional. list of keys to remove to compute the reward.</p> <code>None</code> Source code in <code>synalinks/src/rewards/exact_match.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.ExactMatch\",\n        \"synalinks.rewards.ExactMatch\",\n    ]\n)\nclass ExactMatch(RewardFunctionWrapper):\n    \"\"\"Computes the exact match between `y_true` and `y_pred`.\n\n    Example:\n\n    ```python\n    program.compile(\n        reward=synalinks.rewards.ExactMatch(),\n        optimizer=synalinks.optimizers.RandomFewShot(),\n    )\n    ```\n\n    Args:\n        name (str): Optional. string name of the reward instance.\n        in_mask (list): Optional. list of keys to keep to compute the reward.\n        out_mask (list): Optional. list of keys to remove to compute the reward.\n    \"\"\"\n\n    def __init__(\n        self,\n        name=\"exact_match\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            fn=exact_match,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n\n    def get_config(self):\n        return {\n            \"name\": self.name,\n            \"in_mask\": self.in_mask,\n            \"out_mask\": self.out_mask,\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n</code></pre>"},{"location":"Synalinks%20API/Rewards/ExactMatch%20reward/#synalinks.src.rewards.exact_match.exact_match","title":"<code>exact_match(y_true, y_pred)</code>  <code>async</code>","text":"<p>Computes the exact match between <code>y_true</code> and <code>y_pred</code>.</p> <p>If their values are equal, it returns a reward of 1.0; otherwise, it returns 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>JsonDataModel</code> <p>The ground truth JSON data_model.</p> required <code>y_pred</code> <code>JsonDataModel</code> <p>The predicted JSON data_model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The reward value, which is 1.0 if the values match exactly, and 0.0 otherwise.</p> Source code in <code>synalinks/src/rewards/exact_match.py</code> <pre><code>@synalinks_export(\"synalinks.rewards.exact_match\")\nasync def exact_match(y_true, y_pred):\n    \"\"\"\n    Computes the exact match between `y_true` and `y_pred`.\n\n    If their values are equal, it returns a reward of 1.0; otherwise, it returns 0.0.\n\n    Args:\n        y_true (JsonDataModel): The ground truth JSON data_model.\n        y_pred (JsonDataModel): The predicted JSON data_model.\n\n    Returns:\n        (float): The reward value, which is 1.0 if the values match exactly,\n            and 0.0 otherwise.\n    \"\"\"\n    reward = 0.0\n    if y_pred is not None:\n        if y_pred.get_json() == y_true.get_json():\n            reward = 1.0\n    return reward\n</code></pre>"},{"location":"Synalinks%20API/Rewards/LMAsJudge%20reward/","title":"LMAsJudge reward","text":""},{"location":"Synalinks%20API/Rewards/LMAsJudge%20reward/#synalinks.src.rewards.lm_as_judge.LMAsJudge","title":"<code>LMAsJudge</code>","text":"<p>               Bases: <code>ProgramAsJudge</code></p> <p>Evaluate the output of a program using a <code>LanguageModel</code>.</p> <p>Example:</p> <pre><code>async def main():\n    # ... program definition\n\n    program.compile(\n        reward=synalinks.rewards.LMAsJudge(\n            language_model=language_model,\n        )\n        optimizer=synalinks.optimizers.RandomFewShot(),\n    )\n\n    history = await program.fit(...)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The default jinja2 prompt template to use (see <code>Generator</code>).</p> <code>None</code> <code>instructions</code> <code>list</code> <p>The default instructions to use (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default examples to use in the prompt (see <code>Generator</code>).</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. string name of the reward instance.</p> <code>'lm_as_judge'</code> <code>in_mask</code> <code>list</code> <p>Optional. list of keys to keep to compute the reward.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>Optional. list of keys to remove to compute the reward.</p> <code>None</code> Source code in <code>synalinks/src/rewards/lm_as_judge.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.LMAsJudge\",\n        \"synalinks.rewards.LMAsJudge\",\n    ]\n)\nclass LMAsJudge(ProgramAsJudge):\n    \"\"\"Evaluate the output of a program using a `LanguageModel`.\n\n    Example:\n\n    ```python\n\n    async def main():\n        # ... program definition\n\n        program.compile(\n            reward=synalinks.rewards.LMAsJudge(\n                language_model=language_model,\n            )\n            optimizer=synalinks.optimizers.RandomFewShot(),\n        )\n\n        history = await program.fit(...)\n\n    ```\n\n    Args:\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The default jinja2 prompt template\n            to use (see `Generator`).\n        instructions (list): The default instructions to use (see `Generator`).\n        examples (list): The default examples to use in the prompt\n            (see `Generator`).\n        name (str): Optional. string name of the reward instance.\n        in_mask (list): Optional. list of keys to keep to compute the reward.\n        out_mask (list): Optional. list of keys to remove to compute the reward.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        name=\"lm_as_judge\",\n        in_mask=None,\n        out_mask=None,\n    ):\n        program = LMAsJudgeProgram(\n            language_model=language_model,\n            prompt_template=prompt_template,\n            examples=examples,\n            instructions=instructions,\n        )\n        super().__init__(\n            program=program,\n            name=name,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n</code></pre>"},{"location":"Synalinks%20API/Rewards/LMAsJudge%20reward/#synalinks.src.rewards.lm_as_judge.LMAsJudgeProgram","title":"<code>LMAsJudgeProgram</code>","text":"<p>               Bases: <code>Program</code></p> <p>Evaluate the output of a program using a <code>LanguageModel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>LanguageModel</code> <p>The language model to use.</p> <code>None</code> <code>prompt_template</code> <code>str</code> <p>The default jinja2 prompt template to use (see <code>Generator</code>).</p> <code>None</code> <code>examples</code> <code>list</code> <p>The default examples to use in the prompt (see <code>Generator</code>).</p> <code>None</code> <code>instructions</code> <code>list</code> <p>The default instructions to use (see <code>Generator</code>).</p> <code>None</code> <code>name</code> <code>str</code> <p>Optional. The name of the program.</p> <code>None</code> <code>description</code> <code>str</code> <p>Optional. The description of the program.</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Whether the program's variables should be trainable.</p> <code>True</code> Source code in <code>synalinks/src/rewards/lm_as_judge.py</code> <pre><code>class LMAsJudgeProgram(Program):\n    \"\"\"Evaluate the output of a program using a `LanguageModel`.\n\n    Args:\n        language_model (LanguageModel): The language model to use.\n        prompt_template (str): The default jinja2 prompt template\n            to use (see `Generator`).\n        examples (list): The default examples to use in the prompt\n            (see `Generator`).\n        instructions (list): The default instructions to use (see `Generator`).\n        name (str): Optional. The name of the program.\n        description (str): Optional. The description of the program.\n        trainable (bool): Whether the program's variables should be trainable.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model=None,\n        prompt_template=None,\n        examples=None,\n        instructions=None,\n        name=None,\n        description=None,\n        trainable=True,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            trainable=trainable,\n        )\n        self.critique = SelfCritique(\n            language_model=language_model,\n            prompt_template=prompt_template,\n            examples=examples,\n            instructions=instructions,\n            name=\"self_critique_\" + self.name,\n        )\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.examples = examples\n        self.instructions = instructions\n\n    async def call(self, inputs):\n        if not isinstance(inputs, (list, tuple)):\n            raise ValueError(\"The inputs should be a list or tuple.\")\n        if len(inputs) != 2:\n            raise ValueError(\"The inputs of the program should have a length of 2.\")\n        y_true = inputs[0]\n        y_pred = inputs[1]\n        if not y_pred:\n            return 0.0\n        if y_true:\n            y_true = await ops.prefix(\n                y_true,\n                prefix=\"gold\",\n                name=\"gold_y_true\",\n            )\n            return await self.critique(\n                await ops.concat(\n                    y_true,\n                    y_pred,\n                    name=\"y_true_with_y_pred\",\n                )\n            )\n        else:\n            return await self.critique(y_pred)\n\n    def get_config(self):\n        config = {\n            \"prompt_template\": self.prompt_template,\n            \"examples\": self.examples,\n            \"instructions\": self.instructions,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"trainable\": self.trainable,\n        }\n        language_model_config = {\n            \"language_model\": serialization_lib.serialize_synalinks_object(\n                self.language_model\n            )\n        }\n        return {**language_model_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        language_model = serialization_lib.deserialize_synalinks_object(\n            config.pop(\"language_model\")\n        )\n        return cls(language_model=language_model, **config)\n</code></pre>"},{"location":"Synalinks%20API/Rewards/Reward%20wrappers/","title":"Reward wrappers","text":""},{"location":"Synalinks%20API/Rewards/Reward%20wrappers/#synalinks.src.rewards.reward_wrappers.ProgramAsJudge","title":"<code>ProgramAsJudge</code>","text":"<p>               Bases: <code>Reward</code></p> <p>Wrap a <code>Program</code> into a <code>Reward</code>.</p> <p>You can use this to create advanced reward functions that use a Synalinks <code>Program</code>. The program should have two inputs and one output.</p> <p>Note: The output data model/schema should have a field named <code>reward</code>.</p> <p>Example:</p> <pre><code># ... your program declaration\n\nprogram = synalinks.Program(\n    inputs=x0,\n    outputs=xn,\n)\n\nprogram.compile(\n    reward=synalinks.rewards.ProgramAsJudge(program=program)\n    optimizer=synalinks.optimizers.RandomFewShot(),\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>program</code> <code>Program</code> <p>The reward program to wrap.</p> required <code>name</code> <code>str</code> <p>Optional. string name of the reward instance.</p> <code>None</code> <code>in_mask</code> <code>list</code> <p>Optional. list of keys to keep to compute the reward.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>Optional. list of keys to remove to compute the reward.</p> <code>None</code> Source code in <code>synalinks/src/rewards/reward_wrappers.py</code> <pre><code>@synalinks_export(\n    [\n        \"synalinks.ProgramAsJudge\",\n        \"synalinks.rewards.ProgramAsJudge\",\n    ]\n)\nclass ProgramAsJudge(Reward):\n    \"\"\"Wrap a `Program` into a `Reward`.\n\n    You can use this to create advanced reward functions that use a Synalinks `Program`.\n    The program should have two inputs and one output.\n\n    **Note:** The output data model/schema should have a field named `reward`.\n\n    Example:\n\n    ```python\n    # ... your program declaration\n\n    program = synalinks.Program(\n        inputs=x0,\n        outputs=xn,\n    )\n\n    program.compile(\n        reward=synalinks.rewards.ProgramAsJudge(program=program)\n        optimizer=synalinks.optimizers.RandomFewShot(),\n    )\n    ```\n\n    Args:\n        program (Program): The reward program to wrap.\n        name (str): Optional. string name of the reward instance.\n        in_mask (list): Optional. list of keys to keep to compute the reward.\n        out_mask (list): Optional. list of keys to remove to compute the reward.\n    \"\"\"\n\n    def __init__(\n        self,\n        program,\n        reduction=\"mean\",\n        name=None,\n        in_mask=None,\n        out_mask=None,\n    ):\n        super().__init__(\n            name=name,\n            reduction=reduction,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n        self.program = program\n\n    async def call(self, y_true, y_pred):\n        result = await self.program([y_true, y_pred])\n        return float(result.get(\"reward\", 0.0))\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"program\": self.program})\n        return config\n\n    def __repr__(self):\n        return f\"&lt;ProgramAsJudge({self.program})&gt;\"\n</code></pre>"},{"location":"Synalinks%20API/Rewards/Reward%20wrappers/#synalinks.src.rewards.reward_wrappers.RewardFunctionWrapper","title":"<code>RewardFunctionWrapper</code>","text":"<p>               Bases: <code>Reward</code></p> <p>Wrap a stateless function into a <code>Reward</code>.</p> <p>You can use this to quickly build a reward from a function. The function needs to have the signature <code>fn(y_true, y_pred)</code>.</p> <p>Example:</p> <pre><code>def my_reward(y_true, y_pred):\n    # ...\n    return reward\n\nprogram.compile(\n    reward=synalinks.rewards.RewardFunctionWrapper,\n    optimizer=synalinks.optimizers.RandomFewShot()\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>callable</code> <p>The reward function to wrap, with signature <code>fn(y_true, y_pred, **kwargs)</code>.</p> required <code>name</code> <code>str</code> <p>Optional. string name of the reward instance.</p> <code>None</code> <code>in_mask</code> <code>list</code> <p>Optional. list of keys to keep to compute the reward.</p> <code>None</code> <code>out_mask</code> <code>list</code> <p>Optional. list of keys to remove to compute the reward.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Keyword arguments to pass on to <code>fn</code>.</p> <code>{}</code> Source code in <code>synalinks/src/rewards/reward_wrappers.py</code> <pre><code>@synalinks_export(\"synalinks.rewards.RewardFunctionWrapper\")\nclass RewardFunctionWrapper(Reward):\n    \"\"\"Wrap a stateless function into a `Reward`.\n\n    You can use this to quickly build a reward from a function. The function needs\n    to have the signature `fn(y_true, y_pred)`.\n\n    Example:\n\n    ```python\n\n    def my_reward(y_true, y_pred):\n        # ...\n        return reward\n\n    program.compile(\n        reward=synalinks.rewards.RewardFunctionWrapper,\n        optimizer=synalinks.optimizers.RandomFewShot()\n    )\n    ```\n\n    Args:\n        fn (callable): The reward function to wrap, with signature\n            `fn(y_true, y_pred, **kwargs)`.\n        name (str): Optional. string name of the reward instance.\n        in_mask (list): Optional. list of keys to keep to compute the reward.\n        out_mask (list): Optional. list of keys to remove to compute the reward.\n        **kwargs (keyword arguments): Keyword arguments to pass on to `fn`.\n    \"\"\"\n\n    def __init__(\n        self,\n        fn,\n        reduction=\"mean\",\n        name=None,\n        in_mask=None,\n        out_mask=None,\n        **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            reduction=reduction,\n            in_mask=in_mask,\n            out_mask=out_mask,\n        )\n        self.fn = fn\n        self._fn_kwargs = kwargs\n\n    async def call(self, y_true, y_pred):\n        return await self.fn(y_true, y_pred, **self._fn_kwargs)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"fn\": serialization_lib.serialize_synalinks_object(self.fn)})\n        config.update(serialization_lib.serialize_synalinks_object(self._fn_kwargs))\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        if \"fn\" in config:\n            config = serialization_lib.deserialize_synalinks_object(config)\n        return cls(**config)\n\n    def __repr__(self):\n        return f\"&lt;RewardFunctionWrapper({self.fn}, kwargs={self._fn_kwargs})&gt;\"\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/","title":"More plotting utilities","text":""},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_history.plot_history","title":"<code>plot_history(history, to_file='training_history.png', to_folder=None, xlabel='Epochs', ylabel='Scores', title='Training history', grid=True, metrics_filter=None, **kwargs)</code>","text":"<p>Plots the training history of a program and saves it to a file.</p> <p>Code Example:</p> <pre><code>program.compile(...)\nhistory = await program.fit(...)\n\nsynalinks.utils.plot_history(history)\n</code></pre> <p>Example:</p> <p></p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>History</code> <p>The training history.</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"training_history.png\".</p> <code>'training_history.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Epochs\".</p> <code>'Epochs'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Training history\".</p> <code>'Training history'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.plot()</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_history.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_history\")\ndef plot_history(\n    history,\n    to_file=\"training_history.png\",\n    to_folder=None,\n    xlabel=\"Epochs\",\n    ylabel=\"Scores\",\n    title=\"Training history\",\n    grid=True,\n    metrics_filter=None,\n    **kwargs,\n):\n    \"\"\"Plots the training history of a program and saves it to a file.\n\n    Code Example:\n\n    ```python\n    program.compile(...)\n    history = await program.fit(...)\n\n    synalinks.utils.plot_history(history)\n    ```\n\n    Example:\n\n    ![training_history.png](../../assets/training_history.png)\n\n    Args:\n        history (History): The training history.\n        to_file (str): The file path where the plot will be saved.\n            Default to \"training_history.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Epochs\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot. Default to \"Training history\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.plot()`\n\n    Raises:\n        ValueError: If there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    all_metrics = list(history.history.keys())\n\n    if metrics_filter is not None:\n        if not all(metric in all_metrics for metric in metrics_filter):\n            raise ValueError(f\"Requested metrics {metrics_filter} not found in history\")\n        all_metrics = metrics_filter\n\n    colors = generate_distinct_colors(len(all_metrics))\n\n    for i, metric in enumerate(all_metrics):\n        plt.plot(history.history[metric], label=metric, color=colors[i], **kwargs)\n\n    if xlabel:\n        plt.xlabel(xlabel)\n\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    all_values = [val for metric in all_metrics for val in history.history[metric]]\n    max_value = max(all_values) if all_values else 1.0\n    plt.ylim(0.0, max(1.0, max_value * 1.05))  # 5% padding above max value\n\n    plt.legend()\n    plt.grid(grid)\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_history.plot_history_comparison","title":"<code>plot_history_comparison(history_dict, to_file='training_history_comparison.png', to_folder=None, xlabel='Epochs', ylabel='Scores', title='Training History Comparison', grid=True, metrics_filter=None, linestyle_cycle=None, **kwargs)</code>","text":"<p>Plots comparison of training histories across different conditions/models.</p> <p>Code Example:</p> <pre><code>import synalinks\nimport asyncio\n\nNB_RUN = 5\n\nasync def main():\n\n    # ... program definition\n\n    program.compile(...)\n\n    history_list = []\n    for i in range(NB_RUN):\n        history = await program.fit(...)\n        history_list.append(history)\n\n    # ... program_1 definition\n\n    program_1.compile(...)\n\n    history_list_1 = []\n    for i in range(NB_RUN):\n        history = await program.fit(...)\n        history_list_1.append(history)\n\n    history_comparaison = {\n        \"program_a\": history_list\n        \"program_b: history_list_1\n    }\n\n    synalinks.utils.plot_history_comparison(history_comparison)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>history_dict</code> <code>dict</code> <p>Dictionary where keys are condition names (e.g., model names) and values are History objects. Format: {\"condition1\": history1, \"condition2\": history2, ...}</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"training_history_comparison.png\".</p> <code>'training_history_comparison.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Epochs\".</p> <code>'Epochs'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot (Default to \"Training History Comparison\").</p> <code>'Training History Comparison'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>linestyle_cycle</code> <code>list</code> <p>List of line styles to cycle through for conditions (Default to ['-', '--', '-.', ':']).</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.plot()</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If history_dict is empty, has inconsistent metric names, or if there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_history.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_history_comparison\")\ndef plot_history_comparison(\n    history_dict,\n    to_file=\"training_history_comparison.png\",\n    to_folder=None,\n    xlabel=\"Epochs\",\n    ylabel=\"Scores\",\n    title=\"Training History Comparison\",\n    grid=True,\n    metrics_filter=None,\n    linestyle_cycle=None,\n    **kwargs,\n):\n    \"\"\"Plots comparison of training histories across different conditions/models.\n\n    Code Example:\n\n    ```python\n    import synalinks\n    import asyncio\n\n    NB_RUN = 5\n\n    async def main():\n\n        # ... program definition\n\n        program.compile(...)\n\n        history_list = []\n        for i in range(NB_RUN):\n            history = await program.fit(...)\n            history_list.append(history)\n\n        # ... program_1 definition\n\n        program_1.compile(...)\n\n        history_list_1 = []\n        for i in range(NB_RUN):\n            history = await program.fit(...)\n            history_list_1.append(history)\n\n        history_comparaison = {\n            \"program_a\": history_list\n            \"program_b: history_list_1\n        }\n\n        synalinks.utils.plot_history_comparison(history_comparison)\n    ```\n\n    Args:\n        history_dict (dict): Dictionary where keys are condition names (e.g., model names)\n            and values are History objects. Format:\n            {\"condition1\": history1, \"condition2\": history2, ...}\n        to_file (str): The file path where the plot will be saved.\n            Default to \"training_history_comparison.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Epochs\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot\n            (Default to \"Training History Comparison\").\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        linestyle_cycle (list, optional): List of line styles to cycle through\n            for conditions (Default to ['-', '--', '-.', ':']).\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.plot()`\n\n    Raises:\n        ValueError: If history_dict is empty, has inconsistent metric names,\n            or if there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    if not history_dict:\n        raise ValueError(\"history_dict cannot be empty\")\n\n    # Get all metric names and validate consistency\n    condition_names = list(history_dict.keys())\n    all_metric_names = list(history_dict[condition_names[0]].history.keys())\n\n    # Validate that all conditions have the same metrics\n    for condition in condition_names:\n        if set(history_dict[condition].history.keys()) != set(all_metric_names):\n            raise ValueError(\n                f\"Condition '{condition}' has inconsistent metric names. \"\n                f\"Expected: {all_metric_names}, \"\n                f\"Got: {list(history_dict[condition].history.keys())}\"\n            )\n\n    if metrics_filter is not None:\n        if not all(metric in all_metric_names for metric in metrics_filter):\n            raise ValueError(f\"Requested metrics {metrics_filter} not found in history\")\n        metric_names = metrics_filter\n    else:\n        metric_names = all_metric_names\n\n    if linestyle_cycle is None:\n        linestyle_cycle = [\"-\", \"--\", \"-.\", \":\"]\n\n    # Get colors for metrics and line styles for conditions\n    colors = generate_distinct_colors(len(metric_names))\n\n    plt.figure(figsize=(12, 8))\n\n    # Plot each metric for each condition\n    for metric_idx, metric in enumerate(metric_names):\n        for cond_idx, condition in enumerate(condition_names):\n            history = history_dict[condition]\n            linestyle = linestyle_cycle[cond_idx % len(linestyle_cycle)]\n\n            plt.plot(\n                history.history[metric],\n                label=f\"{condition} - {metric}\",\n                color=colors[metric_idx],\n                linestyle=linestyle,\n                **kwargs,\n            )\n\n    if xlabel:\n        plt.xlabel(xlabel)\n\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    all_values = []\n    for condition in condition_names:\n        for metric in metric_names:\n            all_values.extend(history_dict[condition].history[metric])\n    max_value = max(all_values) if all_values else 1.0\n    plt.ylim(0.0, max(1.0, max_value * 1.05))  # 5% padding above max value\n\n    plt.legend()\n    plt.grid(grid)\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_history.plot_history_comparison_with_mean_and_std","title":"<code>plot_history_comparison_with_mean_and_std(history_comparison_dict, to_file='training_history_comparison_with_mean_and_std.png', to_folder=None, xlabel='Epochs', ylabel='Scores', title='Training History Comparison with Mean and Std', grid=True, alpha=0.2, metrics_filter=None, linestyle_cycle=None, **kwargs)</code>","text":"<p>Plots comparison of training histories with mean and standard deviation     across conditions.</p> <p>Calculates mean and standard deviation for each condition across multiple runs and displays them as line plots with error bands for comparison.</p> <p>Code Example: </p><pre><code># Compare training histories from different models with multiple runs each\nhistory_comparison = {\n    \"Model A\": [history_a1, history_a2, history_a3],\n    \"Model B\": [history_b1, history_b2, history_b3]\n}\n\nsynalinks.utils.plot_history_comparison_with_mean_and_std(history_comparison)\n</code></pre><p></p> <p>Parameters:</p> Name Type Description Default <code>history_comparison_dict</code> <code>dict</code> <p>Dictionary where keys are condition names and values are lists of History objects. Format: {\"condition1\": [history1, history2, ...], ...}</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"training_history_comparison_with_mean_and_std.png\".</p> <code>'training_history_comparison_with_mean_and_std.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Epochs\".</p> <code>'Epochs'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Training History Comparison with Mean and Std\".</p> <code>'Training History Comparison with Mean and Std'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>The transparency of the standard deviation area. Default to 0.2.</p> <code>0.2</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>linestyle_cycle</code> <code>list</code> <p>List of line styles to cycle through for conditions (Default to ['-', '--', '-.', ':']).</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.plot()</code> for the mean lines.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If history_comparison_dict is empty, has inconsistent structures, or if there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_history.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_history_comparison_with_mean_and_std\")\ndef plot_history_comparison_with_mean_and_std(\n    history_comparison_dict,\n    to_file=\"training_history_comparison_with_mean_and_std.png\",\n    to_folder=None,\n    xlabel=\"Epochs\",\n    ylabel=\"Scores\",\n    title=\"Training History Comparison with Mean and Std\",\n    grid=True,\n    alpha=0.2,\n    metrics_filter=None,\n    linestyle_cycle=None,\n    **kwargs,\n):\n    \"\"\"Plots comparison of training histories with mean and standard deviation\n        across conditions.\n\n    Calculates mean and standard deviation for each condition across multiple runs and\n    displays them as line plots with error bands for comparison.\n\n    Code Example:\n    ```python\n    # Compare training histories from different models with multiple runs each\n    history_comparison = {\n        \"Model A\": [history_a1, history_a2, history_a3],\n        \"Model B\": [history_b1, history_b2, history_b3]\n    }\n\n    synalinks.utils.plot_history_comparison_with_mean_and_std(history_comparison)\n    ```\n\n    Args:\n        history_comparison_dict (dict): Dictionary where keys are condition names and\n            values are lists of History objects. Format:\n            {\"condition1\": [history1, history2, ...], ...}\n        to_file (str): The file path where the plot will be saved.\n            Default to \"training_history_comparison_with_mean_and_std.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Epochs\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot.\n            Default to \"Training History Comparison with Mean and Std\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        alpha (float): The transparency of the standard deviation area. Default to 0.2.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        linestyle_cycle (list, optional): List of line styles to cycle through\n            for conditions (Default to ['-', '--', '-.', ':']).\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.plot()` for the mean lines.\n\n    Raises:\n        ValueError: If history_comparison_dict is empty, has inconsistent structures,\n            or if there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    if not history_comparison_dict:\n        raise ValueError(\"history_comparison_dict cannot be empty\")\n\n    condition_names = list(history_comparison_dict.keys())\n\n    # Validate structure and get metric names\n    for condition in condition_names:\n        if not isinstance(history_comparison_dict[condition], list):\n            raise ValueError(\n                f\"Values for condition '{condition}' must be a list of History objects\"\n            )\n        if not history_comparison_dict[condition]:\n            raise ValueError(f\"History list for condition '{condition}' cannot be empty\")\n\n    # Get metric names from first condition's first history\n    all_metric_names = list(history_comparison_dict[condition_names[0]][0].history.keys())\n\n    # Validate consistency across all conditions and histories\n    for condition in condition_names:\n        for i, history in enumerate(history_comparison_dict[condition]):\n            if set(history.history.keys()) != set(all_metric_names):\n                raise ValueError(\n                    f\"History {i} for condition '{condition}' has inconsistent \"\n                    \"metric names. \"\n                    f\"Expected: {all_metric_names}, Got: {list(history.history.keys())}\"\n                )\n\n    if metrics_filter is not None:\n        if not all(metric in all_metric_names for metric in metrics_filter):\n            raise ValueError(f\"Requested metrics {metrics_filter} not found in history\")\n        metric_names = metrics_filter\n    else:\n        metric_names = all_metric_names\n\n    if linestyle_cycle is None:\n        linestyle_cycle = [\"-\", \"--\", \"-.\", \":\"]\n\n    # Calculate statistics for each condition and metric\n    condition_stats = {}\n    min_epochs = float(\"inf\")\n\n    for condition in condition_names:\n        # Find minimum epochs across all histories for this condition\n        cond_min_epochs = min(\n            len(history.history[metric_names[0]])\n            for history in history_comparison_dict[condition]\n        )\n        min_epochs = min(min_epochs, cond_min_epochs)\n\n        condition_stats[condition] = {}\n        for metric in metric_names:\n            # Collect all values for this metric across runs\n            all_values = []\n            for history in history_comparison_dict[condition]:\n                all_values.append(history.history[metric][:cond_min_epochs])\n\n            # Calculate mean and std across runs\n            values_array = np.array(all_values)\n            mean_vals = np.mean(values_array, axis=0)[:min_epochs]\n            std_vals = (\n                np.std(values_array, axis=0, ddof=1)[:min_epochs]\n                if len(all_values) &gt; 1\n                else np.zeros_like(mean_vals)\n            )\n\n            condition_stats[condition][metric] = {\"mean\": mean_vals, \"std\": std_vals}\n\n    # Get colors for metrics\n    colors = generate_distinct_colors(len(metric_names))\n\n    plt.figure(figsize=(12, 8))\n\n    # Plot each metric for each condition\n    x = range(min_epochs)\n    for metric_idx, metric in enumerate(metric_names):\n        for cond_idx, condition in enumerate(condition_names):\n            mean_vals = condition_stats[condition][metric][\"mean\"]\n            std_vals = condition_stats[condition][metric][\"std\"]\n\n            linestyle = linestyle_cycle[cond_idx % len(linestyle_cycle)]\n            color = colors[metric_idx]\n\n            # Plot mean line\n            plt.plot(\n                x,\n                mean_vals,\n                label=f\"{condition} - {metric}\",\n                color=color,\n                linestyle=linestyle,\n                **kwargs,\n            )\n\n            # Plot std band\n            plt.fill_between(\n                x, mean_vals - std_vals, mean_vals + std_vals, color=color, alpha=alpha\n            )\n\n    if xlabel:\n        plt.xlabel(xlabel)\n\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    all_values = []\n    for condition in condition_names:\n        for metric in metric_names:\n            mean_vals = condition_stats[condition][metric][\"mean\"]\n            std_vals = condition_stats[condition][metric][\"std\"]\n            all_values.extend(mean_vals + std_vals)\n    max_value = max(all_values) if all_values else 1.0\n    plt.ylim(0.0, max(1.0, max_value * 1.05))  # 5% padding above max value\n\n    plt.legend()\n    plt.grid(grid)\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_history.plot_history_with_mean_and_std","title":"<code>plot_history_with_mean_and_std(history_list, to_file='training_history_with_mean_and_std.png', to_folder=None, xlabel='Epochs', ylabel='Scores', title='Training history with mean and std', grid=True, alpha=0.2, metrics_filter=None, **kwargs)</code>","text":"<p>Plots the mean and standard deviation of multiple training history list.</p> <p>This function takes a list of history objects from multiple runs of the same model and plots the mean and standard deviation for each metric.</p> <p>Code Example:</p> <pre><code>program.compile(...)\nhistory_list = []\nfor i in range(5):  # run 5 times\n    history = await program.fit(...)\n    history_list.append(history)\n\nsynalinks.utils.plot_history_with_mean_and_std(history_list)\n</code></pre> <p>Example:</p> <p></p> <p>Parameters:</p> Name Type Description Default <code>history_list</code> <code>list</code> <p>A list of History objects from multiple runs.</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"training_history_with_mean_and_std.png\".</p> <code>'training_history_with_mean_and_std.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Epochs\".</p> <code>'Epochs'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Training history with mean and std\".</p> <code>'Training history with mean and std'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>The transparency of the standard deviation area. Default to 0.2.</p> <code>0.2</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.plot()</code> for the mean lines.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>history_list</code> is empty, not a list, or if metrics_filter don't match across <code>history_list</code>.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_history.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_history_with_mean_and_std\")\ndef plot_history_with_mean_and_std(\n    history_list,\n    to_file=\"training_history_with_mean_and_std.png\",\n    to_folder=None,\n    xlabel=\"Epochs\",\n    ylabel=\"Scores\",\n    title=\"Training history with mean and std\",\n    grid=True,\n    alpha=0.2,\n    metrics_filter=None,\n    **kwargs,\n):\n    \"\"\"Plots the mean and standard deviation of multiple training history list.\n\n    This function takes a list of history objects from multiple runs of the same model\n    and plots the mean and standard deviation for each metric.\n\n    Code Example:\n\n    ```python\n    program.compile(...)\n    history_list = []\n    for i in range(5):  # run 5 times\n        history = await program.fit(...)\n        history_list.append(history)\n\n    synalinks.utils.plot_history_with_mean_and_std(history_list)\n    ```\n\n    Example:\n\n    ![training_history_with_mean_and_std.png](../../assets/training_history_with_mean_and_std.png)\n\n    Args:\n        history_list (list): A list of History objects from multiple runs.\n        to_file (str): The file path where the plot will be saved.\n            Default to \"training_history_with_mean_and_std.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Epochs\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot.\n            Default to \"Training history with mean and std\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        alpha (float): The transparency of the standard deviation area. Default to 0.2.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.plot()` for the mean lines.\n\n    Raises:\n        ValueError: If `history_list` is empty, not a list, or if metrics_filter\n            don't match across `history_list`.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    if not history_list:\n        raise ValueError(\"history_list cannot be empty\")\n\n    if not isinstance(history_list, list):\n        raise ValueError(\"history_list must be a list of History objects\")\n\n    all_metrics = list(history_list[0].history.keys())\n\n    if metrics_filter is not None:\n        if not all(metric in all_metrics for metric in metrics_filter):\n            raise ValueError(\n                f\"Requested metrics {metrics_filter} not found in history_list\"\n            )\n        all_metrics = metrics_filter\n\n    for i, history in enumerate(history_list):\n        if not all(metric in history.history for metric in all_metrics):\n            raise ValueError(\n                f\"Entry {i}: All history objects must contain the same metrics\"\n            )\n\n    min_epochs = min(len(history.history[all_metrics[0]]) for history in history_list)\n\n    all_values = {metric: [] for metric in all_metrics}\n\n    for history in history_list:\n        for metric in all_metrics:\n            all_values[metric].append(history.history[metric][:min_epochs])\n\n    mean_values = {}\n    std_values = {}\n    for metric in all_metrics:\n        values_array = np.array(all_values[metric])\n        mean_values[metric] = np.mean(values_array, axis=0)\n        std_values[metric] = (\n            np.std(values_array, axis=0, ddof=1)\n            if len(all_values[metric]) &gt; 1\n            else np.zeros_like(np.mean(values_array, axis=0))\n        )\n\n    colors = generate_distinct_colors(len(all_metrics))\n\n    plt.figure(figsize=(10, 6))\n\n    for i, metric in enumerate(all_metrics):\n        color = colors[i]\n        x = range(min_epochs)\n        mean = mean_values[metric]\n        std = std_values[metric]\n\n        plt.plot(x, mean, label=f\"{metric} (mean)\", color=color, **kwargs)\n\n        plt.fill_between(\n            x,\n            mean - std,\n            mean + std,\n            color=color,\n            alpha=alpha,\n            label=f\"{metric} (\u00b1std)\",\n        )\n\n    if xlabel:\n        plt.xlabel(xlabel)\n\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    all_vals = []\n    for metric in all_metrics:\n        all_vals.extend(mean_values[metric] + std_values[metric])\n    max_value = max(all_vals) if all_vals else 1.0\n    plt.ylim(0.0, max(1.0, max_value * 1.05))  # 5% padding above max value\n\n    plt.legend()\n    plt.grid(grid)\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_metrics.plot_metrics","title":"<code>plot_metrics(metrics, to_file='evaluation_metrics.png', to_folder=None, xlabel='Metrics', ylabel='Scores', title='Evaluation metrics', grid=True, metrics_filter=None, **kwargs)</code>","text":"<p>Plots the evaluation metrics of a program and saves it to a file.</p> <p>Code Example:</p> <pre><code>program.compile(...)\nmetrics = await program.evaluate(...)\n\nsynalinks.utils.plot_metrics(metrics)\n</code></pre> <p>Example:</p> <p></p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>The metrics from a program evaluation.</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"evaluation_metrics.png\".</p> <code>'evaluation_metrics.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Metrics\".</p> <code>'Metrics'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Evaluation metrics\".</p> <code>'Evaluation metrics'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.bar()</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_metrics\")\ndef plot_metrics(\n    metrics,\n    to_file=\"evaluation_metrics.png\",\n    to_folder=None,\n    xlabel=\"Metrics\",\n    ylabel=\"Scores\",\n    title=\"Evaluation metrics\",\n    grid=True,\n    metrics_filter=None,\n    **kwargs,\n):\n    \"\"\"Plots the evaluation metrics of a program and saves it to a file.\n\n    Code Example:\n\n    ```python\n    program.compile(...)\n    metrics = await program.evaluate(...)\n\n    synalinks.utils.plot_metrics(metrics)\n    ```\n\n    Example:\n\n    ![evaluation_metrics.png](../../assets/evaluation_metrics.png)\n\n    Args:\n        metrics (dict): The metrics from a program evaluation.\n        to_file (str): The file path where the plot will be saved.\n            Default to \"evaluation_metrics.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Metrics\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot. Default to \"Evaluation metrics\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.bar()`\n\n    Raises:\n        ValueError: If there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    all_metrics = list(metrics.keys())\n\n    if metrics_filter is not None:\n        if not all(metric in all_metrics for metric in metrics_filter):\n            raise ValueError(f\"Requested metrics {metrics_filter} not found in metrics\")\n        filtered_metrics = {k: v for k, v in metrics.items() if k in metrics_filter}\n    else:\n        filtered_metrics = metrics\n\n    metric_names = list(filtered_metrics.keys())\n    metric_values = list(filtered_metrics.values())\n\n    colors = generate_distinct_colors(len(metric_names))\n\n    plt.bar(metric_names, metric_values, color=colors, **kwargs)\n\n    if xlabel:\n        plt.xlabel(xlabel)\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    max_value = max(metric_values) if metric_values else 1.0\n    plt.ylim(0.0, max(1.0, max_value * 1.05))  # 5% padding above max value\n    plt.grid(grid)\n\n    # Rotate x-axis labels if there are many metrics\n    if len(metric_names) &gt; 5:\n        plt.xticks(rotation=45, ha=\"right\")\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_metrics.plot_metrics_comparison","title":"<code>plot_metrics_comparison(metrics_dict, to_file='evaluation_metrics_comparison.png', to_folder=None, xlabel='Metrics', ylabel='Scores', title='Metrics Comparison', grid=True, metrics_filter=None, bar_width=0.35, **kwargs)</code>","text":"<p>Plots comparison of evaluation metrics across different runs/models/conditions.</p> <p>Code Example:</p> <pre><code># Compare metrics from different models\nmetrics_comparison = {\n    \"Program A\": metrics_a,\n    \"Program B\": metrics_b,\n    \"Program C\": metrics_c,\n}\n\nsynalinks.utils.plot_metrics_comparison(metrics_comparison)\n</code></pre> <p></p> <p>Parameters:</p> Name Type Description Default <code>metrics_dict</code> <code>dict</code> <p>Dictionary where keys are condition names (e.g., model names) and values are metrics dictionaries. Format: {\"condition1\": {\"metric1\": value1, \"metric2\": value2}, ...}</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"evaluation_metrics_comparison.png\".</p> <code>'evaluation_metrics_comparison.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Metrics\".</p> <code>'Metrics'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Metrics Comparison\".</p> <code>'Metrics Comparison'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>bar_width</code> <code>float</code> <p>Width of the bars. Default to 0.35.</p> <code>0.35</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.bar()</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metrics_dict is empty, has inconsistent metric names, or if there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_metrics_comparison\")\ndef plot_metrics_comparison(\n    metrics_dict,\n    to_file=\"evaluation_metrics_comparison.png\",\n    to_folder=None,\n    xlabel=\"Metrics\",\n    ylabel=\"Scores\",\n    title=\"Metrics Comparison\",\n    grid=True,\n    metrics_filter=None,\n    bar_width=0.35,\n    **kwargs,\n):\n    \"\"\"Plots comparison of evaluation metrics across different runs/models/conditions.\n\n    Code Example:\n\n    ```python\n    # Compare metrics from different models\n    metrics_comparison = {\n        \"Program A\": metrics_a,\n        \"Program B\": metrics_b,\n        \"Program C\": metrics_c,\n    }\n\n    synalinks.utils.plot_metrics_comparison(metrics_comparison)\n    ```\n\n    ![evaluation_metrics_comparison.png](../../assets/evaluation_metrics_comparison.png)\n\n    Args:\n        metrics_dict (dict): Dictionary where keys are condition names (e.g., model names)\n            and values are metrics dictionaries. Format:\n            {\"condition1\": {\"metric1\": value1, \"metric2\": value2}, ...}\n        to_file (str): The file path where the plot will be saved.\n            Default to \"evaluation_metrics_comparison.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Metrics\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot. Default to \"Metrics Comparison\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        bar_width (float): Width of the bars. Default to 0.35.\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.bar()`\n\n    Raises:\n        ValueError: If metrics_dict is empty, has inconsistent metric names,\n            or if there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    if not metrics_dict:\n        raise ValueError(\"metrics_dict cannot be empty\")\n\n    # Get all metric names and validate consistency\n    condition_names = list(metrics_dict.keys())\n    all_metric_names = list(metrics_dict[condition_names[0]].keys())\n\n    # Validate that all conditions have the same metrics\n    for condition in condition_names:\n        if set(metrics_dict[condition].keys()) != set(all_metric_names):\n            raise ValueError(\n                f\"Condition '{condition}' has inconsistent metric names. \"\n                f\"Expected: {all_metric_names}, \"\n                f\"Got: {list(metrics_dict[condition].keys())}\"\n            )\n\n    if metrics_filter is not None:\n        if not all(metric in all_metric_names for metric in metrics_filter):\n            raise ValueError(f\"Requested metrics {metrics_filter} not found in metrics\")\n        metric_names = metrics_filter\n    else:\n        metric_names = all_metric_names\n\n    # Set up the plot\n    x = np.arange(len(metric_names))\n    num_conditions = len(condition_names)\n    colors = generate_distinct_colors(num_conditions)\n\n    # Calculate bar positions\n    bar_positions = []\n    for i in range(num_conditions):\n        pos = x + (i - num_conditions / 2 + 0.5) * bar_width\n        bar_positions.append(pos)\n\n    # Plot bars for each condition\n    for i, condition in enumerate(condition_names):\n        values = [metrics_dict[condition][metric] for metric in metric_names]\n        plt.bar(\n            bar_positions[i],\n            values,\n            bar_width,\n            label=condition,\n            color=colors[i],\n            **kwargs,\n        )\n\n    if xlabel:\n        plt.xlabel(xlabel)\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    all_values = [\n        metrics_dict[cond][metric] for cond in condition_names for metric in metric_names\n    ]\n    max_value = max(all_values) if all_values else 1.0\n    plt.ylim(0.0, max(1.0, max_value * 1.05))  # 5% padding above max value\n\n    plt.grid(grid)\n    plt.xticks(x, metric_names)\n    plt.legend()\n\n    # Rotate x-axis labels if there are many metrics\n    if len(metric_names) &gt; 5:\n        plt.xticks(x, metric_names, rotation=45, ha=\"right\")\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_metrics.plot_metrics_comparison_with_mean_and_std","title":"<code>plot_metrics_comparison_with_mean_and_std(metrics_comparison_dict, to_file='evaluation_metrics_comparison_with_mean_and_std.png', to_folder=None, xlabel='Metrics', ylabel='Scores', title='Metrics Comparison with Mean and Std', grid=True, show_values=False, capsize=5, metrics_filter=None, bar_width=0.35, **kwargs)</code>","text":"<p>Plots comparison of evaluation metrics with mean and standard deviation     across conditions.</p> <p>Calculates mean and standard deviation for each condition across multiple runs and displays them as grouped bar plots with error bars for comparison.</p> <p>Code Example: </p><pre><code># Compare metrics from different models with multiple runs each\nmetrics_comparison = {\n    \"Program A\": metrics_list_a,\n    \"Program B\": metrics_list_b\n}\n\nsynalinks.utils.plot_metrics_comparison_with_mean_and_std(\n    metrics_comparison,\n    show_values=True,\n)\n</code></pre><p></p> <p></p> <p>Parameters:</p> Name Type Description Default <code>metrics_comparison_dict</code> <code>dict</code> <p>Dictionary where keys are condition names and values are lists of metrics dictionaries. Format: {\"condition1\": [{\"metric1\": val, \"metric2\": val}, ...], ...}</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"evaluation_metrics_comparison_with_mean_and_std.png\".</p> <code>'evaluation_metrics_comparison_with_mean_and_std.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Metrics\".</p> <code>'Metrics'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Metrics Comparison with Mean and Std\".</p> <code>'Metrics Comparison with Mean and Std'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>show_values</code> <code>bool</code> <p>Whether to display mean values on top of bars (Default to False).</p> <code>False</code> <code>capsize</code> <code>float</code> <p>Size of the error bar caps. Default to 5.</p> <code>5</code> <code>metrics_filter</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>bar_width</code> <code>float</code> <p>Width of the bars. Default to 0.35.</p> <code>0.35</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.bar()</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metrics_comparison_dict is empty, has inconsistent structures, or if there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_metrics_comparison_with_mean_and_std\")\ndef plot_metrics_comparison_with_mean_and_std(\n    metrics_comparison_dict,\n    to_file=\"evaluation_metrics_comparison_with_mean_and_std.png\",\n    to_folder=None,\n    xlabel=\"Metrics\",\n    ylabel=\"Scores\",\n    title=\"Metrics Comparison with Mean and Std\",\n    grid=True,\n    show_values=False,\n    capsize=5,\n    metrics_filter=None,\n    bar_width=0.35,\n    **kwargs,\n):\n    \"\"\"Plots comparison of evaluation metrics with mean and standard deviation\n        across conditions.\n\n    Calculates mean and standard deviation for each condition across multiple runs and\n    displays them as grouped bar plots with error bars for comparison.\n\n    Code Example:\n    ```python\n    # Compare metrics from different models with multiple runs each\n    metrics_comparison = {\n        \"Program A\": metrics_list_a,\n        \"Program B\": metrics_list_b\n    }\n\n    synalinks.utils.plot_metrics_comparison_with_mean_and_std(\n        metrics_comparison,\n        show_values=True,\n    )\n    ```\n\n    ![evaluation_comparaison_with_mean_and_std.png](../../assets/evaluation_comparaison_with_mean_and_std.png)\n\n    Args:\n        metrics_comparison_dict (dict): Dictionary where keys are condition names and\n            values are lists of metrics dictionaries. Format:\n            {\"condition1\": [{\"metric1\": val, \"metric2\": val}, ...], ...}\n        to_file (str): The file path where the plot will be saved.\n            Default to \"evaluation_metrics_comparison_with_mean_and_std.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Metrics\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot.\n            Default to \"Metrics Comparison with Mean and Std\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        show_values (bool): Whether to display mean values on top of bars\n            (Default to False).\n        capsize (float): Size of the error bar caps. Default to 5.\n        metrics_filter (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        bar_width (float): Width of the bars. Default to 0.35.\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.bar()`\n\n    Raises:\n        ValueError: If metrics_comparison_dict is empty, has inconsistent structures,\n            or if there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    if not metrics_comparison_dict:\n        raise ValueError(\"metrics_comparison_dict cannot be empty\")\n\n    condition_names = list(metrics_comparison_dict.keys())\n\n    # Validate structure and get metric names\n    for condition in condition_names:\n        if not isinstance(metrics_comparison_dict[condition], list):\n            raise ValueError(\n                f\"Values for condition '{condition}' must be a list of\"\n                \" metric dictionaries\"\n            )\n        if not metrics_comparison_dict[condition]:\n            raise ValueError(f\"Metrics list for condition '{condition}' cannot be empty\")\n\n    # Get metric names from first condition's first run\n    all_metric_names = list(metrics_comparison_dict[condition_names[0]][0].keys())\n\n    # Validate consistency across all conditions and runs\n    for condition in condition_names:\n        for i, metrics_dict in enumerate(metrics_comparison_dict[condition]):\n            if not isinstance(metrics_dict, dict):\n                raise ValueError(\n                    f\"Entry {i} for condition '{condition}' is not a dictionary\"\n                )\n            if set(metrics_dict.keys()) != set(all_metric_names):\n                raise ValueError(\n                    f\"Entry {i} for condition '{condition}' has inconsistent\"\n                    \" metric names. \"\n                    f\"Expected: {all_metric_names}, Got: {list(metrics_dict.keys())}\"\n                )\n\n    if metrics_filter is not None:\n        if not all(metric in all_metric_names for metric in metrics_filter):\n            raise ValueError(f\"Requested metrics {metrics_filter} not found in metrics\")\n        metric_names = metrics_filter\n    else:\n        metric_names = all_metric_names\n\n    # Calculate means and stds for each condition\n    condition_stats = {}\n    for condition in condition_names:\n        means = []\n        stds = []\n        for metric_name in metric_names:\n            values = [\n                metrics_dict[metric_name]\n                for metrics_dict in metrics_comparison_dict[condition]\n            ]\n            means.append(np.mean(values))\n            stds.append(np.std(values, ddof=1) if len(values) &gt; 1 else 0.0)\n        condition_stats[condition] = {\"means\": means, \"stds\": stds}\n\n    # Set up the plot\n    x = np.arange(len(metric_names))\n    num_conditions = len(condition_names)\n    colors = generate_distinct_colors(num_conditions)\n\n    # Calculate bar positions\n    bar_positions = []\n    for i in range(num_conditions):\n        pos = x + (i - num_conditions / 2 + 0.5) * bar_width\n        bar_positions.append(pos)\n\n    # Plot bars for each condition\n    bars_list = []\n    for i, condition in enumerate(condition_names):\n        means = condition_stats[condition][\"means\"]\n        stds = condition_stats[condition][\"stds\"]\n\n        bars = plt.bar(\n            bar_positions[i],\n            means,\n            bar_width,\n            yerr=stds,\n            label=condition,\n            color=colors[i],\n            capsize=capsize,\n            **kwargs,\n        )\n        bars_list.append(bars)\n\n        # Add value labels on top of bars if requested\n        if show_values:\n            for j, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n                height = bar.get_height()\n                plt.text(\n                    bar.get_x() + bar.get_width() / 2.0,\n                    height + std + 0.01,\n                    f\"{mean:.3f}\u00b1{std:.3f}\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                    fontsize=8,\n                    rotation=90 if num_conditions &gt; 2 else 0,\n                )\n\n    if xlabel:\n        plt.xlabel(xlabel)\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    all_means = [\n        mean\n        for condition in condition_names\n        for mean in condition_stats[condition][\"means\"]\n    ]\n    all_stds = [\n        std for condition in condition_names for std in condition_stats[condition][\"stds\"]\n    ]\n    max_val = max(np.array(all_means) + np.array(all_stds)) if all_means else 1.0\n    y_padding = 0.15 if show_values else 0.05\n    plt.ylim(0.0, max(1.0, max_val + y_padding))\n\n    plt.grid(grid)\n    plt.xticks(x, metric_names)\n    plt.legend()\n\n    # Rotate x-axis labels if there are many metrics\n    if len(metric_names) &gt; 5:\n        plt.xticks(x, metric_names, rotation=45, ha=\"right\")\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/More%20plotting%20utilities/#synalinks.src.utils.plot_metrics.plot_metrics_with_mean_and_std","title":"<code>plot_metrics_with_mean_and_std(metrics_list, to_file='evaluation_metrics_with_mean_and_std.png', to_folder=None, xlabel='Metrics', ylabel='Scores', title='Evaluation metrics with mean and std', grid=True, show_values=False, capsize=5, metrics=None, **kwargs)</code>","text":"<p>Plots the evaluation metrics with mean and standard deviation error bars.</p> <p>Calculates mean and standard deviation across multiple evaluation runs and displays them as bar plots with error bars.</p> <p>Code Example: </p><pre><code>program.compile(...)\nmetrics_list = []\nfor i in range(5):  # Multiple evaluation runs\n    metrics = await program.evaluate(...)\n    metrics_list.append(metrics)\n\nsynalinks.utils.plot_metrics_with_mean_and_std(metrics_list)\n</code></pre><p></p> <p>Example:</p> <p></p> <p>Parameters:</p> Name Type Description Default <code>metrics_list</code> <code>list[dict]</code> <p>List of metrics dictionaries from multiple program evaluations. Each dict should have format: {'metric_name': float_value, ...}</p> required <code>to_file</code> <code>str</code> <p>The file path where the plot will be saved. Default to \"evaluation_metrics_with_mean_and_std.png\".</p> <code>'evaluation_metrics_with_mean_and_std.png'</code> <code>to_folder</code> <code>str</code> <p>The folder where the plot will be saved. If provided, will be combined with to_file.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>Optional. The label for the x-axis. Default to \"Metrics\".</p> <code>'Metrics'</code> <code>ylabel</code> <code>str</code> <p>Optional. The label for the y-axis. Default to \"Scores\".</p> <code>'Scores'</code> <code>title</code> <code>str</code> <p>Optional. The title of the plot. Default to \"Evaluation metrics with mean and std\".</p> <code>'Evaluation metrics with mean and std'</code> <code>grid</code> <code>bool</code> <p>Whether to display the grid on the plot. Default to True.</p> <code>True</code> <code>show_values</code> <code>bool</code> <p>Whether to display mean values on top of bars (Default to True).</p> <code>False</code> <code>capsize</code> <code>float</code> <p>Size of the error bar caps. Default to 5.</p> <code>5</code> <code>metrics</code> <code>list</code> <p>List of specific metrics to plot. If None, all metrics will be plotted.</p> <code>None</code> <code>**kwargs</code> <code>keyword arguments</code> <p>Additional keyword arguments forwarded to <code>plt.bar()</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metrics_list is empty, not a list, contains inconsistent metric names, or if there are unrecognized keyword arguments.</p> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image has been saved.</p> Source code in <code>synalinks/src/utils/plot_metrics.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_metrics_with_mean_and_std\")\ndef plot_metrics_with_mean_and_std(\n    metrics_list,\n    to_file=\"evaluation_metrics_with_mean_and_std.png\",\n    to_folder=None,\n    xlabel=\"Metrics\",\n    ylabel=\"Scores\",\n    title=\"Evaluation metrics with mean and std\",\n    grid=True,\n    show_values=False,\n    capsize=5,\n    metrics=None,\n    **kwargs,\n):\n    \"\"\"Plots the evaluation metrics with mean and standard deviation error bars.\n\n    Calculates mean and standard deviation across multiple evaluation runs and\n    displays them as bar plots with error bars.\n\n    Code Example:\n    ```python\n    program.compile(...)\n    metrics_list = []\n    for i in range(5):  # Multiple evaluation runs\n        metrics = await program.evaluate(...)\n        metrics_list.append(metrics)\n\n    synalinks.utils.plot_metrics_with_mean_and_std(metrics_list)\n    ```\n\n    Example:\n\n    ![evaluation_metrics_with_mean_and_std.png](../../assets/evaluation_metrics_with_mean_and_std.png)\n\n    Args:\n        metrics_list (list[dict]): List of metrics dictionaries from multiple\n            program evaluations. Each dict should have format:\n            {'metric_name': float_value, ...}\n        to_file (str): The file path where the plot will be saved.\n            Default to \"evaluation_metrics_with_mean_and_std.png\".\n        to_folder (str, optional): The folder where the plot will be saved.\n            If provided, will be combined with to_file.\n        xlabel (str): Optional. The label for the x-axis. Default to \"Metrics\".\n        ylabel (str): Optional. The label for the y-axis. Default to \"Scores\".\n        title (str): Optional. The title of the plot.\n            Default to \"Evaluation metrics with mean and std\".\n        grid (bool): Whether to display the grid on the plot. Default to True.\n        show_values (bool): Whether to display mean values on top of bars\n            (Default to True).\n        capsize (float): Size of the error bar caps. Default to 5.\n        metrics (list, optional): List of specific metrics to plot.\n            If None, all metrics will be plotted.\n        **kwargs (keyword arguments): Additional keyword arguments\n            forwarded to `plt.bar()`\n\n    Raises:\n        ValueError: If metrics_list is empty, not a list, contains inconsistent\n            metric names, or if there are unrecognized keyword arguments.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image has been saved.\n    \"\"\"\n    if not metrics_list:\n        raise ValueError(\"metrics_list cannot be empty\")\n\n    if not isinstance(metrics_list, list):\n        raise ValueError(\"metrics_list must be a list of metric dictionaries\")\n\n    # Get metric names from first run and validate consistency\n    all_metric_names = list(metrics_list[0].keys())\n\n    if metrics is not None:\n        if not all(metric in all_metric_names for metric in metrics):\n            raise ValueError(f\"Requested metrics {metrics} not found in metrics_list\")\n        metric_names = metrics\n    else:\n        metric_names = all_metric_names\n\n    for i, metrics_dict in enumerate(metrics_list):\n        if not isinstance(metrics_dict, dict):\n            raise ValueError(f\"Entry {i} in metrics_list is not a dictionary\")\n        if set(metrics_dict.keys()) != set(all_metric_names):\n            raise ValueError(\n                f\"Entry {i} has inconsistent metric names. \"\n                f\"Expected: {all_metric_names}, Got: {list(metrics_dict.keys())}\"\n            )\n\n    # Calculate mean and std for each metric\n    means = []\n    stds = []\n\n    for metric_name in metric_names:\n        values = [metrics_dict[metric_name] for metrics_dict in metrics_list]\n        means.append(np.mean(values))\n        stds.append(np.std(values, ddof=1) if len(values) &gt; 1 else 0.0)\n\n    colors = generate_distinct_colors(len(metric_names))\n\n    # Create bar plot with error bars\n    bars = plt.bar(\n        metric_names, means, yerr=stds, color=colors, capsize=capsize, **kwargs\n    )\n\n    # Add value labels on top of bars if requested\n    if show_values:\n        for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n            height = bar.get_height()\n            plt.text(\n                bar.get_x() + bar.get_width() / 2.0,\n                height + std + 0.01,\n                f\"{mean:.3f}\u00b1{std:.3f}\",\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=9,\n            )\n\n    if xlabel:\n        plt.xlabel(xlabel)\n    if ylabel:\n        plt.ylabel(ylabel)\n    if title:\n        plt.title(title)\n\n    # Set y-axis limits: minimum 0.0, maximum 1.0 but allow exceeding if needed\n    max_val = max(np.array(means) + np.array(stds)) if means else 1.0\n    y_padding = 0.1 if show_values else 0.05\n    plt.ylim(0.0, max(1.0, max_val + y_padding))\n\n    plt.grid(grid)\n\n    # Rotate x-axis labels if there are many metrics\n    if len(metric_names) &gt; 5:\n        plt.xticks(rotation=45, ha=\"right\")\n\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n\n    plt.savefig(to_file, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    try:\n        import marimo as mo\n\n        if mo.running_in_notebook():\n            return mo.image(src=to_file).center()\n    except ImportError:\n        pass\n\n    try:\n        from IPython import display\n\n        return display.Image(filename=to_file)\n    except ImportError:\n        pass\n\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/","title":"NLP utilities","text":""},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.add_suffix","title":"<code>add_suffix(property_key, suffix)</code>","text":"<p>Add a suffix to a property key.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to process.</p> required <code>suffix</code> <code>int</code> <p>The suffix to add.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The property key with the suffix added.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def add_suffix(property_key, suffix):\n    \"\"\"\n    Add a suffix to a property key.\n\n    Args:\n        property_key (str): The property key to process.\n        suffix (int): The suffix to add.\n\n    Returns:\n        (str): The property key with the suffix added.\n    \"\"\"\n    return f\"{property_key}_{suffix}\"\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.is_plural","title":"<code>is_plural(property_key)</code>","text":"<p>Check if the last word of a property key is in plural form.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the last word is plural, False otherwise.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def is_plural(property_key):\n    \"\"\"\n    Check if the last word of a property key is in plural form.\n\n    Args:\n        property_key (str): The property key to check.\n\n    Returns:\n        (bool): True if the last word is plural, False otherwise.\n    \"\"\"\n    words = property_key.split(\"_\")\n    if len(words) &gt; 1:\n        noun = words[-1]\n    else:\n        noun = words[0]\n\n    singular_form = to_singular(noun)\n    return singular_form != noun\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.normalize_and_tokenize","title":"<code>normalize_and_tokenize(text)</code>","text":"<p>Normalize the text and tokenize it into words.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to process.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of normalized words.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def normalize_and_tokenize(text):\n    \"\"\"\n    Normalize the text and tokenize it into words.\n\n    Args:\n        text (str): The text to process.\n\n    Returns:\n        (list): A list of normalized words.\n    \"\"\"\n    text = text.lower()\n    text = remove_articles(text)\n    text = remove_punctuation(text)\n    return text.split()\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.normalize_text","title":"<code>normalize_text(text)</code>","text":"<p>Normalize the text by converting to lowercase, removing articles,     and removing punctuation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized text.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def normalize_text(text):\n    \"\"\"\n    Normalize the text by converting to lowercase, removing articles,\n        and removing punctuation.\n\n    Args:\n        text (str): The text to normalize.\n\n    Returns:\n        (str): The normalized text.\n    \"\"\"\n    return remove_articles(remove_punctuation(text.strip().lower()))\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.remove_articles","title":"<code>remove_articles(text)</code>","text":"<p>Remove common English articles from the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to process.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text with articles removed.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def remove_articles(text):\n    \"\"\"\n    Remove common English articles from the text.\n\n    Args:\n        text (str): The text to process.\n\n    Returns:\n        (str): The text with articles removed.\n    \"\"\"\n    return \" \".join(re.sub(ARTICLE_REGEX, \"\", text).split())\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.remove_numerical_suffix","title":"<code>remove_numerical_suffix(property_key)</code>","text":"<p>Remove the numerical suffix from a property key.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to process.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The property key with the suffix removed.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def remove_numerical_suffix(property_key):\n    \"\"\"\n    Remove the numerical suffix from a property key.\n\n    Args:\n        property_key (str): The property key to process.\n\n    Returns:\n        (str): The property key with the suffix removed.\n    \"\"\"\n    return re.sub(SUFFIX_PATTERN, \"\", property_key)\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.remove_punctuation","title":"<code>remove_punctuation(text)</code>","text":"<p>Remove punctuation from the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to process.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text with punctuation removed.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def remove_punctuation(text):\n    \"\"\"\n    Remove punctuation from the text.\n\n    Args:\n        text (str): The text to process.\n\n    Returns:\n        (str): The text with punctuation removed.\n    \"\"\"\n    return text.translate(PUNCTUATION_TRANSLATOR)\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.shorten_text","title":"<code>shorten_text(text, nb_words_offset=10)</code>","text":"<p>Shorten a text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to shorten.</p> required <code>nb_words_offset</code> <code>int</code> <p>The number of words to keep from the beginning and end of the text (Default is 20).</p> <code>10</code> <p>Returns:</p> Type Description <code>str</code> <p>The shortened text. If the original text has more than nb_words words, returns the first nb_words and last nb_words separated by \" (...) \". Otherwise, returns the original text unchanged.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def shorten_text(text, nb_words_offset=10):\n    \"\"\"\n    Shorten a text.\n\n    Args:\n        text (str): The text to shorten.\n        nb_words_offset (int): The number of words to keep\n            from the beginning and end of the text\n            (Default is 20).\n\n    Returns:\n        (str): The shortened text. If the original text has more than nb_words words,\n            returns the first nb_words and last nb_words separated by \" (...) \".\n            Otherwise, returns the original text unchanged.\n    \"\"\"\n    if not isinstance(text, str):\n        text = str(text)\n\n    words = text.split(\" \")\n    if len(words) &lt;= nb_words_offset * 2:\n        return text\n    nb_words_removed = len(words) - 2 * nb_words_offset\n    short_text = (\n        \" \".join(words[:nb_words_offset])\n        + f\" (... {nb_words_removed} words removed for clarity) \"\n        + \" \".join(words[-nb_words_offset:])\n    )\n    return short_text\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.to_plural","title":"<code>to_plural(word)</code>","text":"<p>Convert a singular word to its plural form.</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>The singular word to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The plural form of the word.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def to_plural(word):\n    \"\"\"\n    Convert a singular word to its plural form.\n\n    Args:\n        word (str): The singular word to convert.\n\n    Returns:\n        (str): The plural form of the word.\n    \"\"\"\n    if word in IRREGULAR_PLURALS:\n        return IRREGULAR_PLURALS.get(word)\n    else:\n        # Use rules for regular plurals\n        if Y_ENDING.search(word):\n            return f\"{word[:-1]}ies\"\n        elif S_ENDING.search(word) or SH_CH_ENDING.search(word):\n            return f\"{word}es\"\n        else:\n            return f\"{word}s\"\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.to_plural_property","title":"<code>to_plural_property(property_key)</code>","text":"<p>Convert the last word of a property key to its plural form.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The property key with the last word in plural form.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def to_plural_property(property_key):\n    \"\"\"\n    Convert the last word of a property key to its plural form.\n\n    Args:\n        property_key (str): The property key to convert.\n\n    Returns:\n        (str): The property key with the last word in plural form.\n    \"\"\"\n    words = property_key.split(\"_\")\n    if len(words) &gt; 1:\n        # Assume the last word is the noun\n        words[-1] = to_plural(words[-1])\n    else:\n        words[0] = to_plural(words[0])\n    return \"_\".join(words)\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.to_plural_without_numerical_suffix","title":"<code>to_plural_without_numerical_suffix(property_key)</code>","text":"<p>Convert a property key to its list (plural) form by removing     the numerical suffix and converting to plural.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The list (plural) form of the property key.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def to_plural_without_numerical_suffix(property_key):\n    \"\"\"\n    Convert a property key to its list (plural) form by removing\n        the numerical suffix and converting to plural.\n\n    Args:\n        property_key (str): The property key to convert.\n\n    Returns:\n        (str): The list (plural) form of the property key.\n    \"\"\"\n    property_key = remove_numerical_suffix(property_key)\n    return to_plural_property(property_key)\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.to_singular","title":"<code>to_singular(word)</code>","text":"<p>Convert a plural word to its singular form.</p> <p>Parameters:</p> Name Type Description Default <code>word</code> <code>str</code> <p>The plural word to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The singular form of the word.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def to_singular(word):\n    \"\"\"\n    Convert a plural word to its singular form.\n\n    Args:\n        word (str): The plural word to convert.\n\n    Returns:\n        (str): The singular form of the word.\n    \"\"\"\n    if word in IRREGULAR_SINGULARS:\n        return IRREGULAR_SINGULARS.get(word)\n    else:\n        # Use rules for regular singulars\n        if IES_ENDING.search(word):\n            return f\"{word[:-3]}y\"\n        elif ES_ENDING.search(word):\n            if S_ENDING.search(word[:-2]) or SH_CH_ENDING.search(word[:-2]):\n                return word[:-2]\n            else:\n                return word[:-1]\n        elif word.endswith(\"s\"):\n            return word[:-1]\n        else:\n            return word\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.to_singular_property","title":"<code>to_singular_property(property_key)</code>","text":"<p>Convert the last word of a property key to its singular form.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The property key with the last word in singular form.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def to_singular_property(property_key):\n    \"\"\"\n    Convert the last word of a property key to its singular form.\n\n    Args:\n        property_key (str): The property key to convert.\n\n    Returns:\n        (str): The property key with the last word in singular form.\n    \"\"\"\n    words = property_key.split(\"_\")\n    if len(words) &gt; 1:\n        # Assume the last word is the noun\n        words[-1] = to_singular(words[-1])\n    else:\n        words[0] = to_singular(words[0])\n    return \"_\".join(words)\n</code></pre>"},{"location":"Synalinks%20API/Utilities/NLP%20utilities/#synalinks.src.utils.nlp_utils.to_singular_without_numerical_suffix","title":"<code>to_singular_without_numerical_suffix(property_key)</code>","text":"<p>Convert a property key to its base (singular) form by removing     the numerical suffix and converting to singular.</p> <p>Parameters:</p> Name Type Description Default <code>property_key</code> <code>str</code> <p>The property key to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The base (singular) form of the property key.</p> Source code in <code>synalinks/src/utils/nlp_utils.py</code> <pre><code>def to_singular_without_numerical_suffix(property_key):\n    \"\"\"\n    Convert a property key to its base (singular) form by removing\n        the numerical suffix and converting to singular.\n\n    Args:\n        property_key (str): The property key to convert.\n\n    Returns:\n        (str): The base (singular) form of the property key.\n    \"\"\"\n    property_key = remove_numerical_suffix(property_key)\n    return to_singular_property(property_key)\n</code></pre>"},{"location":"Synalinks%20API/Utilities/Program%20plotting%20utilities/","title":"Program plotting utilities","text":""},{"location":"Synalinks%20API/Utilities/Program%20plotting%20utilities/#synalinks.src.utils.program_visualization.plot_program","title":"<code>plot_program(program, to_file=None, to_folder=None, show_schemas=False, show_module_names=False, show_defs=False, rankdir='TB', expand_nested=False, dpi=200, show_trainable=False, **kwargs)</code>","text":"<p>Converts a Synalinks program to dot format and save to a file.</p> <p>Code example:</p> <pre><code>inputs = ...\noutputs = ...\nprogram = synalinks.Program(\n    inputs=inputs,\n    outputs=outputs,\n)\n\nsynalinks.utils.plot_program(\n    program,\n    to_file=\"program_1.png\",\n    to_folder=\"/tmp\",\n    show_schemas=True,\n    show_defs=True,\n    show_trainable=True,\n)\n</code></pre> <p>Example:</p> <p></p> <p>Parameters:</p> Name Type Description Default <code>program</code> <code>Program</code> <p>A Synalinks program instance</p> required <code>to_file</code> <code>str | None</code> <p>Optional. File name of the plot image.</p> <code>None</code> <code>show_schemas</code> <code>bool</code> <p>whether to display schema information.</p> <code>False</code> <code>show_defs</code> <code>bool</code> <p>whether to display defs schema information.</p> <code>False</code> <code>show_module_names</code> <code>bool</code> <p>whether to display module names.</p> <code>False</code> <code>rankdir</code> <code>str</code> <p><code>rankdir</code> argument passed to PyDot, a string specifying the format of the plot: <code>\"TB\"</code> creates a vertical plot; <code>\"LR\"</code> creates a horizontal plot.</p> <code>'TB'</code> <code>expand_nested</code> <code>bool</code> <p>whether to expand nested Functional programs into clusters.</p> <code>False</code> <code>dpi</code> <code>int</code> <p>Image resolution in dots per inch.</p> <code>200</code> <code>show_trainable</code> <code>bool</code> <p>whether to display if a module is trainable.</p> <code>False</code> <p>Returns:</p> Type Description <code>Image | Image | str</code> <p>If running in a Jupyter notebook, returns an IPython Image object for inline display. If running in a Marimo notebook returns a marimo image. Otherwise returns the filepath where the image have been saved.</p> Source code in <code>synalinks/src/utils/program_visualization.py</code> <pre><code>@synalinks_export(\"synalinks.utils.plot_program\")\ndef plot_program(\n    program,\n    to_file=None,\n    to_folder=None,\n    show_schemas=False,\n    show_module_names=False,\n    show_defs=False,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=200,\n    show_trainable=False,\n    **kwargs,\n):\n    \"\"\"Converts a Synalinks program to dot format and save to a file.\n\n    Code example:\n\n    ```python\n    inputs = ...\n    outputs = ...\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n    )\n\n    synalinks.utils.plot_program(\n        program,\n        to_file=\"program_1.png\",\n        to_folder=\"/tmp\",\n        show_schemas=True,\n        show_defs=True,\n        show_trainable=True,\n    )\n    ```\n\n    Example:\n\n    ![chain_of_thought.png](../../assets/chain_of_thought.png)\n\n    Args:\n        program (Program): A Synalinks program instance\n        to_file (str | None): Optional. File name of the plot image.\n        show_schemas (bool): whether to display schema information.\n        show_defs (bool): whether to display defs schema information.\n        show_module_names (bool): whether to display module names.\n        rankdir (str): `rankdir` argument passed to PyDot,\n            a string specifying the format of the plot: `\"TB\"`\n            creates a vertical plot; `\"LR\"` creates a horizontal plot.\n        expand_nested (bool): whether to expand nested Functional programs\n            into clusters.\n        dpi (int): Image resolution in dots per inch.\n        show_trainable (bool): whether to display if a module is trainable.\n\n    Returns:\n        (IPython.display.Image | marimo.Image | str):\n            If running in a Jupyter notebook, returns an IPython Image object\n            for inline display. If running in a Marimo notebook returns a marimo image.\n            Otherwise returns the filepath where the image have been saved.\n    \"\"\"\n\n    if not to_file:\n        to_file = f\"{program.name}.png\"\n\n    if not program.built:\n        raise ValueError(\n            \"This program has not yet been built. \"\n            \"Build the program first by calling `build()` or by calling \"\n            \"the program on a batch of data.\"\n        )\n    if not check_pydot():\n        message = (\n            \"You must install pydot (`pip install pydot`) for `plot_program` to work.\"\n        )\n        if \"IPython.core.magics.namespace\" in sys.modules:\n            # We don't raise an exception here in order to avoid crashing\n            # notebook tests where graphviz is not available.\n            io_utils.print_msg(message)\n            return\n        else:\n            raise ImportError(message)\n    if not check_graphviz():\n        message = (\n            \"You must install graphviz \"\n            \"(see instructions at https://graphviz.gitlab.io/download/) \"\n            \"for `plot_program` to work.\"\n        )\n        if \"IPython.core.magics.namespace\" in sys.modules:\n            # We don't raise an exception here in order to avoid crashing\n            # notebook tests where graphviz is not available.\n            io_utils.print_msg(message)\n            return\n        else:\n            raise ImportError(message)\n\n    if kwargs:\n        raise ValueError(f\"Unrecognized keyword arguments: {kwargs}\")\n\n    dot = program_to_dot(\n        program,\n        show_schemas=show_schemas,\n        show_defs=show_defs,\n        show_module_names=show_module_names,\n        rankdir=rankdir,\n        expand_nested=expand_nested,\n        dpi=dpi,\n        show_trainable=show_trainable,\n    )\n    to_file = str(to_file)\n    if dot is None:\n        return\n    dot = remove_unused_edges(dot)\n    _, extension = os.path.splitext(to_file)\n    if to_folder:\n        to_file = os.path.join(to_folder, to_file)\n    if not extension:\n        extension = \"png\"\n    else:\n        extension = extension[1:]\n    # Save image to disk.\n    dot.write(to_file, format=extension)\n    # Return the image as a Jupyter Image object or Marimo Image object, to be\n    # displayed in-line. Note that we cannot easily detect whether the code is\n    # running in a Jupyter notebook, and thus we always return the Image if\n    # Jupyter is available.\n    if extension != \"pdf\":\n        try:\n            import marimo as mo\n\n            if mo.running_in_notebook():\n                return mo.image(src=to_file).center()\n        except ImportError:\n            pass\n        try:\n            from IPython import display\n\n            return display.Image(filename=to_file)\n        except ImportError:\n            pass\n    else:\n        try:\n            import marimo as mo\n\n            if mo.running_in_notebook():\n                return mo.pdf(src=to_file)\n        except ImportError:\n            pass\n    return to_file\n</code></pre>"},{"location":"Synalinks%20API/Utilities/Program%20plotting%20utilities/#synalinks.src.utils.program_visualization.program_to_dot","title":"<code>program_to_dot(program, show_schemas=False, show_defs=False, show_module_names=True, rankdir='TB', expand_nested=False, dpi=200, subgraph=False, show_trainable=False, **kwargs)</code>","text":"<p>Convert a Synalinks program to dot format.</p> <p>Parameters:</p> Name Type Description Default <code>program</code> <code>Program</code> <p>A Synalinks program instance.</p> required <code>show_schemas</code> <code>bool</code> <p>whether to display schema information.</p> <code>False</code> <code>show_defs</code> <code>bool</code> <p>whether to display schema defs information.</p> <code>False</code> <code>show_module_names</code> <code>bool</code> <p>whether to display module names.</p> <code>True</code> <code>rankdir</code> <code>str</code> <p><code>rankdir</code> argument passed to PyDot, a string specifying the format of the plot: <code>\"TB\"</code> creates a vertical plot; <code>\"LR\"</code> creates a horizontal plot.</p> <code>'TB'</code> <code>expand_nested</code> <code>bool</code> <p>whether to expand nested Functional programs into clusters.</p> <code>False</code> <code>dpi</code> <code>int</code> <p>Image resolution in dots per inch.</p> <code>200</code> <code>subgraph</code> <code>bool</code> <p>whether to return a <code>pydot.Cluster</code> instance.</p> <code>False</code> <code>show_trainable</code> <code>bool</code> <p>whether to display if a module is trainable.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dot | Cluster</code> <p>A <code>pydot.Dot</code> instance representing the program or a <code>pydot.Cluster</code> instance representing nested program if <code>subgraph=True</code>.</p> Source code in <code>synalinks/src/utils/program_visualization.py</code> <pre><code>@synalinks_export(\"synalinks.utils.program_to_dot\")\ndef program_to_dot(\n    program,\n    show_schemas=False,\n    show_defs=False,\n    show_module_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=200,\n    subgraph=False,\n    show_trainable=False,\n    **kwargs,\n):\n    \"\"\"Convert a Synalinks program to dot format.\n\n    Args:\n        program (Program): A Synalinks program instance.\n        show_schemas (bool): whether to display schema information.\n        show_defs (bool): whether to display schema defs information.\n        show_module_names (bool): whether to display module names.\n        rankdir (str): `rankdir` argument passed to PyDot,\n            a string specifying the format of the plot: `\"TB\"`\n            creates a vertical plot; `\"LR\"` creates a horizontal plot.\n        expand_nested (bool): whether to expand nested Functional programs\n            into clusters.\n        dpi (int): Image resolution in dots per inch.\n        subgraph (bool): whether to return a `pydot.Cluster` instance.\n        show_trainable (bool): whether to display if a module is trainable.\n\n    Returns:\n        (pydot.Dot | pydot.Cluster): A `pydot.Dot` instance representing the\n            program or a `pydot.Cluster` instance representing\n            nested program if `subgraph=True`.\n    \"\"\"\n    from synalinks.src.ops.function import make_node_key\n\n    if not program.built:\n        raise ValueError(\n            \"This program has not yet been built. \"\n            \"Build the program first by calling `build()` or by calling \"\n            \"the program on a batch of data.\"\n        )\n\n    from synalinks.src.programs import functional\n    from synalinks.src.programs import sequential\n\n    if not check_pydot():\n        raise ImportError(\n            \"You must install pydot (`pip install pydot`) for program_to_dot to work.\"\n        )\n\n    if subgraph:\n        dot = pydot.Cluster(style=\"dashed\", graph_name=program.name)\n        dot.set(\"label\", program.name)\n        dot.set(\"labeljust\", \"l\")\n    else:\n        dot = pydot.Dot()\n        dot.set(\"rankdir\", rankdir)\n        dot.set(\"concentrate\", True)\n        dot.set(\"dpi\", dpi)\n        dot.set(\"splines\", \"ortho\")\n        dot.set_node_defaults(schema=\"record\")\n\n    if kwargs.pop(\"module_range\", None) is not None:\n        raise ValueError(\"Argument `module_range` is no longer supported.\")\n    if kwargs:\n        raise ValueError(f\"Unrecognized keyword arguments: {kwargs}\")\n\n    kwargs = {\n        \"show_module_names\": show_module_names,\n        \"show_schemas\": show_schemas,\n        \"show_defs\": show_defs,\n        \"show_trainable\": show_trainable,\n    }\n\n    if isinstance(program, sequential.Sequential):\n        modules = program.modules\n    elif not isinstance(program, functional.Functional):\n        # We treat subclassed programs as a single node.\n        node = make_node(program, **kwargs)\n        dot.add_node(node)\n        return dot\n    else:\n        modules = program._operations\n\n    # Create graph nodes.\n    sub_n_first_node = {}\n    sub_n_last_node = {}\n    for i, module in enumerate(modules):\n        # Process nested functional programs.\n        if expand_nested and isinstance(module, functional.Functional):\n            subprogram = program_to_dot(\n                module,\n                show_schemas,\n                show_module_names,\n                rankdir,\n                expand_nested,\n                subgraph=True,\n                show_trainable=show_trainable,\n            )\n            # sub_n : subprogram\n            sub_n_nodes = subprogram.get_nodes()\n            sub_n_first_node[module.name] = sub_n_nodes[0]\n            sub_n_last_node[module.name] = sub_n_nodes[-1]\n            dot.add_subgraph(subprogram)\n\n        else:\n            node = make_node(module, **kwargs)\n            dot.add_node(node)\n\n    # Connect nodes with edges.\n    # Sequential case.\n    if isinstance(program, sequential.Sequential):\n        for i in range(len(modules) - 1):\n            inbound_module_id = str(id(modules[i]))\n            module_id = str(id(modules[i + 1]))\n            add_edge(dot, inbound_module_id, module_id)\n        return dot\n\n    # Functional case.\n    for i, module in enumerate(modules):\n        module_id = str(id(module))\n        for i, node in enumerate(module._inbound_nodes):\n            node_key = make_node_key(module, i)\n            if node_key in program._nodes:\n                for parent_node in node.parent_nodes:\n                    inbound_module = parent_node.operation\n                    inbound_module_id = str(id(inbound_module))\n                    if not expand_nested:\n                        assert dot.get_node(inbound_module_id)\n                        assert dot.get_node(module_id)\n                        add_edge(dot, inbound_module_id, module_id)\n                    else:\n                        # if inbound_module is not Functional\n                        if not isinstance(inbound_module, functional.Functional):\n                            # if current module is not Functional\n                            if not isinstance(module, functional.Functional):\n                                assert dot.get_node(inbound_module_id)\n                                assert dot.get_node(module_id)\n                                add_edge(dot, inbound_module_id, module_id)\n                            # if current module is Functional\n                            elif isinstance(module, functional.Functional):\n                                add_edge(\n                                    dot,\n                                    inbound_module_id,\n                                    sub_n_first_node[module.name].get_name(),\n                                )\n                        # if inbound_module is Functional\n                        elif isinstance(inbound_module, functional.Functional):\n                            name = sub_n_last_node[inbound_module.name].get_name()\n                            if isinstance(module, functional.Functional):\n                                output_name = sub_n_first_node[module.name].get_name()\n                                add_edge(dot, name, output_name)\n                            else:\n                                add_edge(dot, name, module_id)\n    return dot\n</code></pre>"},{"location":"guides/Agents/","title":"Agents","text":""},{"location":"guides/Agents/#guides.5_agents--agents","title":"Agents","text":"<p>Agents represent a paradigm shift from simple LLM calls to autonomous systems that can reason, plan, and take action. While a Generator produces a single output, an Agent iteratively thinks, selects tools, executes them, and uses the results to inform its next steps - continuing until it achieves its goal or reaches a limit.</p>"},{"location":"guides/Agents/#guides.5_agents--the-agent-loop","title":"The Agent Loop","text":"<p>The <code>FunctionCallingAgent</code> uses an internal ChainOfThought module to reason about which tools to call. Here's the complete autonomous loop:</p> <pre><code>flowchart TD\n    A[Input + Trajectory] --&gt; B[ChainOfThought]\n    B --&gt; C[thinking + tool_calls]\n    C --&gt; D{tool_calls empty?}\n    D --&gt;|Yes| E[Final Generator]\n    E --&gt; F[Formatted Output]\n    D --&gt;|No| G[Execute Tools in Parallel]\n    G --&gt; H[Append Results to Trajectory]\n    H --&gt; I{max_iterations?}\n    I --&gt;|No| A\n    I --&gt;|Yes| E</code></pre> <p>At each iteration, the agent:</p> <ol> <li>Thinks: Uses ChainOfThought to analyze the trajectory and decide next action</li> <li>Decides: Returns <code>tool_calls</code> array (empty if done) with reasoning in <code>thinking</code></li> <li>Acts: Executes all requested tools in parallel using <code>asyncio.gather</code></li> <li>Observes: Appends tool results to trajectory for next iteration</li> </ol>"},{"location":"guides/Agents/#guides.5_agents--functioncallingagent-the-primary-agent","title":"FunctionCallingAgent: The Primary Agent","text":"<p>The <code>FunctionCallingAgent</code> is Synalinks' main agent module. It uses the language model's function calling capabilities to intelligently select and invoke tools:</p> <pre><code>import synalinks\n\nagent = await synalinks.FunctionCallingAgent(\n    data_model=Answer,           # Output schema\n    language_model=lm,           # Which LLM to use\n    tools=[tool1, tool2, tool3], # Available tools\n    autonomous=True,             # Run until complete\n    max_iterations=10,           # Safety limit\n)(inputs)\n</code></pre>"},{"location":"guides/Agents/#guides.5_agents--defining-tools","title":"Defining Tools","text":"<p>Tools are async Python functions wrapped with <code>synalinks.Tool()</code>. They must:</p> <ol> <li>Have type hints for all parameters</li> <li>Have a complete docstring with an <code>Args:</code> section documenting every parameter</li> <li>Be asynchronous (use <code>async def</code>)</li> <li>Have NO optional parameters - all parameters must be required</li> </ol> <pre><code>import synalinks\n\nasync def calculator(expression: str):\n    \"\"\"Evaluate a mathematical expression.\n\n    Args:\n        expression (str): A mathematical expression like '2 + 2' or '15 * 23'.\n    \"\"\"\n    try:\n        result = eval(expression)\n        return {\"result\": str(result)}\n    except Exception as e:\n        return {\"error\": str(e)}\n\n# Wrap the function as a Tool\ncalculator_tool = synalinks.Tool(calculator)\n</code></pre> <p>The <code>synalinks.Tool()</code> wrapper extracts the function's schema from its type hints and docstring, making it available to the agent.</p> <p>Important Tool Constraints:</p> <ul> <li> <p>No Optional Parameters: OpenAI and other LLM providers require all   tool parameters to be required in their JSON schemas. Do not use default   values for parameters.</p> </li> <li> <p>Complete Docstring Required: Every parameter must be documented in the   <code>Args:</code> section of the docstring. The Tool uses these descriptions to build   the JSON schema sent to the LLM. Missing descriptions will raise a ValueError.</p> </li> </ul>"},{"location":"guides/Agents/#guides.5_agents--tool-design-best-practices","title":"Tool Design Best Practices","text":"<pre><code>graph LR\n    A[Good Tool Design] --&gt; B[Clear Name]\n    A --&gt; C[Detailed Docstring]\n    A --&gt; D[Type Hints]\n    A --&gt; E[Error Handling]\n    A --&gt; F[Return Dict]</code></pre> <ol> <li> <p>Clear Names: Use descriptive function names (e.g., <code>search_database</code>,    not <code>search</code> or <code>db_query</code>)</p> </li> <li> <p>Detailed Docstrings: The docstring is sent to the LLM - be specific    about what the tool does, its parameters, and expected output</p> </li> <li> <p>Type Hints: All parameters must have types. The types are converted    to JSON schema for the LLM</p> </li> <li> <p>Error Handling: Return error messages in the result dict rather than    raising exceptions</p> </li> <li> <p>Return Dicts: Tools should return dictionaries with meaningful keys</p> </li> </ol>"},{"location":"guides/Agents/#guides.5_agents--agent-modes","title":"Agent Modes","text":""},{"location":"guides/Agents/#guides.5_agents--autonomous-mode","title":"Autonomous Mode","text":"<p>In autonomous mode, the agent runs until it decides to output a final answer or reaches <code>max_iterations</code>:</p> <pre><code>calculator_tool = synalinks.Tool(calculator)\n\noutputs = await synalinks.FunctionCallingAgent(\n    data_model=Answer,\n    language_model=lm,\n    tools=[calculator_tool],\n    autonomous=True,       # Keep running until done\n    max_iterations=10,     # Safety limit\n)(inputs)\n</code></pre> <p>Use autonomous mode when:</p> <ul> <li>The task requires multiple tool calls</li> <li>You want the agent to figure out the workflow</li> <li>The number of steps is not known in advance</li> </ul>"},{"location":"guides/Agents/#guides.5_agents--non-autonomous-mode-single-step","title":"Non-Autonomous Mode (Single Step)","text":"<p>In non-autonomous mode, the agent executes one iteration and returns:</p> <pre><code>calculator_tool = synalinks.Tool(calculator)\n\noutputs = await synalinks.FunctionCallingAgent(\n    data_model=Answer,\n    language_model=lm,\n    tools=[calculator_tool],\n    autonomous=False,      # Single step only\n    max_iterations=1,\n)(inputs)\n</code></pre> <p>Use non-autonomous mode when:</p> <ul> <li>You want manual control over each step</li> <li>You're building a human-in-the-loop system</li> <li>You need to inspect/modify state between steps</li> </ul>"},{"location":"guides/Agents/#guides.5_agents--parallel-tool-calling","title":"Parallel Tool Calling","text":"<p>Modern LLMs support calling multiple tools in parallel. Synalinks agents leverage this for efficiency:</p> <pre><code>graph LR\n    A[Query] --&gt; B[Agent]\n    B --&gt; C[Tool Call 1]\n    B --&gt; D[Tool Call 2]\n    B --&gt; E[Tool Call 3]\n    C --&gt; F[Results]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Continue/Output]</code></pre> <p>When the LLM determines that multiple tool calls are independent, it can request them simultaneously. Synalinks executes these in parallel:</p> <pre><code># Agent might decide to call multiple tools at once\n# Query: \"What's 2+2 and what's 3*3?\"\n# Parallel calls: calculator(\"2+2\"), calculator(\"3*3\")\n</code></pre> <p>This significantly reduces latency for complex tasks.</p>"},{"location":"guides/Agents/#guides.5_agents--trajectory-tracking","title":"Trajectory Tracking","text":"<p>Use <code>return_inputs_with_trajectory=True</code> to include the full history of tool calls in the output:</p> <pre><code>calculator_tool = synalinks.Tool(calculator)\n\noutputs = await synalinks.FunctionCallingAgent(\n    data_model=Answer,\n    language_model=lm,\n    tools=[calculator_tool],\n    autonomous=True,\n    return_inputs_with_trajectory=True,  # Include history\n)(inputs)\n\n# Output includes:\n# - Original input\n# - All tool calls made\n# - All tool results\n# - Final answer\n</code></pre> <p>This is useful for:</p> <ul> <li>Debugging agent behavior</li> <li>Creating training data</li> <li>Auditing agent decisions</li> </ul>"},{"location":"guides/Agents/#guides.5_agents--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\n# =============================================================================\n# Data Models\n# =============================================================================\n\nclass Query(synalinks.DataModel):\n    \"\"\"User request.\"\"\"\n    query: str = synalinks.Field(description=\"User request\")\n\nclass Answer(synalinks.DataModel):\n    \"\"\"Final answer.\"\"\"\n    answer: str = synalinks.Field(description=\"Final answer to the user\")\n\n# =============================================================================\n# Tools (define async functions, then wrap with synalinks.Tool)\n# =============================================================================\n\nasync def calculator(expression: str):\n    \"\"\"Evaluate a mathematical expression.\n\n    Args:\n        expression (str): A mathematical expression like '2 + 2' or '15 * 23'.\n    \"\"\"\n    try:\n        result = eval(expression)\n        return {\"result\": str(result)}\n    except Exception as e:\n        return {\"error\": str(e)}\n\nasync def get_current_time():\n    \"\"\"Get the current date and time.\"\"\"\n    from datetime import datetime\n    return {\"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n\nasync def convert_temperature(value: float, from_unit: str, to_unit: str):\n    \"\"\"Convert temperature between Celsius and Fahrenheit.\n\n    Args:\n        value (float): The temperature value to convert.\n        from_unit (str): Source unit ('celsius' or 'fahrenheit').\n        to_unit (str): Target unit ('celsius' or 'fahrenheit').\n    \"\"\"\n    if from_unit.lower() == \"celsius\" and to_unit.lower() == \"fahrenheit\":\n        result = (value * 9 / 5) + 32\n        return {\"result\": f\"{result:.1f}F\"}\n    elif from_unit.lower() == \"fahrenheit\" and to_unit.lower() == \"celsius\":\n        result = (value - 32) * 5 / 9\n        return {\"result\": f\"{result:.1f}C\"}\n    else:\n        return {\"error\": f\"Cannot convert from {from_unit} to {to_unit}\"}\n\n# =============================================================================\n# Main\n# =============================================================================\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Wrap async functions as Tool objects\n    calculator_tool = synalinks.Tool(calculator)\n    time_tool = synalinks.Tool(get_current_time)\n    temp_tool = synalinks.Tool(convert_temperature)\n\n    # Create agent with tools\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.FunctionCallingAgent(\n        data_model=Answer,\n        language_model=lm,\n        tools=[calculator_tool, time_tool, temp_tool],\n        autonomous=True,\n        max_iterations=10,\n    )(inputs)\n\n    agent = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"tool_agent\",\n    )\n\n    # Test the agent\n    result = await agent(Query(query=\"What is 15 * 23 + 7?\"))\n    print(f\"Answer: {result['answer']}\")\n\n    result = await agent(Query(query=\"Convert 100 Fahrenheit to Celsius\"))\n    print(f\"Answer: {result['answer']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Agents/#guides.5_agents--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Agent Loop: Agents operate in an observe-think-act loop, iterating   until they achieve their goal or reach a limit.</p> </li> <li> <p>FunctionCallingAgent: The primary agent module that uses LLM function   calling to select and invoke tools intelligently.</p> </li> <li> <p>Tool Requirements: Tools are async functions with type hints and   docstrings, wrapped with <code>synalinks.Tool()</code> before passing to the agent.</p> </li> <li> <p>Autonomous vs Non-Autonomous: Use autonomous mode for multi-step tasks,   non-autonomous for single-step or human-in-the-loop workflows.</p> </li> <li> <p>Parallel Tool Calling: Agents can call multiple tools simultaneously   for efficiency when the LLM determines calls are independent.</p> </li> <li> <p>Error Handling in Tools: Return error information in the result dict   rather than raising exceptions, so the agent can reason about errors.</p> </li> </ul>"},{"location":"guides/Agents/#guides.5_agents--api-references","title":"API References","text":"<ul> <li>FunctionCallingAgent</li> <li>Tool</li> <li>DataModel</li> <li>LanguageModel</li> </ul>"},{"location":"guides/Agents/#guides.5_agents.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Final answer.</p> Source code in <code>guides/5_agents.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Final answer.\"\"\"\n\n    answer: str = synalinks.Field(description=\"Final answer to the user\")\n</code></pre>"},{"location":"guides/Agents/#guides.5_agents.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User request.</p> Source code in <code>guides/5_agents.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User request.\"\"\"\n\n    query: str = synalinks.Field(description=\"User request\")\n</code></pre>"},{"location":"guides/Agents/#guides.5_agents.calculator","title":"<code>calculator(expression)</code>  <code>async</code>","text":"<p>Evaluate a mathematical expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>A mathematical expression like '2 + 2' or '15 * 23'.</p> required Source code in <code>guides/5_agents.py</code> <pre><code>async def calculator(expression: str):\n    \"\"\"Evaluate a mathematical expression.\n\n    Args:\n        expression (str): A mathematical expression like '2 + 2' or '15 * 23'.\n    \"\"\"\n    try:\n        result = eval(expression)\n        return {\"result\": str(result)}\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"guides/Agents/#guides.5_agents.convert_temperature","title":"<code>convert_temperature(value, from_unit, to_unit)</code>  <code>async</code>","text":"<p>Convert temperature between Celsius and Fahrenheit.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The temperature value to convert.</p> required <code>from_unit</code> <code>str</code> <p>Source unit ('celsius' or 'fahrenheit').</p> required <code>to_unit</code> <code>str</code> <p>Target unit ('celsius' or 'fahrenheit').</p> required Source code in <code>guides/5_agents.py</code> <pre><code>async def convert_temperature(value: float, from_unit: str, to_unit: str):\n    \"\"\"Convert temperature between Celsius and Fahrenheit.\n\n    Args:\n        value (float): The temperature value to convert.\n        from_unit (str): Source unit ('celsius' or 'fahrenheit').\n        to_unit (str): Target unit ('celsius' or 'fahrenheit').\n    \"\"\"\n    if from_unit.lower() == \"celsius\" and to_unit.lower() == \"fahrenheit\":\n        result = (value * 9 / 5) + 32\n        return {\"result\": f\"{result:.1f}F\"}\n    elif from_unit.lower() == \"fahrenheit\" and to_unit.lower() == \"celsius\":\n        result = (value - 32) * 5 / 9\n        return {\"result\": f\"{result:.1f}C\"}\n    else:\n        return {\"error\": f\"Cannot convert from {from_unit} to {to_unit}\"}\n</code></pre>"},{"location":"guides/Agents/#guides.5_agents.get_current_time","title":"<code>get_current_time()</code>  <code>async</code>","text":"<p>Get the current date and time.</p> Source code in <code>guides/5_agents.py</code> <pre><code>async def get_current_time():\n    \"\"\"Get the current date and time.\"\"\"\n    from datetime import datetime\n\n    return {\"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n</code></pre>"},{"location":"guides/Data%20Models/","title":"Data Models","text":""},{"location":"guides/Data%20Models/#guides.2_data_models--data-models","title":"Data Models","text":"<p>Data Models are the cornerstone of Synalinks. They define the contract between your application and the Language Model - specifying exactly what structure your inputs and outputs should have.</p>"},{"location":"guides/Data%20Models/#guides.2_data_models--why-data-models-matter","title":"Why Data Models Matter","text":"<p>In traditional LLM development, you send text and receive text. This creates several problems:</p> <pre><code>graph LR\n    subgraph Without Data Models\n        A[Text Prompt] --&gt; B[LLM]\n        B --&gt; C[Unstructured Text]\n        C --&gt; D[Parse &amp; Hope]\n        D --&gt; E[Runtime Errors?]\n    end</code></pre> <p>With Synalinks Data Models:</p> <pre><code>graph LR\n    subgraph With Data Models\n        A[DataModel Input] --&gt; B[Synalinks]\n        B --&gt; C[LLM with Schema]\n        C --&gt; D[Validated Output]\n        D --&gt; E[DataModel Instance]\n    end</code></pre> <p>Data Models provide:</p> <ol> <li>Type Safety: Know exactly what fields you'll receive</li> <li>Validation: Invalid responses are rejected automatically</li> <li>Documentation: Field descriptions guide the LLM</li> <li>IDE Support: Autocomplete and type checking</li> </ol>"},{"location":"guides/Data%20Models/#guides.2_data_models--creating-data-models","title":"Creating Data Models","text":"<p>A Data Model is a Python class that inherits from <code>synalinks.DataModel</code>:</p> <pre><code>import synalinks\nfrom typing import Literal\nfrom enum import Enum\n\nclass Rating(int, Enum):\n    ONE = 1\n    TWO = 2\n    THREE = 3\n    FOUR = 4\n    FIVE = 5\n    SIX = 6\n    SEVEN = 7\n    EIGHT = 8\n    NINE = 9\n    TEN = 10\n\nclass MovieReview(synalinks.DataModel):\n    \"\"\"Analysis of a movie review.\"\"\"\n\n    sentiment: Literal['positive', 'negative', 'neutral'] = synalinks.Field(\n        description=\"The overall sentiment: positive, negative, or neutral\"\n    )\n    key_points: list[str] = synalinks.Field(\n        description=\"Main points mentioned in the review\"\n    )\n    rating: Rating = synalinks.Field(\n        description=\"Estimated rating from 1 to 10\"\n    )\n</code></pre>"},{"location":"guides/Data%20Models/#guides.2_data_models--the-field-function","title":"The Field Function","text":"<p><code>synalinks.Field()</code> is where you communicate with the LLM. The <code>description</code> parameter becomes part of the prompt, telling the model what to generate:</p> <pre><code># Good description - specific and actionable\nanswer: str = synalinks.Field(\n    description=\"A concise answer in 1-2 sentences, based only on the provided context\"\n)\n\n# Poor description - vague\nanswer: str = synalinks.Field(\n    description=\"The answer\"\n)\n</code></pre>"},{"location":"guides/Data%20Models/#guides.2_data_models--supported-types","title":"Supported Types","text":"<p>Synalinks supports these Python types:</p> Type JSON Schema Example <code>str</code> string <code>\"hello world\"</code> <code>int</code> integer <code>42</code> <code>float</code> number <code>3.14</code> <code>bool</code> boolean <code>true</code> <code>list[T]</code> array <code>[\"a\", \"b\", \"c\"]</code> <code>dict</code> object <code>{\"key\": \"value\"}</code> <code>Enum</code> enum Constrained choices <code>synalinks.Score</code> enum 0.0 to 1.0 in steps"},{"location":"guides/Data%20Models/#guides.2_data_models--using-enums-for-constrained-outputs","title":"Using Enums for Constrained Outputs","text":"<p>When you need the LLM to choose from specific options, use Python Enums (similar to the Literal above):</p> <pre><code>from enum import Enum\n\nclass Priority(str, Enum):\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    CRITICAL = \"critical\"\n\nclass TaskAnalysis(synalinks.DataModel):\n    reasoning: str = synalinks.Field(\n        description=\"Why this priority was assigned\"\n    )\n    priority: Priority = synalinks.Field(\n        description=\"The priority level of this task\"\n    )\n</code></pre> <p>The LLM is forced to output one of the enum values - it cannot hallucinate invalid options.</p>"},{"location":"guides/Data%20Models/#guides.2_data_models--using-synalinksscore-for-confidence","title":"Using synalinks.Score for Confidence","text":"<p>For confidence scores or ratings between 0 and 1, use <code>synalinks.Score</code>:</p> <pre><code>class Analysis(synalinks.DataModel):\n    result: str = synalinks.Field(description=\"The analysis result\")\n    confidence: synalinks.Score = synalinks.Field(\n        description=\"Confidence in the result (0.0 = uncertain, 1.0 = certain)\"\n    )\n</code></pre> <p><code>synalinks.Score</code> is an enum with values: <code>NONE (0.0)</code>, <code>VERY_BAD (0.1)</code>, <code>BAD (0.2)</code>, ..., <code>VERY_GOOD (0.9)</code>, <code>PERFECT (1.0)</code>.</p>"},{"location":"guides/Data%20Models/#guides.2_data_models--data-model-operations","title":"Data Model Operations","text":"<p>Synalinks provides operators for combining and manipulating data models:</p> <pre><code>graph TD\n    A[DataModel A] --&gt; C[Operator]\n    B[DataModel B] --&gt; C\n    C --&gt; D[Combined DataModel]</code></pre> Operator Name Behavior <code>+</code> Concat Merge all fields from both models <code>&amp;</code> And Merge, but return None if either is None <code>|</code> Or Return first non-None value for each field <code>^</code> Xor Return None if both have the same field <code>~</code> Not Logical negation (for boolean fields)"},{"location":"guides/Data%20Models/#guides.2_data_models--example-combining-results","title":"Example: Combining Results","text":"<pre><code># Two different analysis results\nresult1 = Analysis1(summary=\"First analysis\", score=0.8)\nresult2 = Analysis2(details=\"Additional details\", tags=[\"a\", \"b\"])\n\n# Combine into one data model with all fields\ncombined = result1.to_json_data_model() + result2.to_json_data_model()\n# Result: {summary, score, details, tags}\n</code></pre>"},{"location":"guides/Data%20Models/#guides.2_data_models--masking-filtering-fields","title":"Masking: Filtering Fields","text":"<p>Sometimes you need to extract or remove specific fields. Use masking operations:</p>"},{"location":"guides/Data%20Models/#guides.2_data_models--inmask-keep-only-specified-fields","title":"InMask: Keep Only Specified Fields","text":"<pre><code># Keep only 'answer' and 'confidence', discard everything else\nfiltered = await synalinks.ops.in_mask(\n    full_result,\n    mask=[\"answer\", \"confidence\"]\n)\n</code></pre>"},{"location":"guides/Data%20Models/#guides.2_data_models--outmask-remove-specified-fields","title":"OutMask: Remove Specified Fields","text":"<pre><code># Remove 'thinking', keep everything else\nfiltered = await synalinks.ops.out_mask(\n    full_result,\n    mask=[\"thinking\"]\n)\n</code></pre> <p>This is particularly useful when:</p> <ul> <li>You want to hide intermediate reasoning from the final output</li> <li>You're training and only want to evaluate certain fields</li> <li>You're chaining modules and need to reshape data between them</li> </ul>"},{"location":"guides/Data%20Models/#guides.2_data_models--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom enum import Enum\nfrom dotenv import load_dotenv\nimport synalinks\n\n# Define an enum for constrained choices\nclass Sentiment(str, Enum):\n    POSITIVE = \"positive\"\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n\n# Input data model\nclass ReviewInput(synalinks.DataModel):\n    \"\"\"A product review to analyze.\"\"\"\n    review_text: str = synalinks.Field(\n        description=\"The text of the product review\"\n    )\n\n# Output data model with multiple field types\nclass ReviewAnalysis(synalinks.DataModel):\n    \"\"\"Structured analysis of a review.\"\"\"\n    sentiment: Sentiment = synalinks.Field(\n        description=\"The overall sentiment of the review\"\n    )\n    confidence: synalinks.Score = synalinks.Field(\n        description=\"Confidence in the sentiment classification\"\n    )\n    key_points: list[str] = synalinks.Field(\n        description=\"Main points mentioned by the reviewer\"\n    )\n    recommended: bool = synalinks.Field(\n        description=\"Whether the reviewer would recommend the product\"\n    )\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    language_model = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Build the program\n    inputs = synalinks.Input(data_model=ReviewInput)\n    outputs = await synalinks.Generator(\n        data_model=ReviewAnalysis,\n        language_model=language_model,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"review_analyzer\",\n    )\n\n    # Run analysis\n    result = await program(\n        ReviewInput(\n            review_text=\"This laptop is amazing! Fast processor, great screen, \"\n            \"and the battery lasts all day. Only complaint is it runs a bit warm. \"\n            \"Would definitely buy again!\"\n        )\n    )\n\n    # Access structured results\n    print(f\"Sentiment: {result['sentiment']}\")\n    print(f\"Confidence: {result['confidence']}\")\n    print(f\"Key Points: {result['key_points']}\")\n    print(f\"Recommended: {result['recommended']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Data%20Models/#guides.2_data_models--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Field Descriptions Are Instructions: The <code>description</code> parameter tells   the LLM what to generate. Write clear, specific descriptions.</p> </li> <li> <p>Use Enums for Choices: When the output must be one of several options,   use Python Enums to constrain the LLM's output.</p> </li> <li> <p>synalinks.Score for Confidence: Use the built-in Score type for   confidence values, ratings, or any 0-1 scale.</p> </li> <li> <p>Operators Combine Models: Use <code>+</code>, <code>&amp;</code>, <code>|</code>, <code>^</code> to merge data models   from different sources or processing branches.</p> </li> <li> <p>Masking Filters Fields: Use <code>in_mask</code> to keep specific fields or   <code>out_mask</code> to remove them.</p> </li> </ul>"},{"location":"guides/Data%20Models/#guides.2_data_models--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>JsonDataModel</li> <li>Base DataModels</li> <li>JSON Ops</li> </ul>"},{"location":"guides/Data%20Models/#guides.2_data_models.ReviewAnalysis","title":"<code>ReviewAnalysis</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Structured analysis of a review.</p> Source code in <code>guides/2_data_models.py</code> <pre><code>class ReviewAnalysis(synalinks.DataModel):\n    \"\"\"Structured analysis of a review.\"\"\"\n\n    sentiment: Sentiment = synalinks.Field(\n        description=\"The overall sentiment of the review\"\n    )\n    confidence: synalinks.Score = synalinks.Field(\n        description=\"Confidence in the sentiment classification\"\n    )\n    key_points: list[str] = synalinks.Field(\n        description=\"Main points mentioned by the reviewer\"\n    )\n    recommended: bool = synalinks.Field(\n        description=\"Whether the reviewer would recommend the product\"\n    )\n</code></pre>"},{"location":"guides/Data%20Models/#guides.2_data_models.ReviewInput","title":"<code>ReviewInput</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A product review to analyze.</p> Source code in <code>guides/2_data_models.py</code> <pre><code>class ReviewInput(synalinks.DataModel):\n    \"\"\"A product review to analyze.\"\"\"\n\n    review_text: str = synalinks.Field(description=\"The text of the product review\")\n</code></pre>"},{"location":"guides/Getting%20Started/","title":"Getting Started","text":""},{"location":"guides/Getting%20Started/#guides.1_getting_started--getting-started-with-synalinks","title":"Getting Started with Synalinks","text":"<p>Welcome to Synalinks! This guide introduces you to the fundamental concepts and helps you build your first AI-powered application.</p>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--what-is-synalinks","title":"What is Synalinks?","text":"<p>Synalinks is a neuro-symbolic framework for building Language Model (LM) applications. Unlike traditional approaches where you write prompts manually, Synalinks lets you define what you want (using data models) and handles the how (prompt construction, parsing, validation) automatically.</p> <pre><code>graph LR\n    subgraph Traditional Approach\n        A[Write Prompt] --&gt; B[Call LLM API]\n        B --&gt; C[Parse Response]\n        C --&gt; D[Handle Errors]\n    end\n    subgraph Synalinks Approach\n        E[Define DataModel] --&gt; F[Synalinks]\n        F --&gt; G[Structured Output]\n    end</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--why-synalinks","title":"Why Synalinks?","text":"<p>Traditional LLM development has several pain points:</p> <ol> <li>Prompt Engineering: Manually crafting prompts is tedious and error-prone</li> <li>Output Parsing: LLM responses are unstructured text that needs parsing</li> <li>Schema Validation: Ensuring responses match expected formats is difficult</li> <li>Reproducibility: Results vary based on prompt wording</li> </ol> <p>Synalinks solves these by:</p> <ul> <li>Auto-generating prompts from your data model definitions</li> <li>Constraining outputs to always match your schema (no parsing errors!)</li> <li>Providing type safety through Pydantic-based data models</li> <li>Enabling training to improve your programs over time</li> </ul>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--installation","title":"Installation","text":"<pre><code># Using pip\npip install synalinks\n\n# Or using uv (recommended for faster installs)\nuv pip install synalinks\n</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--environment-setup","title":"Environment Setup","text":"<p>Synalinks works with any OpenAI-compatible API. Set up your credentials:</p> <pre><code># Create a .env file in your project root\nOPENAI_API_KEY=your-api-key-here\n\n# For Anthropic models\nANTHROPIC_API_KEY=your-anthropic-key\n\n# For local models (Ollama) - no key needed\n# Just run: ollama serve\n</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--core-concepts","title":"Core Concepts","text":""},{"location":"guides/Getting%20Started/#guides.1_getting_started--1-data-models-the-foundation","title":"1. Data Models: The Foundation","text":"<p>In Synalinks, everything revolves around Data Models. A DataModel defines the structure of your inputs and outputs using Python classes:</p> <pre><code>import synalinks\n\nclass Question(synalinks.DataModel):\n    \"\"\"The input to our program.\"\"\"\n    question: str = synalinks.Field(\n        description=\"The question to answer\"\n    )\n\nclass Answer(synalinks.DataModel):\n    \"\"\"The output from our program.\"\"\"\n    thinking: str = synalinks.Field(\n        description=\"Step-by-step reasoning process\"\n    )\n    answer: str = synalinks.Field(\n        description=\"The final answer\"\n    )\n</code></pre> <p>The <code>description</code> parameter is crucial - it tells the LLM what each field should contain. Think of it as documentation that the AI reads.</p>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--2-the-generator-module","title":"2. The Generator Module","text":"<p>The <code>Generator</code> is the core module that transforms inputs into outputs:</p> <pre><code>generator = synalinks.Generator(\n    data_model=Answer,           # What structure to output\n    language_model=language_model,  # Which LLM to use\n)\n</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--3-programs-composable-pipelines","title":"3. Programs: Composable Pipelines","text":"<p>A <code>Program</code> wraps your modules into a reusable, trainable unit:</p> <pre><code>program = synalinks.Program(\n    inputs=inputs,\n    outputs=outputs,\n    name=\"my_program\",\n)\n</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--complete-example","title":"Complete Example","text":"<p>Here's a complete, runnable example that demonstrates the core concepts:</p> <pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\n# Step 1: Define your data models\nclass Question(synalinks.DataModel):\n    \"\"\"Input: A question from the user.\"\"\"\n    question: str = synalinks.Field(\n        description=\"The question to answer\"\n    )\n\nclass Answer(synalinks.DataModel):\n    \"\"\"Output: An answer with reasoning.\"\"\"\n    thinking: str = synalinks.Field(\n        description=\"Your step-by-step reasoning process\"\n    )\n    answer: str = synalinks.Field(\n        description=\"The final answer based on your reasoning\"\n    )\n\nasync def main():\n    # Load environment variables (API keys)\n    load_dotenv()\n\n    # Clear session for reproducible module naming\n    synalinks.clear_session()\n\n    # Step 2: Initialize the language model\n    language_model = synalinks.LanguageModel(\n        model=\"openai/gpt-4.1-mini\"  # Or \"anthropic/claude-3-haiku\", etc.\n    )\n\n    # Step 3: Build the program using the Functional API\n    inputs = synalinks.Input(data_model=Question)\n    outputs = await synalinks.Generator(\n        data_model=Answer,\n        language_model=language_model,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"qa_program\",\n        description=\"A simple question-answering program\",\n    )\n\n    # Step 4: Run the program\n    result = await program(\n        Question(question=\"What is the capital of France?\")\n    )\n\n    # Step 5: Access the structured output\n    print(f\"Thinking: {result['thinking']}\")\n    print(f\"Answer: {result['answer']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--understanding-the-output","title":"Understanding the Output","text":"<p>When you run the program, you get a structured dictionary that exactly matches your <code>Answer</code> data model:</p> <pre><code>{\n    \"thinking\": \"France is a country in Western Europe. Its capital city...\",\n    \"answer\": \"Paris\"\n}\n</code></pre> <p>This is guaranteed by Synalinks' constrained generation - the LLM is forced to produce valid JSON that matches your schema.</p>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>No Manual Prompting: Define data models instead of writing prompts.   Synalinks automatically constructs effective prompts from your field   descriptions.</p> </li> <li> <p>Structured Output: Every response is guaranteed to match your schema.   No more parsing errors or malformed responses.</p> </li> <li> <p>Field Descriptions Matter: The <code>description</code> parameter guides the LLM.   Write clear, specific descriptions for best results.</p> </li> <li> <p>Chain of Thought: Adding a \"thinking\" field encourages the LLM to   reason step-by-step, improving accuracy on complex tasks.</p> </li> <li> <p>Session Management: Use <code>synalinks.clear_session()</code> at the start of   scripts for reproducible module naming.</p> </li> </ul>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, continue to:</p> <ul> <li>Guide 2: Data Models - Deep dive into data model features</li> <li>Guide 3: Programs - Learn about different program architectures</li> <li>Guide 4: Modules - Explore the available modules</li> </ul>"},{"location":"guides/Getting%20Started/#guides.1_getting_started--api-references","title":"API References","text":"<ul> <li>DataModel</li> <li>Generator</li> <li>Program</li> <li>LanguageModel</li> </ul>"},{"location":"guides/Getting%20Started/#guides.1_getting_started.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Output: An answer with reasoning.</p> Source code in <code>guides/1_getting_started.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Output: An answer with reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Your step-by-step reasoning process\")\n    answer: str = synalinks.Field(description=\"The final answer based on your reasoning\")\n</code></pre>"},{"location":"guides/Getting%20Started/#guides.1_getting_started.Question","title":"<code>Question</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Input: A question from the user.</p> Source code in <code>guides/1_getting_started.py</code> <pre><code>class Question(synalinks.DataModel):\n    \"\"\"Input: A question from the user.\"\"\"\n\n    question: str = synalinks.Field(description=\"The question to answer\")\n</code></pre>"},{"location":"guides/Input%20Guard/","title":"Input Guard","text":""},{"location":"guides/Input%20Guard/#guides.9_input_guard--input-guard-patterns","title":"Input Guard Patterns","text":"<p>Input Guards protect your LM applications by filtering dangerous or unwanted inputs BEFORE they reach the language model. This saves compute costs and prevents prompt injection attacks.</p>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--why-input-guards-matter","title":"Why Input Guards Matter","text":"<p>Without input guards, every request hits your LLM - even malicious ones:</p> <pre><code>graph LR\n    subgraph Without Guards\n        A[Any Input] --&gt; B[LLM]\n        B --&gt; C[Response]\n    end\n    subgraph With Input Guard\n        D[Input] --&gt; E{Input Guard}\n        E --&gt;|Safe| F[LLM]\n        F --&gt; G[Response]\n        E --&gt;|Unsafe| H[Warning]\n    end</code></pre> <p>Input guards provide:</p> <ol> <li>Cost Savings: Skip LLM calls entirely for blocked inputs</li> <li>Security: Block prompt injection attempts</li> <li>Policy Enforcement: Ensure inputs meet your requirements</li> <li>Graceful Degradation: Return helpful warnings instead of errors</li> </ol>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--the-xor-and-or-operators","title":"The XOR and OR Operators","text":"<p>For input guards, we use two key operators:</p>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--xor-computation-bypass","title":"XOR (^): Computation Bypass","text":"<p>XOR returns None if both operands have values. When a guard triggers (returns a warning), XOR with the input produces None, blocking downstream computation:</p> <pre><code>graph LR\n    A[warning] --&gt; C[\"warning ^ inputs\"]\n    B[inputs] --&gt; C\n    C --&gt; D{Result}\n    D --&gt;|\"warning exists\"| E[\"None (blocked)\"]\n    D --&gt;|\"warning is None\"| F[\"inputs (pass-through)\"]</code></pre> warning inputs warning ^ inputs None data data (pass-through) data data None (blocked)"},{"location":"guides/Input%20Guard/#guides.9_input_guard--or-result-selection","title":"OR (|): Result Selection","text":"<p>OR returns the first non-None value, perfect for choosing between warning and answer:</p> warning answer warning | answer None data data (use answer) data None data (use warning) data data merged (warning fields take priority)"},{"location":"guides/Input%20Guard/#guides.9_input_guard--input-guard-pattern","title":"Input Guard Pattern","text":"<p>The complete input guard pattern:</p> <pre><code>graph LR\n    A[inputs] --&gt; B[InputGuard]\n    B --&gt; C[warning]\n    A --&gt; D[\"warning ^ inputs\"]\n    C --&gt; D\n    D --&gt; E[\"guarded_inputs\"]\n    E --&gt; F[Generator]\n    F --&gt; G[answer]\n    C --&gt; H[\"warning | answer\"]\n    G --&gt; H\n    H --&gt; I[output]</code></pre> <p>Flow when input is BLOCKED: 1. Guard returns warning (not None) 2. XOR: warning ^ inputs = None (blocks input) 3. Generator receives None, returns None 4. OR: warning | None = warning (use warning)</p> <p>Flow when input is SAFE: 1. Guard returns None (no warning) 2. XOR: None ^ inputs = inputs (pass-through) 3. Generator processes inputs, returns answer 4. OR: None | answer = answer (use answer)</p>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--building-an-input-guard","title":"Building an Input Guard","text":"<p>An input guard is a custom Module that: - Returns <code>None</code> when input is safe (no warning) - Returns a warning DataModel when input should be blocked</p> <pre><code>import synalinks\n\nclass InputGuard(synalinks.Module):\n    \"\"\"Guard that blocks inputs containing blacklisted words.\"\"\"\n\n    def __init__(self, blacklisted_words, warning_message, **kwargs):\n        super().__init__(**kwargs)\n        self.blacklisted_words = blacklisted_words\n        self.warning_message = warning_message\n\n    async def call(self, inputs, training=False):\n        \"\"\"Return warning if blocked, None otherwise.\"\"\"\n        if inputs is None:\n            return None\n\n        # Check the query field for blacklisted words\n        query = inputs.get(\"query\", \"\").lower()\n\n        for word in self.blacklisted_words:\n            if word.lower() in query:\n                # Return warning - this will trigger the guard\n                return Warning(message=self.warning_message).to_json_data_model()\n\n        # Input is safe - return None to pass through\n        return None\n\n    async def compute_output_spec(self, inputs, training=False):\n        \"\"\"Define output schema.\"\"\"\n        return Warning.to_symbolic_data_model(name=self.name)\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"User query\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The answer\")\n\nclass Warning(synalinks.DataModel):\n    message: str = synalinks.Field(description=\"Warning message\")\n\nclass InputGuard(synalinks.Module):\n    def __init__(self, blacklisted_words, warning_message, **kwargs):\n        super().__init__(**kwargs)\n        self.blacklisted_words = blacklisted_words\n        self.warning_message = warning_message\n\n    async def call(self, inputs, training=False):\n        if inputs is None:\n            return None\n        query = inputs.get(\"query\", \"\").lower()\n        for word in self.blacklisted_words:\n            if word.lower() in query:\n                return Warning(message=self.warning_message).to_json_data_model()\n        return None\n\n    async def compute_output_spec(self, inputs, training=False):\n        return Warning.to_symbolic_data_model(name=self.name)\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Build the guarded program\n    inputs = synalinks.Input(data_model=Query)\n\n    # Guard checks for blacklisted words\n    warning = await InputGuard(\n        blacklisted_words=[\"hack\", \"exploit\", \"forbidden\"],\n        warning_message=\"I cannot process this request.\",\n    )(inputs)\n\n    # XOR: If warning exists, block the input\n    guarded_inputs = warning ^ inputs\n\n    # Generator only runs if guarded_inputs is not None\n    answer = await synalinks.Generator(\n        data_model=Answer,\n        language_model=lm,\n    )(guarded_inputs)\n\n    # OR: Return warning if it exists, otherwise return answer\n    outputs = warning | answer\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"input_guarded_qa\",\n    )\n\n    # Test blocked input\n    result = await program(Query(query=\"How do I hack into systems?\"))\n    print(f\"Blocked: {result}\")  # Shows warning\n\n    # Test safe input\n    result = await program(Query(query=\"What is the capital of France?\"))\n    print(f\"Safe: {result}\")  # Shows answer\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>XOR (^) for Blocking: When guard returns a warning, XOR with input   produces None, preventing downstream computation.</p> </li> <li> <p>OR (|) for Selection: OR returns the first non-None value, choosing   between warning and answer.</p> </li> <li> <p>None Propagation: Modules receiving None inputs skip execution and   return None, enabling efficient short-circuiting.</p> </li> <li> <p>Custom Guards: Inherit from <code>synalinks.Module</code>, return None when safe   or a warning DataModel when blocked.</p> </li> <li> <p>Cost Savings: Blocked inputs never reach the LLM, saving API costs.</p> </li> </ul>"},{"location":"guides/Input%20Guard/#guides.9_input_guard--api-references","title":"API References","text":"<ul> <li>Module</li> <li>JSON Ops</li> </ul>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Answer to the query.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Answer to the query.\"\"\"\n\n    answer: str = synalinks.Field(description=\"The answer\")\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.InputGuard","title":"<code>InputGuard</code>","text":"<p>               Bases: <code>Module</code></p> <p>Guard that blocks inputs containing blacklisted words.</p> <p>Returns None when input is safe, or a Warning when input should be blocked.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>class InputGuard(synalinks.Module):\n    \"\"\"Guard that blocks inputs containing blacklisted words.\n\n    Returns None when input is safe, or a Warning when input should be blocked.\n    \"\"\"\n\n    def __init__(self, blacklisted_words, warning_message, **kwargs):\n        super().__init__(**kwargs)\n        self.blacklisted_words = blacklisted_words\n        self.warning_message = warning_message\n\n    async def call(\n        self,\n        inputs: synalinks.JsonDataModel,\n        training: bool = False,\n    ) -&gt; synalinks.JsonDataModel:\n        \"\"\"Return warning if blocked, None otherwise.\"\"\"\n        if inputs is None:\n            return None\n\n        query = inputs.get(\"query\", \"\").lower()\n\n        for word in self.blacklisted_words:\n            if word.lower() in query:\n                return Warning(message=self.warning_message).to_json_data_model()\n\n        return None\n\n    async def compute_output_spec(\n        self,\n        inputs: synalinks.SymbolicDataModel,\n        training: bool = False,\n    ) -&gt; synalinks.SymbolicDataModel:\n        \"\"\"Define output schema.\"\"\"\n        return Warning.to_symbolic_data_model(name=self.name)\n\n    def get_config(self):\n        \"\"\"Serialization config.\"\"\"\n        return {\n            \"name\": self.name,\n            \"blacklisted_words\": self.blacklisted_words,\n            \"warning_message\": self.warning_message,\n        }\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.InputGuard.call","title":"<code>call(inputs, training=False)</code>  <code>async</code>","text":"<p>Return warning if blocked, None otherwise.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>async def call(\n    self,\n    inputs: synalinks.JsonDataModel,\n    training: bool = False,\n) -&gt; synalinks.JsonDataModel:\n    \"\"\"Return warning if blocked, None otherwise.\"\"\"\n    if inputs is None:\n        return None\n\n    query = inputs.get(\"query\", \"\").lower()\n\n    for word in self.blacklisted_words:\n        if word.lower() in query:\n            return Warning(message=self.warning_message).to_json_data_model()\n\n    return None\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.InputGuard.compute_output_spec","title":"<code>compute_output_spec(inputs, training=False)</code>  <code>async</code>","text":"<p>Define output schema.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>async def compute_output_spec(\n    self,\n    inputs: synalinks.SymbolicDataModel,\n    training: bool = False,\n) -&gt; synalinks.SymbolicDataModel:\n    \"\"\"Define output schema.\"\"\"\n    return Warning.to_symbolic_data_model(name=self.name)\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.InputGuard.get_config","title":"<code>get_config()</code>","text":"<p>Serialization config.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>def get_config(self):\n    \"\"\"Serialization config.\"\"\"\n    return {\n        \"name\": self.name,\n        \"blacklisted_words\": self.blacklisted_words,\n        \"warning_message\": self.warning_message,\n    }\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User query.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User query.\"\"\"\n\n    query: str = synalinks.Field(description=\"User query\")\n</code></pre>"},{"location":"guides/Input%20Guard/#guides.9_input_guard.Warning","title":"<code>Warning</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Warning message when input is blocked.</p> Source code in <code>guides/9_input_guard.py</code> <pre><code>class Warning(synalinks.DataModel):\n    \"\"\"Warning message when input is blocked.\"\"\"\n\n    message: str = synalinks.Field(description=\"Warning message\")\n</code></pre>"},{"location":"guides/Knowledge%20Base/","title":"Knowledge Base","text":""},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--knowledge-base","title":"Knowledge Base","text":"<p>A Knowledge Base in Synalinks is a structured storage system that enables your LM applications to retrieve and reason over external data. Unlike simple prompt injection, a Knowledge Base provides semantic search capabilities, automatic chunking, and efficient retrieval - the foundation for building Retrieval-Augmented Generation (RAG) systems.</p>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--why-knowledge-bases-matter","title":"Why Knowledge Bases Matter","text":"<p>Language models have a knowledge cutoff and limited context windows. A Knowledge Base solves both problems:</p> <pre><code>graph LR\n    subgraph Without Knowledge Base\n        A[Query] --&gt; B[LLM]\n        B --&gt; C[Hallucination Risk]\n    end\n    subgraph With Knowledge Base\n        D[Query] --&gt; E[Retrieve Relevant Docs]\n        E --&gt; F[LLM + Context]\n        F --&gt; G[Grounded Answer]\n    end</code></pre> <p>Knowledge Bases provide:</p> <ol> <li>Grounded Responses: Answers based on actual data, not hallucinations</li> <li>Unlimited Knowledge: Store documents beyond context limits</li> <li>Up-to-Date Information: Add new data without retraining</li> <li>Source Attribution: Track where answers come from</li> </ol>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--architecture","title":"Architecture","text":"<p>Synalinks Knowledge Base is built on DuckDB, providing:</p> <pre><code>graph TD\n    A[DataModels] --&gt; B[KnowledgeBase]\n    B --&gt; C[DuckDB Storage]\n    B --&gt; D[Full-Text Index]\n    B --&gt; E[Vector Index]\n    F[Search Query] --&gt; G{Search Type}\n    G --&gt;|fulltext| D\n    G --&gt;|similarity| E\n    G --&gt;|hybrid| H[Combine Both]\n    D --&gt; I[Results]\n    E --&gt; I\n    H --&gt; I</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--creating-a-knowledge-base","title":"Creating a Knowledge Base","text":"<p>Define DataModels for your documents, then create the Knowledge Base:</p> <pre><code>import synalinks\n\nclass Document(synalinks.DataModel):\n    \"\"\"A document in the knowledge base.\"\"\"\n    id: str = synalinks.Field(description=\"Unique document ID\")\n    title: str = synalinks.Field(description=\"Document title\")\n    content: str = synalinks.Field(description=\"Document content\")\n\n# Create the knowledge base\nkb = synalinks.KnowledgeBase(\n    uri=\"duckdb://my_database.db\",    # Storage location\n    data_models=[Document],            # What types to store\n    embedding_model=embedding_model,   # For vector search (optional)\n    metric=\"cosine\",                   # Similarity metric\n    wipe_on_start=False,               # Preserve existing data\n)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--key-parameters","title":"Key Parameters","text":"Parameter Description <code>uri</code> Database connection string (e.g., <code>duckdb://path.db</code>) <code>data_models</code> List of DataModel classes to store <code>embedding_model</code> EmbeddingModel for vector search (optional) <code>metric</code> Similarity metric: <code>cosine</code>, <code>l2</code>, or <code>ip</code> <code>wipe_on_start</code> Clear database on initialization"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--search-methods","title":"Search Methods","text":""},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--full-text-search-bm25","title":"Full-Text Search (BM25)","text":"<p>Uses the BM25 algorithm for traditional keyword-based search:</p> <pre><code>results = await kb.fulltext_search(\n    \"machine learning neural networks\",\n    data_models=[Document], # If None search in all tables\n    k=10,           # Number of results\n    threshold=None, # Minimum score (optional)\n)\n</code></pre> <p>Best for:</p> <ul> <li>Exact keyword matching</li> <li>When users search with specific terms</li> <li>Quick, lightweight search</li> </ul>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--similarity-search-vector","title":"Similarity Search (Vector)","text":"<p>Uses embedding vectors for semantic search:</p> <pre><code>results = await kb.similarity_search(\n    \"how do computers learn\",  # Semantically matches \"machine learning\"\n    data_models=[Document], # If None search in all tables\n    k=10,\n    threshold=0.7,  # Minimum similarity score\n)\n</code></pre> <p>Best for:</p> <ul> <li>Semantic meaning matching</li> <li>Natural language queries</li> <li>Finding conceptually related content</li> </ul>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--hybrid-search","title":"Hybrid Search","text":"<p>Combines both methods for best results:</p> <pre><code>results = await kb.hybrid_search(\n    \"machine learning basics\",\n    data_models=[Document],\n    k=10,\n    bm25_weight=0.5,    # Weight for BM25 scores\n    vector_weight=0.5,  # Weight for vector scores\n)\n</code></pre> <p>Best for:</p> <ul> <li>Production RAG systems</li> <li>When you need both exact and semantic matching</li> <li>Complex queries that benefit from both approaches</li> </ul>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--crud-operations","title":"CRUD Operations","text":""},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--createupdate","title":"Create/Update","text":"<p>The <code>update</code> method performs upsert (insert or update). The first field in your DataModel is used as the primary key:</p> <pre><code>doc = Document(\n    id=\"doc1\",\n    title=\"Introduction to AI\",\n    content=\"Artificial intelligence is...\",\n)\n\nawait kb.update(doc.to_json_data_model())\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--read-by-id","title":"Read by ID","text":"<pre><code>result = await kb.get(\n    \"doc1\",  # Primary key value\n    data_models=[Document],\n)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--list-all","title":"List All","text":"<pre><code>all_docs = await kb.getall(\n    Document,\n    limit=100,\n    offset=0,\n)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--delete","title":"Delete","text":"<pre><code>await kb.delete(\n    \"doc1\",\n    data_models=[Document],\n)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--raw-sql","title":"Raw SQL","text":"<p>For complex queries, use raw SQL:</p> <pre><code>results = await kb.query(\n    \"SELECT id, title FROM Document WHERE title LIKE ?\",\n    params=[\"%Learning%\"],\n)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--knowledge-modules","title":"Knowledge Modules","text":"<p>Synalinks provides modules for integrating Knowledge Bases into programs:</p>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--retrieveknowledge","title":"RetrieveKnowledge","text":"<p>Retrieves relevant documents using LM-generated search queries:</p> <pre><code>graph LR\n    A[Input] --&gt; B[Generate Query]\n    B --&gt; C[Search KB]\n    C --&gt; D[Context + Input]</code></pre> <pre><code>retrieved = await synalinks.RetrieveKnowledge(\n    knowledge_base=kb,\n    language_model=lm,\n    search_type=\"hybrid\",  # fulltext, similarity, or hybrid\n    k=10,\n    return_inputs=True,    # Include original input in output\n)(inputs)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--updateknowledge","title":"UpdateKnowledge","text":"<p>Stores DataModels in the Knowledge Base:</p> <pre><code>stored = await synalinks.UpdateKnowledge(\n    knowledge_base=kb,\n)(extracted_data)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--embedknowledge","title":"EmbedKnowledge","text":"<p>Generates embeddings for DataModels:</p> <pre><code>embedded = await synalinks.EmbedKnowledge(\n    embedding_model=embedding_model,\n    in_mask=[\"content\"],  # Which fields to embed\n)(inputs)\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--building-a-rag-pipeline","title":"Building a RAG Pipeline","text":"<p>A complete RAG system combines retrieval with generation:</p> <pre><code>graph LR\n    A[Query] --&gt; B[RetrieveKnowledge]\n    B --&gt; C[Context + Query]\n    C --&gt; D[Generator]\n    D --&gt; E[Grounded Answer]</code></pre> <pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"User question\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"Answer based on context\")\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Assume kb is already populated\n    kb = synalinks.KnowledgeBase(\n        uri=\"duckdb://knowledge.db\",\n        data_models=[Document],\n    )\n\n    inputs = synalinks.Input(data_model=Query)\n\n    # Retrieve relevant documents\n    retrieved = await synalinks.RetrieveKnowledge(\n        knowledge_base=kb,\n        language_model=lm,\n        search_type=\"fulltext\",\n        k=5,\n        return_inputs=True,\n    )(inputs)\n\n    # Generate answer using retrieved context\n    outputs = await synalinks.Generator(\n        data_model=Answer,\n        language_model=lm,\n    )(retrieved)\n\n    rag = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"rag_pipeline\",\n    )\n\n    result = await rag(Query(query=\"What is machine learning?\"))\n    print(result[\"answer\"])\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>DuckDB Backend: Fast, embedded database with full-text and vector search   capabilities. No external services required.</p> </li> <li> <p>Three Search Types: Full-text (BM25) for keywords, similarity for   semantics, hybrid for best of both.</p> </li> <li> <p>DataModel as Schema: Your DataModels define the structure of stored   documents. The first field is the primary key.</p> </li> <li> <p>RetrieveKnowledge Module: Automates query generation and retrieval for   RAG pipelines. Combines seamlessly with Generator.</p> </li> <li> <p>Upsert Semantics: The <code>update</code> method inserts new records or updates   existing ones based on the primary key.</p> </li> <li> <p>Raw SQL Access: For complex queries, you can use raw SQL directly.</p> </li> </ul>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base--api-references","title":"API References","text":"<ul> <li>KnowledgeBase</li> <li>RetrieveKnowledge</li> <li>UpdateKnowledge</li> <li>EmbedKnowledge</li> </ul>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Answer based on retrieved context.</p> Source code in <code>guides/6_knowledge_base.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Answer based on retrieved context.\"\"\"\n\n    answer: str = synalinks.Field(description=\"Answer based on the context provided\")\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A document in the knowledge base.</p> Source code in <code>guides/6_knowledge_base.py</code> <pre><code>class Document(synalinks.DataModel):\n    \"\"\"A document in the knowledge base.\"\"\"\n\n    id: str = synalinks.Field(description=\"Unique document ID\")\n    title: str = synalinks.Field(description=\"Document title\")\n    content: str = synalinks.Field(description=\"Document content\")\n</code></pre>"},{"location":"guides/Knowledge%20Base/#guides.6_knowledge_base.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User query.</p> Source code in <code>guides/6_knowledge_base.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User query.\"\"\"\n\n    query: str = synalinks.Field(description=\"User question\")\n</code></pre>"},{"location":"guides/Modules/","title":"Modules","text":""},{"location":"guides/Modules/#guides.4_modules--modules","title":"Modules","text":"<p>Modules are the fundamental building blocks of Synalinks programs. Just as neurons are the basic units of computation in a neural network, modules are the basic units of computation in a Synalinks program. Each module performs a specific transformation on data, and modules can be composed together to build complex LM applications.</p>"},{"location":"guides/Modules/#guides.4_modules--core-modules","title":"Core Modules","text":""},{"location":"guides/Modules/#guides.4_modules--input-the-entry-point","title":"Input: The Entry Point","text":"<p>Every program starts with an <code>Input</code> module. It defines the schema of data entering your program:</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"User question\")\n\n# Define the entry point - no computation happens here\ninputs = synalinks.Input(data_model=Query)\n</code></pre> <p>The <code>Input</code> module doesn't transform data - it simply marks where data enters the computation graph. Think of it as the \"x\" in f(x).</p>"},{"location":"guides/Modules/#guides.4_modules--generator-the-core-llm-module","title":"Generator: The Core LLM Module","text":"<p>The <code>Generator</code> is the heart of Synalinks. It takes input data and uses a language model to produce structured output:</p> <pre><code>graph LR\n    A[Input DataModel] --&gt; B[Generator]\n    B --&gt; C[LLM Call]\n    C --&gt; D[Constrained Decoding]\n    D --&gt; E[Output DataModel]</code></pre> <pre><code>import synalinks\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The answer\")\n\noutputs = await synalinks.Generator(\n    data_model=Answer,\n    language_model=language_model,\n    instructions=\"Be concise and accurate.\",  # Extra guidance\n)(inputs)\n</code></pre> <p>Key Generator features:</p> <ul> <li>Constrained Output: Output always matches your DataModel schema</li> <li>Automatic Prompting: Synalinks constructs the prompt from your schema</li> <li>Instructions Parameter: Add extra guidance without modifying the schema</li> <li>Trainable: Instructions and examples can be optimized</li> </ul>"},{"location":"guides/Modules/#guides.4_modules--identity-pass-through","title":"Identity: Pass-Through","text":"<p>The <code>Identity</code> module passes data unchanged. Useful for creating parallel paths or as a placeholder:</p> <pre><code># Just pass the data through\nunchanged = await synalinks.Identity()(inputs)\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--tool-wrap-any-function","title":"Tool: Wrap Any Function","text":"<p>The <code>Tool</code> module wraps async Python functions for use in programs:</p> <pre><code>import synalinks\n\n@synalinks.saving.register_synalinks_serializable()\nasync def search_web(query: str):\n    \"\"\"Search the web for information.\n\n    Args:\n        query (str): The search query.\n    \"\"\"\n    # Your search implementation\n    return {\"results\": [...]}\n\ntool = synalinks.Tool(search_web)\n</code></pre> <p>Important Tool Constraints:</p> <ul> <li> <p>No Optional Parameters: All parameters must be required. OpenAI and   other LLM providers require all tool parameters to be required in their   JSON schemas. Do not use default values.</p> </li> <li> <p>Complete Docstring Required: Every parameter must be documented in   the <code>Args:</code> section. The Tool extracts descriptions from the docstring   to build the JSON schema. Missing descriptions raise a ValueError.</p> </li> </ul>"},{"location":"guides/Modules/#guides.4_modules--control-flow-modules","title":"Control Flow Modules","text":""},{"location":"guides/Modules/#guides.4_modules--decision-single-label-classification","title":"Decision: Single-Label Classification","text":"<p>The <code>Decision</code> module classifies input into one of several categories, enabling intelligent routing:</p> <pre><code>graph LR\n    A[Input] --&gt; B[Decision]\n    B --&gt; C{Which Label?}\n    C --&gt;|math| D[Math Handler]\n    C --&gt;|general| E[General Handler]\n    C --&gt;|code| F[Code Handler]</code></pre> <pre><code>decision = await synalinks.Decision(\n    question=\"What type of question is this?\",\n    labels=[\"math\", \"general\", \"code\"],\n    language_model=language_model,\n)(inputs)\n\n# Result: {\"choice\": \"math\"} or {\"choice\": \"general\"} etc.\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--branch-conditional-execution","title":"Branch: Conditional Execution","text":"<p>The <code>Branch</code> module combines decision-making with routing. It takes a question, labels, and corresponding branch modules:</p> <pre><code>(math_output, general_output) = await synalinks.Branch(\n    question=\"Is this a math or general question?\",\n    labels=[\"math\", \"general\"],\n    branches=[\n        synalinks.Generator(\n            data_model=Answer,\n            language_model=lm,\n            instructions=\"You are a math expert.\",\n        ),\n        synalinks.Generator(\n            data_model=Answer,\n            language_model=lm,\n            instructions=\"You are a general knowledge expert.\",\n        ),\n    ],\n    language_model=lm,\n)(inputs)\n\n# Combine with OR - only the selected branch produces output\noutputs = math_output | general_output\n</code></pre> <p>The Branch module:</p> <ol> <li>Asks the question to classify the input</li> <li>Routes to the corresponding branch (others return None)</li> <li>Use OR to combine the outputs</li> </ol>"},{"location":"guides/Modules/#guides.4_modules--action-context-injection","title":"Action: Context Injection","text":"<p>The <code>Action</code> module executes with injected context from previous steps:</p> <pre><code>outputs = await synalinks.Action(\n    language_model=language_model,\n    data_model=Answer,\n    context_key=\"search_results\",  # Inject under this key\n)(inputs, search_results)\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--merging-modules","title":"Merging Modules","text":"<p>When you have multiple data streams, merging modules combine them:</p> <pre><code>graph LR\n    A[DataModel A] --&gt; C[Merge Module]\n    B[DataModel B] --&gt; C\n    C --&gt; D[Combined DataModel]</code></pre>"},{"location":"guides/Modules/#guides.4_modules--concat-merge-all-fields","title":"Concat (+): Merge All Fields","text":"<p>Combines all fields from multiple DataModels:</p> <pre><code># Merge two outputs\nmerged = await synalinks.Concat()([output_a, output_b])\n# Result has all fields from both A and B\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--logical-and-merge-with-none-check","title":"Logical And (&amp;): Merge with None Check","text":"<p>Like Concat, but returns None if any input is None:</p> <pre><code># Only merge if both exist\nmerged = await synalinks.And()([output_a, output_b])\n# Result: merged DataModel or None\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--logical-or-ignore-none","title":"Logical Or (|): ignore None","text":"<p>Returns the first non-None value:</p> <pre><code># Fallback pattern\nresult = await synalinks.Or()([primary, fallback])\n# Returns primary if not None, else fallback\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--xor-exclusive-choice","title":"Xor (^): Exclusive Choice","text":"<p>Returns None if both inputs are provided:</p> <pre><code># For guard patterns\nresult = await synalinks.Xor()([warning, data])\n# If warning exists, data becomes None\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--masking-modules","title":"Masking Modules","text":"<p>Masking modules filter fields from DataModels:</p>"},{"location":"guides/Modules/#guides.4_modules--inmask-keep-specified-fields","title":"InMask: Keep Specified Fields","text":"<pre><code># Keep only \"answer\" field, drop everything else\nfiltered = await synalinks.InMask(mask=[\"answer\"])(full_output)\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--outmask-remove-specified-fields","title":"OutMask: Remove Specified Fields","text":"<pre><code># Remove \"thinking\" field, keep everything else\nfiltered = await synalinks.OutMask(mask=[\"thinking\"])(full_output)\n</code></pre> <p>Masking is useful for:</p> <ul> <li>Hiding intermediate reasoning from final output</li> <li>Reducing token usage in downstream modules</li> <li>Focusing training on specific fields</li> </ul>"},{"location":"guides/Modules/#guides.4_modules--test-time-compute-modules","title":"Test-Time Compute Modules","text":"<p>These modules use extra computation at inference time to improve quality:</p>"},{"location":"guides/Modules/#guides.4_modules--chainofthought-automatic-reasoning","title":"ChainOfThought: Automatic Reasoning","text":"<p>Adds a \"thinking\" field to encourage step-by-step reasoning:</p> <pre><code>outputs = await synalinks.ChainOfThought(\n    data_model=Answer,\n    language_model=language_model,\n)(inputs)\n\n# Result includes both \"thinking\" and \"answer\" fields\nprint(result['thinking'])  # Step-by-step reasoning\nprint(result['answer'])    # Final answer\n</code></pre> <p>The thinking field is automatically added to your schema - you don't need to include it in your DataModel.</p>"},{"location":"guides/Modules/#guides.4_modules--selfcritique-self-evaluation","title":"SelfCritique: Self-Evaluation","text":"<p>Generates output, then evaluates it with a reward function:</p> <pre><code>outputs = await synalinks.SelfCritique(\n    data_model=Answer,\n    language_model=language_model,\n    reward=synalinks.LMAsJudge(language_model=language_model),\n)(inputs)\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\n# =============================================================================\n# Data Models\n# =============================================================================\n\nclass Query(synalinks.DataModel):\n    \"\"\"User question.\"\"\"\n    query: str = synalinks.Field(description=\"User question\")\n\nclass Answer(synalinks.DataModel):\n    \"\"\"Simple answer.\"\"\"\n    answer: str = synalinks.Field(description=\"The answer\")\n\nclass AnswerWithThinking(synalinks.DataModel):\n    \"\"\"Answer with reasoning.\"\"\"\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n    answer: str = synalinks.Field(description=\"The final answer\")\n\n# =============================================================================\n# Main\n# =============================================================================\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # -------------------------------------------------------------------------\n    # Generator Example\n    # -------------------------------------------------------------------------\n    print(\"=\" * 60)\n    print(\"Module 1: Generator\")\n    print(\"=\" * 60)\n\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.Generator(\n        data_model=Answer,\n        language_model=lm,\n    )(inputs)\n\n    program = synalinks.Program(inputs=inputs, outputs=outputs)\n    result = await program(Query(query=\"What is Python?\"))\n    print(f\"Generator output: {result['answer'][:100]}...\")\n\n    # -------------------------------------------------------------------------\n    # Branch Example (in docstring)\n    # -------------------------------------------------------------------------\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Module 2: Branch\")\n    print(\"=\" * 60)\n\n    inputs = synalinks.Input(data_model=Query)\n\n    (math_out, general_out) = await synalinks.Branch(\n        question=\"Is this a math or general question?\",\n        labels=[\"math\", \"general\"],\n        branches=[\n            synalinks.Generator(\n                data_model=Answer,\n                language_model=lm,\n                instructions=\"Show your calculations.\",\n            ),\n            synalinks.Generator(\n                data_model=Answer,\n                language_model=lm,\n            ),\n        ],\n        language_model=lm,\n    )(inputs)\n\n    outputs = math_out | general_out\n\n    program = synalinks.Program(inputs=inputs, outputs=outputs)\n    result = await program(Query(query=\"What is 15 * 23?\"))\n    print(f\"Math result: {result['answer']}\")\n\n    # -------------------------------------------------------------------------\n    # ChainOfThought Example\n    # -------------------------------------------------------------------------\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Module 3: ChainOfThought\")\n    print(\"=\" * 60)\n\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.ChainOfThought(\n        data_model=Answer,\n        language_model=lm,\n    )(inputs)\n\n    program = synalinks.Program(inputs=inputs, outputs=outputs)\n    result = await program(Query(query=\"If I have 3 apples and give 1 away?\"))\n    print(f\"Thinking: {result['thinking'][:100]}...\")\n    print(f\"Answer: {result['answer']}\")\n\n    # -------------------------------------------------------------------------\n    # Masking Example\n    # -------------------------------------------------------------------------\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Module 4: Masking\")\n    print(\"=\" * 60)\n\n    inputs = synalinks.Input(data_model=Query)\n    full_output = await synalinks.Generator(\n        data_model=AnswerWithThinking,\n        language_model=lm,\n    )(inputs)\n\n    # Keep only the answer field\n    masked = await synalinks.InMask(mask=[\"answer\"])(full_output)\n\n    program = synalinks.Program(inputs=inputs, outputs=masked)\n    result = await program(Query(query=\"What is 1+1?\"))\n    print(f\"Masked fields: {list(result.get_json().keys())}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Input: Defines the entry point of your program. Every program needs at least one.</p> </li> <li> <p>Generator: The core LLM module. Takes input, produces structured output   matching your DataModel schema. Supports instructions and is trainable.</p> </li> <li> <p>Decision + Branch: Enable intelligent routing. Decision classifies,   Branch routes to the appropriate handler.</p> </li> <li> <p>ChainOfThought: Automatically adds step-by-step reasoning to improve   accuracy on complex tasks.</p> </li> <li> <p>Merging Operators: <code>+</code> (Concat), <code>&amp;</code> (And), <code>|</code> (Or), <code>^</code> (Xor) combine   DataModels in different ways for different use cases.</p> </li> <li> <p>InMask/OutMask: Filter fields to hide intermediate work or focus on   specific outputs.</p> </li> </ul>"},{"location":"guides/Modules/#guides.4_modules--api-references","title":"API References","text":"<ul> <li>Generator</li> <li>Decision</li> <li>Branch</li> <li>ChainOfThought</li> <li>InMask/OutMask</li> <li>Tool</li> </ul>"},{"location":"guides/Modules/#guides.4_modules.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Simple answer.</p> Source code in <code>guides/4_modules.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Simple answer.\"\"\"\n\n    answer: str = synalinks.Field(description=\"The answer\")\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Answer with reasoning.</p> Source code in <code>guides/4_modules.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"Answer with reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n    answer: str = synalinks.Field(description=\"The final answer\")\n</code></pre>"},{"location":"guides/Modules/#guides.4_modules.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User question.</p> Source code in <code>guides/4_modules.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User question.\"\"\"\n\n    query: str = synalinks.Field(description=\"User question\")\n</code></pre>"},{"location":"guides/Observability/","title":"Observability","text":""},{"location":"guides/Observability/#guides.8_observability--observability","title":"Observability","text":"<p>Observability is the ability to understand the internal state of a system by examining its outputs. In LM applications, this means tracking every prompt, response, token usage, and decision - enabling you to debug issues, optimize performance, and monitor production systems.</p>"},{"location":"guides/Observability/#guides.8_observability--why-observability-matters","title":"Why Observability Matters","text":"<p>LM applications are inherently non-deterministic and complex. Without observability, you're flying blind:</p> <pre><code>graph LR\n    subgraph Without Observability\n        A[Input] --&gt; B[Black Box]\n        B --&gt; C[Output]\n        C --&gt; D[\"Why did it fail?\"]\n    end\n    subgraph With Observability\n        E[Input] --&gt; F[Traced Pipeline]\n        F --&gt; G[Output]\n        H[Traces] --&gt; I[Debug &amp; Optimize]\n    end</code></pre> <p>Observability enables:</p> <ol> <li>Debugging: See exactly what prompts were sent and responses received</li> <li>Performance Monitoring: Track latency, token usage, and costs</li> <li>Quality Assurance: Identify and fix problematic outputs</li> <li>Optimization: Find bottlenecks and improve efficiency</li> </ol>"},{"location":"guides/Observability/#guides.8_observability--enabling-observability","title":"Enabling Observability","text":"<p>Synalinks uses MLflow for tracing and metrics:</p> <pre><code>import synalinks\n\nsynalinks.enable_observability(\n    tracking_uri=\"http://localhost:5000\",  # MLflow server\n    experiment_name=\"my_experiment\",        # Group related runs\n)\n</code></pre> <p>Start the MLflow UI:</p> <pre><code>mlflow ui --port 5000\n</code></pre> <p>Then open http://localhost:5000 in your browser.</p>"},{"location":"guides/Observability/#guides.8_observability--what-gets-traced","title":"What Gets Traced","text":"<p>Every operation in your program is automatically traced:</p> <pre><code>graph TD\n    A[Program Call] --&gt; B[Trace]\n    B --&gt; C[Module: Input]\n    B --&gt; D[Module: Generator]\n    D --&gt; E[LLM Call]\n    E --&gt; F[Prompt]\n    E --&gt; G[Response]\n    E --&gt; H[Token Count]\n    B --&gt; I[Module: Branch]\n    I --&gt; J[Selected Path]</code></pre>"},{"location":"guides/Observability/#guides.8_observability--trace-contents","title":"Trace Contents","text":"Component What's Captured Program Input/output DataModels, execution time Module Each module's inputs, outputs, parameters LLM Call Full prompt, response, model name, tokens Tool Call Tool name, arguments, result Training Metrics, hyperparameters, artifacts"},{"location":"guides/Observability/#guides.8_observability--logging-levels","title":"Logging Levels","text":"<p>Control log verbosity for debugging:</p> <pre><code>import synalinks\n\n# Detailed logging - every LLM call logged\nsynalinks.enable_logging(log_level=\"debug\")\n\n# Standard logging - key events only\nsynalinks.enable_logging(log_level=\"info\")\n\n# Quiet logging - warnings and errors only\nsynalinks.enable_logging(log_level=\"warning\")\n</code></pre>"},{"location":"guides/Observability/#guides.8_observability--debugging-tools","title":"Debugging Tools","text":""},{"location":"guides/Observability/#guides.8_observability--program-summary","title":"Program Summary","text":"<p>Inspect your program's structure:</p> <pre><code>program.summary()\n</code></pre> <p>Output: </p><pre><code>Program: qa_program\ndescription: 'A `Functional` program is a `Program` defined as a directed graph\nof modules.'\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Module (type)               \u2503 Output Schema         \u2503    Variable # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_module (InputModule)  \u2502 Query                 \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 generator (Generator)       \u2502 Answer                \u2502             2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n Total variables: 2\n Trainable variables: 2\n</code></pre><p></p>"},{"location":"guides/Observability/#guides.8_observability--program-visualization","title":"Program Visualization","text":"<p>Generate a visual graph of your program:</p> <pre><code>synalinks.utils.plot_program(\n    program,\n    to_folder=\"output\",\n    show_module_names=True,\n    show_trainable=True,\n)\n</code></pre> <p>This creates a PNG file showing the computation graph.</p>"},{"location":"guides/Observability/#guides.8_observability--trainable-variables","title":"Trainable Variables","text":"<p>Inspect what can be optimized:</p> <pre><code>for var in program.trainable_variables:\n    print(f\"Variable: {var.name}\")\n    print(f\"  Value: {var.value}\")\n</code></pre>"},{"location":"guides/Observability/#guides.8_observability--mlflow-integration","title":"MLflow Integration","text":""},{"location":"guides/Observability/#guides.8_observability--viewing-traces","title":"Viewing Traces","text":"<p>Navigate to the MLflow UI at http://localhost:5000:</p> <ol> <li>Select your experiment from the sidebar</li> <li>Click on a run to see its traces</li> <li>Expand traces to see individual module calls</li> <li>Click on LLM calls to see full prompts/responses</li> </ol>"},{"location":"guides/Observability/#guides.8_observability--trace-structure","title":"Trace Structure","text":"<p>Each trace shows the hierarchical execution:</p> <pre><code>Program Call: qa_program\n\u251c\u2500\u2500 Module: Input\n\u2502   \u251c\u2500\u2500 Input: {\"query\": \"What is Python?\"}\n\u2502   \u2514\u2500\u2500 Duration: 0.001s\n\u251c\u2500\u2500 Module: Generator\n\u2502   \u251c\u2500\u2500 LLM Call: openai/gpt-4.1-mini\n\u2502   \u2502   \u251c\u2500\u2500 Prompt: [full text]\n\u2502   \u2502   \u251c\u2500\u2500 Response: [full text]\n\u2502   \u2502   \u251c\u2500\u2500 Input tokens: 150\n\u2502   \u2502   \u2514\u2500\u2500 Output tokens: 50\n\u2502   \u2514\u2500\u2500 Duration: 1.2s\n\u2514\u2500\u2500 Output: {\"answer\": \"Python is...\"}\n</code></pre>"},{"location":"guides/Observability/#guides.8_observability--training-metrics","title":"Training Metrics","text":"<p>During training, MLflow captures:</p> <ul> <li>Per-epoch metrics (reward, accuracy)</li> <li>Validation metrics</li> <li>Hyperparameters (optimizer settings, epochs)</li> <li>Artifacts (saved program checkpoints)</li> </ul>"},{"location":"guides/Observability/#guides.8_observability--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    \"\"\"User question.\"\"\"\n    query: str = synalinks.Field(description=\"User question\")\n\nclass Answer(synalinks.DataModel):\n    \"\"\"Answer with reasoning.\"\"\"\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n    answer: str = synalinks.Field(description=\"The final answer\")\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    # Enable observability\n    synalinks.enable_observability(\n        tracking_uri=\"http://localhost:5000\",\n        experiment_name=\"observability_demo\",\n    )\n\n    # Enable logging\n    synalinks.enable_logging(log_level=\"info\")\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Create program\n    inputs = synalinks.Input(data_model=Query)\n    outputs = await synalinks.Generator(\n        data_model=Answer,\n        language_model=lm,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"traced_qa\",\n    )\n\n    # Print summary\n    program.summary()\n\n    # Run program - traces are automatically captured\n    result = await program(Query(query=\"What is Python?\"))\n    print(f\"Answer: {result['answer']}\")\n\n    # Visualize program\n    synalinks.utils.plot_program(\n        program,\n        show_module_names=True,\n    )\n\n    # Inspect trainable variables\n    print(\"\\nTrainable variables:\")\n    for var in program.trainable_variables:\n        print(f\"  - {var.name}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Observability/#guides.8_observability--production-monitoring","title":"Production Monitoring","text":"<p>For production deployments, set up persistent MLflow tracking:</p> <pre><code>import synalinks\n\n# Use a remote MLflow server\nsynalinks.enable_observability(\n    tracking_uri=\"http://mlflow.your-domain.com:5000\",\n    experiment_name=\"production\",\n)\n</code></pre> <p>Monitor key metrics:</p> <ul> <li>Latency: Time per request</li> <li>Token Usage: Cost per request</li> <li>Error Rate: Failed requests</li> <li>Quality Scores: If using LMAsJudge or similar</li> </ul>"},{"location":"guides/Observability/#guides.8_observability--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>MLflow Integration: Synalinks uses MLflow for comprehensive tracing   and metrics. All traces are automatically captured.</p> </li> <li> <p>enable_observability(): Call this at startup with your MLflow server   URI and experiment name.</p> </li> <li> <p>Trace Hierarchy: Traces show the full execution path from program   to individual LLM calls.</p> </li> <li> <p>Debugging Tools: Use <code>program.summary()</code>, <code>plot_program()</code>, and   <code>trainable_variables</code> for inspection.</p> </li> <li> <p>Logging Levels: Control verbosity with <code>enable_logging()</code> - use   DEBUG for development, WARNING for production.</p> </li> <li> <p>Production Ready: Point to a remote MLflow server for production   monitoring and alerting.</p> </li> </ul>"},{"location":"guides/Observability/#guides.8_observability--api-references","title":"API References","text":"<ul> <li>enable_observability</li> <li>enable_logging</li> <li>plot_program</li> <li>Program.summary</li> </ul>"},{"location":"guides/Observability/#guides.8_observability.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Answer with reasoning.</p> Source code in <code>guides/8_observability.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Answer with reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n    answer: str = synalinks.Field(description=\"The final answer\")\n</code></pre>"},{"location":"guides/Observability/#guides.8_observability.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User question.</p> Source code in <code>guides/8_observability.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User question.\"\"\"\n\n    query: str = synalinks.Field(description=\"User question\")\n</code></pre>"},{"location":"guides/Output%20Guard/","title":"Output Guard","text":""},{"location":"guides/Output%20Guard/#guides.10_output_guard--output-guard-patterns","title":"Output Guard Patterns","text":"<p>Output Guards protect your LM applications by filtering dangerous or inappropriate outputs AFTER they are generated by the language model. This ensures safe responses even when the LLM produces unexpected content.</p>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--why-output-guards-matter","title":"Why Output Guards Matter","text":"<p>LLMs can sometimes produce unexpected or inappropriate outputs:</p> <pre><code>graph LR\n    subgraph Without Guards\n        A[Input] --&gt; B[LLM]\n        B --&gt; C[Unsafe Output?]\n    end\n    subgraph With Output Guard\n        D[Input] --&gt; E[LLM]\n        E --&gt; F[Response]\n        F --&gt; G{Output Guard}\n        G --&gt;|Safe| H[Response]\n        G --&gt;|Unsafe| I[Warning]\n    end</code></pre> <p>Output guards provide:</p> <ol> <li>Safety Net: Catch inappropriate content before it reaches users</li> <li>Compliance: Ensure outputs meet regulatory requirements</li> <li>Brand Protection: Filter content that doesn't match your brand voice</li> <li>Graceful Replacement: Substitute unsafe content with helpful alternatives</li> </ol>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--the-xor-and-or-operators-for-output-guards","title":"The XOR and OR Operators for Output Guards","text":"<p>For output guards, we use XOR and OR slightly differently than input guards:</p>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--pattern-check-and-replace","title":"Pattern: Check-and-Replace","text":"<pre><code>graph LR\n    A[inputs] --&gt; B[Generator]\n    B --&gt; C[answer]\n    C --&gt; D[OutputGuard]\n    D --&gt; E[warning]\n    E --&gt; F[\"warning ^ answer\"]\n    C --&gt; F\n    F --&gt; G[\"safe_answer (None if blocked)\"]\n    E --&gt; H[\"warning | safe_answer\"]\n    G --&gt; H\n    H --&gt; I[output]</code></pre> <p>Flow when output is UNSAFE: 1. Generator produces answer 2. Guard checks answer, returns warning (not None) 3. XOR: warning ^ answer = None (invalidates answer) 4. OR: warning | None = warning (use warning)</p> <p>Flow when output is SAFE: 1. Generator produces answer 2. Guard checks answer, returns None (no warning) 3. XOR: None ^ answer = answer (keep answer) 4. OR: None | answer = answer (use answer)</p>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--building-an-output-guard","title":"Building an Output Guard","text":"<p>An output guard is a custom Module that: - Receives the LLM's output - Returns <code>None</code> when output is safe - Returns a warning DataModel when output should be replaced</p> <pre><code>import synalinks\n\nclass OutputGuard(synalinks.Module):\n    \"\"\"Guard that replaces outputs containing blacklisted words.\"\"\"\n\n    def __init__(self, blacklisted_words, warning_message, **kwargs):\n        super().__init__(**kwargs)\n        self.blacklisted_words = blacklisted_words\n        self.warning_message = warning_message\n\n    async def call(self, inputs, training=False):\n        \"\"\"Return warning if output should be replaced, None otherwise.\"\"\"\n        if inputs is None:\n            return None\n\n        # Check the answer field for blacklisted words\n        answer = inputs.get(\"answer\", \"\").lower()\n\n        for word in self.blacklisted_words:\n            if word.lower() in answer:\n                # Return warning - this will replace the answer\n                return Answer(answer=self.warning_message).to_json_data_model()\n\n        # Output is safe - return None to keep original\n        return None\n\n    async def compute_output_spec(self, inputs, training=False):\n        \"\"\"Define output schema (same as input for replacement).\"\"\"\n        return inputs\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"User query\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The answer\")\n\nclass OutputGuard(synalinks.Module):\n    def __init__(self, blacklisted_words, warning_message, **kwargs):\n        super().__init__(**kwargs)\n        self.blacklisted_words = blacklisted_words\n        self.warning_message = warning_message\n\n    async def call(self, inputs, training=False):\n        if inputs is None:\n            return None\n        answer = inputs.get(\"answer\", \"\").lower()\n        for word in self.blacklisted_words:\n            if word.lower() in answer:\n                return Answer(answer=self.warning_message).to_json_data_model()\n        return None\n\n    async def compute_output_spec(self, inputs, training=False):\n        return inputs  # Same schema as input\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Build the guarded program\n    inputs = synalinks.Input(data_model=Query)\n\n    # Generate answer\n    answer = await synalinks.Generator(\n        data_model=Answer,\n        language_model=lm,\n    )(inputs)\n\n    # Guard checks for blacklisted words in output\n    warning = await OutputGuard(\n        blacklisted_words=[\"violence\", \"harmful\", \"dangerous\"],\n        warning_message=\"I cannot provide that information.\",\n    )(answer)\n\n    # XOR: If warning exists, invalidate the answer\n    safe_answer = warning ^ answer\n\n    # OR: Return warning if it exists, otherwise return safe_answer\n    outputs = warning | safe_answer\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"output_guarded_qa\",\n    )\n\n    # Test with various queries\n    result = await program(Query(query=\"What is Python?\"))\n    print(f\"Result: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Post-Processing: Output guards check content AFTER the LLM generates it,   acting as a safety net.</p> </li> <li> <p>Same Schema: Output guards typically return the same schema as the input   they check, enabling seamless replacement.</p> </li> <li> <p>XOR for Invalidation: When guard triggers, XOR invalidates the original   answer by producing None.</p> </li> <li> <p>OR for Replacement: OR selects the warning (replacement) when the   original answer is invalidated.</p> </li> <li> <p>Defense in Depth: Combine with input guards for comprehensive protection.</p> </li> </ul>"},{"location":"guides/Output%20Guard/#guides.10_output_guard--api-references","title":"API References","text":"<ul> <li>Module</li> <li>JSON Ops</li> </ul>"},{"location":"guides/Output%20Guard/#guides.10_output_guard.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Answer to the query.</p> Source code in <code>guides/10_output_guard.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Answer to the query.\"\"\"\n\n    answer: str = synalinks.Field(description=\"The answer\")\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard.OutputGuard","title":"<code>OutputGuard</code>","text":"<p>               Bases: <code>Module</code></p> <p>Guard that replaces outputs containing blacklisted words.</p> <p>Returns None when output is safe, or a replacement Answer when output should be filtered.</p> Source code in <code>guides/10_output_guard.py</code> <pre><code>class OutputGuard(synalinks.Module):\n    \"\"\"Guard that replaces outputs containing blacklisted words.\n\n    Returns None when output is safe, or a replacement Answer when output\n    should be filtered.\n    \"\"\"\n\n    def __init__(self, blacklisted_words, replacement_message, **kwargs):\n        super().__init__(**kwargs)\n        self.blacklisted_words = blacklisted_words\n        self.replacement_message = replacement_message\n\n    async def call(\n        self,\n        inputs: synalinks.JsonDataModel,\n        training: bool = False,\n    ) -&gt; synalinks.JsonDataModel:\n        \"\"\"Return replacement if output should be filtered, None otherwise.\"\"\"\n        if inputs is None:\n            return None\n\n        answer = inputs.get(\"answer\", \"\").lower()\n\n        for word in self.blacklisted_words:\n            if word.lower() in answer:\n                return Answer(answer=self.replacement_message).to_json_data_model()\n\n        return None\n\n    async def compute_output_spec(\n        self,\n        inputs: synalinks.SymbolicDataModel,\n        training: bool = False,\n    ) -&gt; synalinks.SymbolicDataModel:\n        \"\"\"Define output schema (same type as Answer for replacement).\"\"\"\n        return Answer.to_symbolic_data_model(name=self.name)\n\n    def get_config(self):\n        \"\"\"Serialization config.\"\"\"\n        return {\n            \"name\": self.name,\n            \"blacklisted_words\": self.blacklisted_words,\n            \"replacement_message\": self.replacement_message,\n        }\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard.OutputGuard.call","title":"<code>call(inputs, training=False)</code>  <code>async</code>","text":"<p>Return replacement if output should be filtered, None otherwise.</p> Source code in <code>guides/10_output_guard.py</code> <pre><code>async def call(\n    self,\n    inputs: synalinks.JsonDataModel,\n    training: bool = False,\n) -&gt; synalinks.JsonDataModel:\n    \"\"\"Return replacement if output should be filtered, None otherwise.\"\"\"\n    if inputs is None:\n        return None\n\n    answer = inputs.get(\"answer\", \"\").lower()\n\n    for word in self.blacklisted_words:\n        if word.lower() in answer:\n            return Answer(answer=self.replacement_message).to_json_data_model()\n\n    return None\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard.OutputGuard.compute_output_spec","title":"<code>compute_output_spec(inputs, training=False)</code>  <code>async</code>","text":"<p>Define output schema (same type as Answer for replacement).</p> Source code in <code>guides/10_output_guard.py</code> <pre><code>async def compute_output_spec(\n    self,\n    inputs: synalinks.SymbolicDataModel,\n    training: bool = False,\n) -&gt; synalinks.SymbolicDataModel:\n    \"\"\"Define output schema (same type as Answer for replacement).\"\"\"\n    return Answer.to_symbolic_data_model(name=self.name)\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard.OutputGuard.get_config","title":"<code>get_config()</code>","text":"<p>Serialization config.</p> Source code in <code>guides/10_output_guard.py</code> <pre><code>def get_config(self):\n    \"\"\"Serialization config.\"\"\"\n    return {\n        \"name\": self.name,\n        \"blacklisted_words\": self.blacklisted_words,\n        \"replacement_message\": self.replacement_message,\n    }\n</code></pre>"},{"location":"guides/Output%20Guard/#guides.10_output_guard.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User query.</p> Source code in <code>guides/10_output_guard.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User query.\"\"\"\n\n    query: str = synalinks.Field(description=\"User query\")\n</code></pre>"},{"location":"guides/Programs/","title":"Programs","text":""},{"location":"guides/Programs/#guides.3_programs--programs","title":"Programs","text":"<p>A Program in Synalinks is the fundamental unit of deployment and training. Just as a function encapsulates logic in traditional programming, a Program encapsulates the entire computation graph of your Language Model application, from input to output, including all intermediate transformations.</p>"},{"location":"guides/Programs/#guides.3_programs--why-programs-matter","title":"Why Programs Matter","text":"<p>In traditional LLM development, you write procedural code that calls APIs:</p> <pre><code>graph LR\n    subgraph Traditional Approach\n        A[Function] --&gt; B[API Call 1]\n        B --&gt; C[Parse]\n        C --&gt; D[API Call 2]\n        D --&gt; E[Return]\n    end</code></pre> <p>This approach has limitations: no training, no serialization, no visualization.</p> <p>Synalinks Programs provide a declarative computation graph:</p> <pre><code>graph LR\n    subgraph Synalinks Program\n        A[Input DataModel] --&gt; B[Module 1]\n        B --&gt; C[Module 2]\n        C --&gt; D[Output DataModel]\n    end\n    E[Training] -.-&gt; B\n    E -.-&gt; C\n    F[Save/Load] -.-&gt; B\n    F -.-&gt; C</code></pre> <p>Programs provide:</p> <ol> <li>Trainability: Optimize instructions and examples over time</li> <li>Serialization: Save and load trained state</li> <li>Visualization: Understand your computation graph</li> <li>Composability: Nest programs within programs</li> </ol>"},{"location":"guides/Programs/#guides.3_programs--the-four-program-creation-strategies","title":"The Four Program Creation Strategies","text":"<p>Synalinks offers four distinct strategies for creating programs, each suited to different use cases:</p> <pre><code>graph TD\n    A[Program Creation] --&gt; B[Functional API]\n    A --&gt; C[Subclassing API]\n    A --&gt; D[Sequential API]\n    A --&gt; E[Mixing Strategy]\n    B --&gt; F[\"Most Flexible&lt;br&gt;(Recommended)\"]\n    C --&gt; G[\"Custom Logic&lt;br&gt;in call()\"]\n    D --&gt; H[\"Simple Linear&lt;br&gt;Pipelines\"]\n    E --&gt; I[\"Reusable&lt;br&gt;Components\"]</code></pre>"},{"location":"guides/Programs/#guides.3_programs--strategy-1-the-functional-api-recommended","title":"Strategy 1: The Functional API (Recommended)","text":"<p>The Functional API is the most powerful and flexible approach. You build a computation graph by chaining module calls, starting from an <code>Input</code> node:</p> <pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\nclass Query(synalinks.DataModel):\n    \"\"\"User question.\"\"\"\n    query: str = synalinks.Field(description=\"User question\")\n\nclass Answer(synalinks.DataModel):\n    \"\"\"Answer with reasoning.\"\"\"\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n    answer: str = synalinks.Field(description=\"The final answer\")\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # Step 1: Define the entry point\n    inputs = synalinks.Input(data_model=Query)\n\n    # Step 2: Chain module calls (this builds the graph)\n    outputs = await synalinks.Generator(\n        data_model=Answer,\n        language_model=lm,\n    )(inputs)\n\n    # Step 3: Wrap in a Program\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"qa_program\",\n    )\n\n    # Step 4: Use the program\n    result = await program(Query(query=\"What is 2+2?\"))\n    print(f\"Answer: {result['answer']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>The Functional API excels at:</p> <ul> <li>Parallel branches: Multiple modules can process the same input</li> <li>Complex routing: Decisions and branches based on content</li> <li>Merging: Combining outputs from multiple paths</li> </ul>"},{"location":"guides/Programs/#guides.3_programs--strategy-2-the-subclassing-api","title":"Strategy 2: The Subclassing API","text":"<p>The Subclassing API gives you complete control over the execution logic. You inherit from <code>synalinks.Program</code> and override the <code>call()</code> method:</p> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"User question\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The final answer\")\n\nclass QAProgram(synalinks.Program):\n    \"\"\"A custom QA program using subclassing.\"\"\"\n\n    def __init__(self, language_model, **kwargs):\n        super().__init__(**kwargs)\n        self.language_model = language_model\n        # Create modules in __init__\n        self.generator = synalinks.Generator(\n            data_model=Answer,\n            language_model=language_model,\n        )\n\n    async def call(\n        self,\n        inputs: synalinks.JsonDataModel,\n        training: bool = False,\n    ) -&gt; synalinks.JsonDataModel:\n        # Custom logic here\n        return await self.generator(inputs, training=training)\n</code></pre> <p>Use the Subclassing API when you need:</p> <ul> <li>Custom logic that doesn't fit the functional paradigm</li> <li>State management beyond trainable variables</li> <li>Integration with external systems during execution</li> </ul>"},{"location":"guides/Programs/#guides.3_programs--strategy-3-the-sequential-api","title":"Strategy 3: The Sequential API","text":"<p>The Sequential API is the simplest approach for linear pipelines where each module feeds directly into the next:</p> <pre><code>graph LR\n    A[Input] --&gt; B[Module 1]\n    B --&gt; C[Module 2]\n    C --&gt; D[Module 3]\n    D --&gt; E[Output]</code></pre> <pre><code>import synalinks\n\nclass Query(synalinks.DataModel):\n    query: str = synalinks.Field(description=\"User question\")\n\nclass Thinking(synalinks.DataModel):\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n\nclass Answer(synalinks.DataModel):\n    answer: str = synalinks.Field(description=\"The final answer\")\n\nlm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n# Simple linear pipeline using .add() method\nprogram = synalinks.Sequential(\n    name=\"sequential_qa\",\n    description=\"A sequential question-answering pipeline\",\n)\nprogram.add(synalinks.Input(data_model=Query))\nprogram.add(synalinks.Generator(data_model=Thinking, language_model=lm))\nprogram.add(synalinks.Generator(data_model=Answer, language_model=lm))\n</code></pre> <p>The Sequential API is ideal for:</p> <ul> <li>Simple, linear processing pipelines</li> <li>Quick prototyping</li> <li>When each step naturally flows to the next</li> </ul>"},{"location":"guides/Programs/#guides.3_programs--strategy-4-the-mixing-strategy","title":"Strategy 4: The Mixing Strategy","text":"<p>The Mixing Strategy combines subclassing with the Functional API to create reusable components that can be used inside other programs:</p> <pre><code>graph TD\n    subgraph Reusable Component\n        A[build] --&gt; B[Create Functional Graph]\n        B --&gt; C[Reinitialize as Program]\n    end\n    subgraph Main Program\n        D[Input] --&gt; E[Component]\n        E --&gt; F[More Processing]\n        F --&gt; G[Output]\n    end</code></pre> <pre><code>import synalinks\n\nclass ChainOfThought(synalinks.Program):\n    \"\"\"Reusable chain-of-thought component.\"\"\"\n\n    def __init__(self, language_model, **kwargs):\n        super().__init__(**kwargs)\n        self.language_model = language_model\n\n    async def build(self, inputs: synalinks.SymbolicDataModel) -&gt; None:\n        \"\"\"Build the computation graph when first called.\"\"\"\n        outputs = await synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=self.language_model,\n        )(inputs)\n\n        # Reinitialize with the built graph\n        super().__init__(\n            inputs=inputs,\n            outputs=outputs,\n            name=self.name,\n        )\n</code></pre> <p>The Mixing Strategy is powerful for:</p> <ul> <li>Creating library components</li> <li>Encapsulating complex sub-graphs</li> <li>Building a toolkit of reusable patterns</li> </ul>"},{"location":"guides/Programs/#guides.3_programs--program-features","title":"Program Features","text":""},{"location":"guides/Programs/#guides.3_programs--saving-and-loading","title":"Saving and Loading","text":"<p>Programs serialize their entire state to JSON:</p> <pre><code># Save a program\nprogram.save(\"my_program.json\")\n\n# Load a program\nloaded = synalinks.Program.load(\"my_program.json\")\n</code></pre> <p>This includes all trainable variables (optimized instructions and examples).</p>"},{"location":"guides/Programs/#guides.3_programs--program-summary","title":"Program Summary","text":"<p>Inspect your program's structure:</p> <pre><code>program.summary()\n</code></pre> <p>Output: </p><pre><code>Program: qa_program\n===============================\n| Module          | Trainable |\n|-----------------|-----------|\n| Input           | No        |\n| Generator       | Yes       |\n===============================\nTotal parameters: 2\nTrainable parameters: 2\n</code></pre><p></p>"},{"location":"guides/Programs/#guides.3_programs--batch-inference","title":"Batch Inference","text":"<p>Process multiple inputs efficiently:</p> <pre><code>results = await program.predict([query1, query2, query3])\n</code></pre>"},{"location":"guides/Programs/#guides.3_programs--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>Functional API: The recommended approach for most use cases. Build   computation graphs by chaining module calls from <code>Input</code> to outputs. Supports   parallel branches, decisions, and complex routing.</p> </li> <li> <p>Subclassing API: Use when you need custom logic in the <code>call()</code> method.   Gives you complete control but loses some declarative benefits.</p> </li> <li> <p>Sequential API: Perfect for simple linear pipelines where modules feed   directly into each other. Minimal boilerplate.</p> </li> <li> <p>Mixing Strategy: Create reusable components that can be embedded in   other programs. Best for building a library of patterns.</p> </li> <li> <p>Serialization: All programs can be saved to JSON and loaded back,   preserving trained state and configuration.</p> </li> <li> <p>Program.summary(): Use this to inspect your program's structure and   identify trainable modules.</p> </li> </ul>"},{"location":"guides/Programs/#guides.3_programs--api-references","title":"API References","text":"<ul> <li>Program</li> <li>Sequential</li> <li>Input</li> <li>Generator</li> </ul>"},{"location":"guides/Programs/#guides.3_programs.Answer","title":"<code>Answer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Final answer.</p> Source code in <code>guides/3_programs.py</code> <pre><code>class Answer(synalinks.DataModel):\n    \"\"\"Final answer.\"\"\"\n\n    answer: str = synalinks.Field(description=\"The final answer\")\n</code></pre>"},{"location":"guides/Programs/#guides.3_programs.AnswerWithThinking","title":"<code>AnswerWithThinking</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Answer with reasoning.</p> Source code in <code>guides/3_programs.py</code> <pre><code>class AnswerWithThinking(synalinks.DataModel):\n    \"\"\"Answer with reasoning.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n    answer: str = synalinks.Field(description=\"The final answer\")\n</code></pre>"},{"location":"guides/Programs/#guides.3_programs.ChainOfThought","title":"<code>ChainOfThought</code>","text":"<p>               Bases: <code>Program</code></p> <p>Reusable chain-of-thought component.</p> Source code in <code>guides/3_programs.py</code> <pre><code>class ChainOfThought(synalinks.Program):\n    \"\"\"Reusable chain-of-thought component.\"\"\"\n\n    def __init__(self, language_model, **kwargs):\n        super().__init__(**kwargs)\n        self.language_model = language_model\n\n    async def build(self, inputs: synalinks.SymbolicDataModel) -&gt; None:\n        outputs = await synalinks.Generator(\n            data_model=AnswerWithThinking,\n            language_model=self.language_model,\n        )(inputs)\n\n        super().__init__(\n            inputs=inputs,\n            outputs=outputs,\n            name=self.name,\n        )\n</code></pre>"},{"location":"guides/Programs/#guides.3_programs.QAProgram","title":"<code>QAProgram</code>","text":"<p>               Bases: <code>Program</code></p> <p>A QA program using subclassing.</p> Source code in <code>guides/3_programs.py</code> <pre><code>class QAProgram(synalinks.Program):\n    \"\"\"A QA program using subclassing.\"\"\"\n\n    def __init__(self, language_model, **kwargs):\n        super().__init__(**kwargs)\n        self.language_model = language_model\n        self.generator = synalinks.Generator(\n            data_model=Answer,\n            language_model=language_model,\n        )\n\n    async def call(\n        self,\n        inputs: synalinks.JsonDataModel,\n        training: bool = False,\n    ) -&gt; synalinks.JsonDataModel:\n        return await self.generator(inputs, training=training)\n</code></pre>"},{"location":"guides/Programs/#guides.3_programs.Query","title":"<code>Query</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>User question.</p> Source code in <code>guides/3_programs.py</code> <pre><code>class Query(synalinks.DataModel):\n    \"\"\"User question.\"\"\"\n\n    query: str = synalinks.Field(description=\"User question\")\n</code></pre>"},{"location":"guides/Programs/#guides.3_programs.ThinkingOutput","title":"<code>ThinkingOutput</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>Intermediate thinking output.</p> Source code in <code>guides/3_programs.py</code> <pre><code>class ThinkingOutput(synalinks.DataModel):\n    \"\"\"Intermediate thinking output.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Step by step thinking\")\n</code></pre>"},{"location":"guides/Training/","title":"Training","text":""},{"location":"guides/Training/#guides.7_training--training","title":"Training","text":"<p>Training in Synalinks is fundamentally different from traditional machine learning. Instead of updating model weights through backpropagation, Synalinks uses in-context learning optimization - improving your programs by optimizing the prompts, instructions, and examples that guide the language model.</p>"},{"location":"guides/Training/#guides.7_training--the-philosophy-of-in-context-learning","title":"The Philosophy of In-Context Learning","text":"<p>Traditional ML updates weights; Synalinks updates context:</p> <pre><code>graph LR\n    subgraph Traditional ML\n        A[Data] --&gt; B[Backprop]\n        B --&gt; C[Update Weights]\n        C --&gt; D[Better Model]\n    end\n    subgraph Synalinks Training\n        E[Data] --&gt; F[Evaluate]\n        F --&gt; G[Update Context]\n        G --&gt; H[Better Prompts/Examples]\n    end</code></pre> <p>This approach has key advantages:</p> <ol> <li>No Gradient Computation: Works with any LLM API</li> <li>Interpretable: You can read and understand what was learned</li> <li>Modular: Each module learns independently</li> <li>Fast: No heavy computation - just prompt optimization</li> </ol>"},{"location":"guides/Training/#guides.7_training--what-gets-optimized","title":"What Gets Optimized","text":"<p>Each Generator module has two trainable variables:</p> <pre><code>graph TD\n    A[Generator] --&gt; B[instruction_variable]\n    A --&gt; C[examples_variable]\n    B --&gt; D[\"System prompt optimization\"]\n    C --&gt; E[\"Few-shot example selection\"]</code></pre> <ul> <li>instruction_variable: The system prompt or instruction prefix</li> <li>examples_variable: Few-shot examples injected into the prompt</li> </ul>"},{"location":"guides/Training/#guides.7_training--the-training-loop","title":"The Training Loop","text":"<p>Training follows a familiar pattern:</p> <pre><code>import synalinks\n\n# 1. Create your program\nprogram = synalinks.Program(inputs=inputs, outputs=outputs)\n\n# 2. Compile with optimizer and reward\nprogram.compile(\n    optimizer=synalinks.optimizers.RandomFewShot(nb_max_examples=3),\n    reward=synalinks.ExactMatch(key=\"answer\"),\n)\n\n# 3. Train\nhistory = await program.fit(\n    x=training_data,\n    epochs=5,\n    validation_data=test_data,\n    verbose=1,\n)\n\n# 4. Save the trained program\nprogram.save(\"trained_program.json\")\n</code></pre>"},{"location":"guides/Training/#guides.7_training--training-data-format","title":"Training Data Format","text":"<p>Training data consists of separate NumPy arrays for inputs (x) and expected outputs (y):</p> <pre><code>import numpy as np\n\nx_train = np.array(\n    [\n        InputModel(field=\"value\"),\n        InputModel(field=\"value2\"),\n        # ... more examples\n    ],\n    dtype=\"object\",\n)\ny_train = np.array(\n    [\n        OutputModel(result=\"expected\"),\n        OutputModel(result=\"expected2\"),\n        # ... more examples\n    ],\n    dtype=\"object\",\n)\n</code></pre> <p>Both arrays must contain DataModel instances matching your program's input and output schemas.</p>"},{"location":"guides/Training/#guides.7_training--optimizers","title":"Optimizers","text":"<p>Optimizers determine how trainable variables are updated:</p>"},{"location":"guides/Training/#guides.7_training--randomfewshot","title":"RandomFewShot","text":"<p>Randomly samples <code>k</code> examples from training data to use as few-shot prompts:</p> <pre><code>optimizer = synalinks.optimizers.RandomFewShot(nb_max_examples=3)\n</code></pre> <ul> <li>Simple and effective</li> <li>Good baseline for most tasks</li> <li>Low computational overhead</li> </ul>"},{"location":"guides/Training/#guides.7_training--omega-optimizing-memory-with-evolution-and-gradient-alignment","title":"OMEGA (Optimizing Memory with Evolution and Gradient Alignment)","text":"<p>Advanced evolutionary optimizer that:</p> <ul> <li>Maintains a population of prompt variants</li> <li>Uses fitness-based selection</li> <li>Applies crossover and mutation</li> <li>Converges to high-performing prompts</li> </ul> <pre><code>optimizer = synalinks.OMEGA(\n    population_size=10,\n    mutation_rate=0.1,\n)\n</code></pre>"},{"location":"guides/Training/#guides.7_training--rewards","title":"Rewards","text":"<p>Rewards measure how well outputs match expected values:</p>"},{"location":"guides/Training/#guides.7_training--exactmatch","title":"ExactMatch","text":"<p>Returns 1.0 if field values match exactly, 0.0 otherwise:</p> <pre><code>reward = synalinks.ExactMatch(key=\"answer\")\n</code></pre> <p>Best for:</p> <ul> <li>Classification tasks</li> <li>Factual QA with known answers</li> <li>Tasks where partial credit doesn't make sense</li> </ul>"},{"location":"guides/Training/#guides.7_training--cosinesimilarity","title":"CosineSimilarity","text":"<p>Uses embedding similarity between outputs and expected values:</p> <pre><code>reward = synalinks.CosineSimilarity(\n    embedding_model=embedding_model,\n    key=\"answer\",\n)\n</code></pre> <p>Best for:</p> <ul> <li>Open-ended generation</li> <li>Semantic similarity matters</li> <li>Multiple valid phrasings</li> </ul>"},{"location":"guides/Training/#guides.7_training--lmasjudge","title":"LMAsJudge","text":"<p>Uses another LLM to evaluate output quality:</p> <pre><code>reward = synalinks.LMAsJudge(\n    language_model=judge_model,\n    instructions=\"accuracy, helpfulness, clarity\",\n)\n</code></pre> <p>Best for:</p> <ul> <li>Complex evaluation criteria</li> <li>Subjective quality assessment</li> <li>When exact matching is too strict</li> </ul>"},{"location":"guides/Training/#guides.7_training--metrics","title":"Metrics","text":"<p>Track performance during training:</p> <pre><code>program.compile(\n    optimizer=optimizer,\n    reward=reward,\n    metrics=[\n        synalinks.metrics.MeanMetricWrapper(fn=reward, name=\"mean_reward\"),\n    ],\n)\n</code></pre> <p>The training history contains all tracked metrics:</p> <pre><code>history = await program.fit(x=data, epochs=5)\n\nprint(history.history.keys())\n# ['mean_reward', 'val_mean_reward']\n</code></pre>"},{"location":"guides/Training/#guides.7_training--complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom dotenv import load_dotenv\nimport synalinks\n\n# =============================================================================\n# Data Models\n# =============================================================================\n\nclass MathProblem(synalinks.DataModel):\n    \"\"\"A math problem.\"\"\"\n    problem: str = synalinks.Field(description=\"The math problem to solve\")\n\nclass MathAnswer(synalinks.DataModel):\n    \"\"\"A math answer.\"\"\"\n    thinking: str = synalinks.Field(description=\"Step by step calculation\")\n    answer: str = synalinks.Field(description=\"The numerical answer only\")\n\n# =============================================================================\n# Main\n# =============================================================================\n\nasync def main():\n    load_dotenv()\n    synalinks.clear_session()\n\n    lm = synalinks.LanguageModel(model=\"openai/gpt-4.1-mini\")\n\n    # -------------------------------------------------------------------------\n    # Prepare Training Data\n    # -------------------------------------------------------------------------\n    train_data = [\n        (MathProblem(problem=\"2 + 3\"), MathAnswer(thinking=\"2 + 3 = 5\", answer=\"5\")),\n        (MathProblem(problem=\"5 * 4\"), MathAnswer(thinking=\"5 * 4 = 20\", answer=\"20\")),\n        (MathProblem(problem=\"10 - 3\"), MathAnswer(thinking=\"10 - 3 = 7\", answer=\"7\")),\n        (MathProblem(problem=\"8 / 2\"), MathAnswer(thinking=\"8 / 2 = 4\", answer=\"4\")),\n        (\n            MathProblem(problem=\"3 + 3 + 3\"),\n            MathAnswer(thinking=\"3 + 3 + 3 = 9\", answer=\"9\"),\n        ),\n        (MathProblem(problem=\"7 * 2\"), MathAnswer(thinking=\"7 * 2 = 14\", answer=\"14\")),\n    ]\n\n    test_data = [\n        (MathProblem(problem=\"4 + 5\"), MathAnswer(thinking=\"4 + 5 = 9\", answer=\"9\")),\n        (MathProblem(problem=\"6 * 3\"), MathAnswer(thinking=\"6 * 3 = 18\", answer=\"18\")),\n    ]\n\n    # -------------------------------------------------------------------------\n    # Create Program\n    # -------------------------------------------------------------------------\n    inputs = synalinks.Input(data_model=MathProblem)\n    outputs = await synalinks.Generator(\n        data_model=MathAnswer,\n        language_model=lm,\n    )(inputs)\n\n    program = synalinks.Program(\n        inputs=inputs,\n        outputs=outputs,\n        name=\"math_solver\",\n    )\n\n    # -------------------------------------------------------------------------\n    # Compile with Optimizer and Reward\n    # -------------------------------------------------------------------------\n    reward = synalinks.ExactMatch(key=\"answer\")\n\n    program.compile(\n        optimizer=synalinks.optimizers.RandomFewShot(nb_max_examples=3),\n        reward=reward,\n        metrics=[\n            synalinks.metrics.MeanMetricWrapper(fn=reward, name=\"mean_reward\"),\n        ],\n    )\n\n    # -------------------------------------------------------------------------\n    # Train\n    # -------------------------------------------------------------------------\n    history = await program.fit(\n        x=train_data,\n        epochs=2,\n        validation_data=test_data,\n        verbose=1,\n    )\n\n    print(f\"Training history: {list(history.history.keys())}\")\n\n    # -------------------------------------------------------------------------\n    # Test Trained Program\n    # -------------------------------------------------------------------------\n    result = await program(MathProblem(problem=\"9 + 1\"))\n    print(f\"9 + 1 = {result['answer']}\")\n\n    # -------------------------------------------------------------------------\n    # Save and Load\n    # -------------------------------------------------------------------------\n    program.save(\"trained_math.json\")\n    loaded = synalinks.Program.load(\"trained_math.json\")\n\n    result = await loaded(MathProblem(problem=\"100 / 10\"))\n    print(f\"100 / 10 = {result['answer']}\")\n\n    import os\n    if os.path.exists(\"trained_math.json\"):\n        os.remove(\"trained_math.json\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/Training/#guides.7_training--best-practices","title":"Best Practices","text":""},{"location":"guides/Training/#guides.7_training--start-simple","title":"Start Simple","text":"<p>Begin with <code>RandomFewShot</code> and <code>ExactMatch</code>:</p> <pre><code>program.compile(\n    optimizer=synalinks.optimizers.RandomFewShot(nb_max_examples=3),\n    reward=synalinks.ExactMatch(key=\"answer\"),\n)\n</code></pre> <p>Only move to more complex optimizers/rewards if needed.</p>"},{"location":"guides/Training/#guides.7_training--use-quality-training-data","title":"Use Quality Training Data","text":"<ul> <li>Ensure examples are correct and representative</li> <li>Include edge cases and variations</li> <li>Balance difficulty levels</li> </ul>"},{"location":"guides/Training/#guides.7_training--monitor-validation-metrics","title":"Monitor Validation Metrics","text":"<p>Always use validation data to detect overfitting:</p> <pre><code>history = await program.fit(\n    x=train_data,\n    validation_data=val_data,  # Always include this\n    epochs=5,\n)\n</code></pre>"},{"location":"guides/Training/#guides.7_training--save-checkpoints","title":"Save Checkpoints","text":"<p>Save your program after training to preserve learned state:</p> <pre><code>program.save(\"checkpoint.json\")\n</code></pre>"},{"location":"guides/Training/#guides.7_training--key-takeaways","title":"Key Takeaways","text":"<ul> <li> <p>In-Context Learning: Synalinks optimizes prompts and examples, not   model weights. This works with any LLM API.</p> </li> <li> <p>Trainable Variables: Each Generator has instruction and example   variables that get optimized during training.</p> </li> <li> <p>compile() + fit(): Familiar Keras-like API for configuring and   running the training loop.</p> </li> <li> <p>Optimizers: Start with <code>RandomFewShot</code>, move to <code>OMEGA</code> for more   sophisticated optimization.</p> </li> <li> <p>Rewards: Choose based on your task - <code>ExactMatch</code> for exact answers,   <code>CosineSimilarity</code> for semantic similarity, <code>LMAsJudge</code> for complex criteria.</p> </li> <li> <p>Save Trained State: Use <code>program.save()</code> to preserve learned prompts   and examples for deployment.</p> </li> </ul>"},{"location":"guides/Training/#guides.7_training--api-references","title":"API References","text":"<ul> <li>Program.compile</li> <li>Program.fit</li> <li>RandomFewShot</li> <li>ExactMatch</li> <li>Metrics</li> </ul>"},{"location":"guides/Training/#guides.7_training.MathAnswer","title":"<code>MathAnswer</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A math answer.</p> Source code in <code>guides/7_training.py</code> <pre><code>class MathAnswer(synalinks.DataModel):\n    \"\"\"A math answer.\"\"\"\n\n    thinking: str = synalinks.Field(description=\"Step by step calculation\")\n    answer: str = synalinks.Field(description=\"The numerical answer only\")\n</code></pre>"},{"location":"guides/Training/#guides.7_training.MathProblem","title":"<code>MathProblem</code>","text":"<p>               Bases: <code>DataModel</code></p> <p>A math problem.</p> Source code in <code>guides/7_training.py</code> <pre><code>class MathProblem(synalinks.DataModel):\n    \"\"\"A math problem.\"\"\"\n\n    problem: str = synalinks.Field(description=\"The math problem to solve\")\n</code></pre>"}]}